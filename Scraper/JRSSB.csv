,title,author,link,abstract
0,Issue Information,no author,https://onlinelibrary.wiley.com/doi/10.1111/rssb.12331,no abstract
1,Graphical models for extremes,"Sebastian Engelke, Adrien S. Hitz",https://onlinelibrary.wiley.com/doi/10.1111/rssb.12355,"Conditional independence, graphical models and sparsity are key notions for parsimonious statistical models and for understanding the structural relationships in the data. The theory of multivariate and spatial extremes describes the risk of rare events through asymptotically justified limit models such as max‐stable and multivariate Pareto distributions. Statistical modelling in this field has been limited to moderate dimensions so far, partly owing to complicated likelihoods and a lack of understanding of the underlying probabilistic structures. We introduce a general theory of conditional independence for multivariate Pareto distributions that enables the definition of graphical models and sparsity for extremes. A Hammersley–Clifford theorem links this new notion to the factorization of densities of extreme value models on graphs. For the popular class of Hüsler–Reiss distributions we show that, similarly to the Gaussian case, the sparsity pattern of a general extremal graphical model can be read off from suitable inverse covariance matrices. New parametric models can be built in a modular way and statistical inference can be simplified to lower dimensional marginals. We discuss learning of minimum spanning trees and model selection for extremal graph structures, and we illustrate their use with an application to flood risk assessment on the Danube river."
2,A unified data‐adaptive framework for high dimensional change point detection,"Bin Liu, Cheng Zhou, Xinsheng Zhang, Yufeng Liu",https://onlinelibrary.wiley.com/doi/10.1111/rssb.12375,"In recent years, change point detection for a high dimensional data sequence has become increasingly important in many scientific fields such as biology and finance. The existing literature develops a variety of methods designed for either a specified parameter (e.g. the mean or covariance) or a particular alternative pattern (sparse or dense), but not for both scenarios simultaneously. To overcome this limitation, we provide a general framework for developing tests that are suitable for a large class of parameters, and also adaptive to various alternative scenarios. In particular, by generalizing the classical cumulative sum statistic, we construct the U‐statistic‐based cumulative sum matrix 
            
            
C
. Two cases corresponding to common or different change point locations across the components are considered. We then propose two types of individual test statistics by aggregating 
            
            
C
 on the basis of the adjusted Lp‐norm with p ∈ {1,…,∞}. Combining the corresponding individual tests, we construct two types of data‐adaptive tests for the two cases, which are both powerful under various alternative patterns. A multiplier bootstrap method is introduced for approximating the proposed test statistics’ limiting distributions. With flexible dependence structure across co‐ordinates and mild moment conditions, we show the optimality of our methods theoretically in terms of size and power by allowing the dimension d and the number of parameters q to be much larger than the sample size n. An R package called AdaptiveCpt is developed to implement our algorithms. Extensive simulation studies provide further support for our theory. An application to a comparative genomic hybridization data set also demonstrates the usefulness of our proposed methods."
3,A scalable estimate of the out‐of‐sample prediction error via approximate leave‐one‐out cross‐validation,"Kamiar Rahnama Rad, Arian Maleki",https://onlinelibrary.wiley.com/doi/10.1111/rssb.12374,"The paper considers the problem of out‐of‐sample risk estimation under the high dimensional settings where standard techniques such as K‐fold cross‐validation suffer from large biases. Motivated by the low bias of the leave‐one‐out cross‐validation method, we propose a computationally efficient closed form approximate leave‐one‐out formula ALO for a large class of regularized estimators. Given the regularized estimate, calculating ALO requires a minor computational overhead. With minor assumptions about the data‐generating process, we obtain a finite sample upper bound for the difference between leave‐one‐out cross‐validation and approximate leave‐one‐out cross‐validation, |LO−ALO|. Our theoretical analysis illustrates that |LO−ALO|→0 with overwhelming probability, when n,p→∞, where the dimension p of the feature vectors may be comparable with or even greater than the number of observations, n. Despite the high dimensionality of the problem, our theoretical results do not require any sparsity assumption on the vector of regression coefficients. Our extensive numerical experiments show that |LO−ALO| decreases as n and p increase, revealing the excellent finite sample performance of approximate leave‐one‐out cross‐validation. We further illustrate the usefulness of our proposed out‐of‐sample risk estimation method by an example of real recordings from spatially sensitive neurons (grid cells) in the medial entorhinal cortex of a rat."
4,False discovery and its control in low rank estimation,"Armeen Taeb, Parikshit Shah, Venkat Chandrasekaran",https://onlinelibrary.wiley.com/doi/10.1111/rssb.12387,"Models specified by low rank matrices are ubiquitous in contemporary applications. In many of these problem domains, the row–column space structure of a low rank matrix carries information about some underlying phenomenon, and it is of interest in inferential settings to evaluate the extent to which the row–column spaces of an estimated low rank matrix signify discoveries about the phenomenon. However, in contrast with variable selection, we lack a formal framework to assess true or false discoveries in low rank estimation; in particular, the key source of difficulty is that the standard notion of a discovery is a discrete notion that is ill suited to the smooth structure underlying low rank matrices. We address this challenge via a geometric reformulation of the concept of a discovery, which then enables a natural definition in the low rank case. We describe and analyse a generalization of the stability selection method of Meinshausen and Bühlmann to control for false discoveries in low rank estimation, and we demonstrate its utility compared with previous approaches via numerical experiments."
5,Adaptive designs for optimal observed Fisher information,Adam Lane,https://onlinelibrary.wiley.com/doi/10.1111/rssb.12378,"Expected Fisher information can be found a priori and as a result its inverse is the primary variance approximation used in the design of experiments. This is in contrast with the common claim that the inverse of the observed Fisher information is a better approximation of the variance of the maximum likelihood estimator. Observed Fisher information cannot be known a priori; however, if an experiment is conducted sequentially, in a series of runs, the observed Fisher information from previous runs is known. In the current work, two adaptive designs are proposed that use the observed Fisher information from previous runs to inform the design of future runs."
6,Visualizing the effects of predictor variables in black box supervised learning models,"Daniel W. Apley, Jingyu Zhu",https://onlinelibrary.wiley.com/doi/10.1111/rssb.12377,"In many supervised learning applications, understanding and visualizing the effects of the predictor variables on the predicted response is of paramount importance. A shortcoming of black box supervised learning models (e.g. complex trees, neural networks, boosted trees, random forests, nearest neighbours, local kernel‐weighted methods and support vector regression) in this regard is their lack of interpretability or transparency. Partial dependence plots, which are the most popular approach for visualizing the effects of the predictors with black box supervised learning models, can produce erroneous results if the predictors are strongly correlated, because they require extrapolation of the response at predictor values that are far outside the multivariate envelope of the training data. As an alternative to partial dependence plots, we present a new visualization approach that we term accumulated local effects plots, which do not require this unreliable extrapolation with correlated predictors. Moreover, accumulated local effects plots are far less computationally expensive than partial dependence plots. We also provide an R package ALEPlot as supplementary material to implement our proposed method."
7,Quasi‐Bayes properties of a procedure for sequential learning in mixture models,"Sandra Fortini, Sonia Petrone",https://onlinelibrary.wiley.com/doi/10.1111/rssb.12385,"Bayesian methods are often optimal, yet increasing pressure for fast computations, especially with streaming data, brings renewed interest in faster, possibly suboptimal, solutions. The extent to which these algorithms approximate Bayesian solutions is a question of interest, but often unanswered. We propose a methodology to address this question in predictive settings, when the algorithm can be reinterpreted as a probabilistic predictive rule. We specifically develop the proposed methodology for a recursive procedure for on‐line learning in non‐parametric mixture models, which is often referred to as Newton's algorithm. This algorithm is simple and fast; however, its approximation properties are unclear. By reinterpreting it as a predictive rule, we can show that it underlies a statistical model which is, asymptotically, a Bayesian, exchangeable mixture model. In this sense, the recursive rule provides a quasi‐Bayes solution. Although the algorithm offers only a point estimate, our clean statistical formulation enables us to provide the asymptotic posterior distribution and asymptotic credible intervals for the mixing distribution. Moreover, it gives insights for tuning the parameters, as we illustrate in simulation studies, and paves the way to extensions in various directions. Beyond mixture models, our approach can be applied to other predictive algorithms."
8,Superconsistent estimation of points of impact in non‐parametric regression with functional predictors,"Dominik Poß, Dominik Liebl, Alois Kneip, Hedwig Eisenbarth, Tor D. Wager, Lisa Feldman Barrett",https://onlinelibrary.wiley.com/doi/10.1111/rssb.12386,"Predicting scalar outcomes by using functional predictors is a classical problem in functional data analysis. In many applications, however, only specific locations or time points of the functional predictors have an influence on the outcome. Such ‘points of impact’ are typically unknown and must be estimated in addition to estimating the usual model components. We show that our points‐of‐impact estimator enjoys a superconsistent rate of convergence and does not require knowledge or pre‐estimates of the unknown model components. This remarkable result facilitates the subsequent estimation of the remaining model components as shown in the theoretical part, where we consider the case of non‐parametric models and the practically relevant case of generalized linear models. The finite sample properties of our estimators are assessed by means of a simulation study. Our methodology is motivated by data from a psychological experiment in which the participants were asked to rate their emotional state continuously while watching an affective video eliciting a varying intensity of emotional reactions."
9,Optimal alpha spending for sequential analysis with binomial data,"Ivair R. Silva, Martin Kulldorff, W. Katherine Yih",https://onlinelibrary.wiley.com/doi/10.1111/rssb.12379,"For sequential analysis hypothesis testing, various alpha spending functions have been proposed. Given a prespecified overall alpha level and power, we derive the optimal alpha spending function that minimizes the expected time to signal for continuous as well as group sequential analysis. If there is also a restriction on the maximum sample size or on the expected sample size, we do the same. Alternatively, for fixed overall alpha, power and expected time to signal, we derive the optimal alpha spending function that minimizes the expected sample size. The method constructs alpha spending functions that are uniformly better than any other method, such as the classical Wald, Pocock or O’Brien–Fleming methods. The results are based on exact calculations using linear programming. All numerical examples were run by using the R Sequential package."
10,Issue Information,no author,https://onlinelibrary.wiley.com/doi/10.1111/rssb.12330,no abstract
11,Unbiased Markov chain Monte Carlo methods with couplings,"Pierre E. Jacob, John O’Leary, Yves F. Atchadé",https://onlinelibrary.wiley.com/doi/10.1111/rssb.12336,"Markov chain Monte Carlo (MCMC) methods provide consistent approximations of integrals as the number of iterations goes to ∞. MCMC estimators are generally biased after any fixed number of iterations. We propose to remove this bias by using couplings of Markov chains together with a telescopic sum argument of Glynn and Rhee. The resulting unbiased estimators can be computed independently in parallel. We discuss practical couplings for popular MCMC algorithms. We establish the theoretical validity of the estimators proposed and study their efficiency relative to the underlying MCMC algorithms. Finally, we illustrate the performance and limitations of the method on toy examples, on an Ising model around its critical temperature, on a high dimensional variable‐selection problem, and on an approximation of the cut distribution arising in Bayesian inference for models made of multiple modules."
12,Robust estimation via robust gradient estimation,"Adarsh Prasad, Arun Sai Suggala, Sivaraman Balakrishnan, Pradeep Ravikumar",https://onlinelibrary.wiley.com/doi/10.1111/rssb.12364,"We provide a new computationally efficient class of estimators for risk minimization. We show that these estimators are robust for general statistical models, under varied robustness settings, including in the classical Huber ε‐contamination model, and in heavy‐tailed settings. Our workhorse is a novel robust variant of gradient descent, and we provide conditions under which our gradient descent variant provides accurate estimators in a general convex risk minimization problem. We provide specific consequences of our theory for linear regression and logistic regression and for canonical parameter estimation in an exponential family. These results provide some of the first computationally tractable and provably robust estimators for these canonical statistical models. Finally, we study the empirical performance of our proposed methods on synthetic and real data sets, and we find that our methods convincingly outperform a variety of baselines."
13,Testing relevant hypotheses in functional time series via self‐normalization,"Holger Dette, Kevin Kokot, Stanislav Volgushev",https://onlinelibrary.wiley.com/doi/10.1111/rssb.12370,"We develop methodology for testing relevant hypotheses about functional time series in a tuning‐free way. Instead of testing for exact equality, e.g. for the equality of two mean functions from two independent time series, we propose to test the null hypothesis of no relevant deviation. In the two‐sample problem this means that an 
            
            

L
2

‐distance between the two mean functions is smaller than a prespecified threshold. For such hypotheses self‐normalization, which was introduced in 2010 by Shao, and Shao and Zhang and is commonly used to avoid the estimation of nuisance parameters, is not directly applicable. We develop new self‐normalized procedures for testing relevant hypotheses in the one‐sample, two‐sample and change point problem and investigate their asymptotic properties. Finite sample properties of the tests proposed are illustrated by means of a simulation study and data examples. Our main focus is on functional time series, but extensions to other settings are also briefly discussed."
14,Causal mediation analysis for stochastic interventions,"Iván Díaz, Nima S. Hejazi",https://onlinelibrary.wiley.com/doi/10.1111/rssb.12362,"Mediation analysis in causal inference has traditionally focused on binary exposures and deterministic interventions, and a decomposition of the average treatment effect in terms of direct and indirect effects. We present an analogous decomposition of the population intervention effect, defined through stochastic interventions on the exposure. Population intervention effects provide a generalized framework in which a variety of interesting causal contrasts can be defined, including effects for continuous and categorical exposures. We show that identification of direct and indirect effects for the population intervention effect requires weaker assumptions than its average treatment effect counterpart, under the assumption of no mediator–outcome confounders affected by exposure. In particular, identification of direct effects is guaranteed in experiments that randomize the exposure and the mediator. We propose various estimators of the direct and indirect effects, including substitution, reweighted and efficient estimators based on flexible regression techniques, allowing for multivariate mediators. Our efficient estimator is asymptotically linear under a condition requiring n1/4‐consistency of certain regression functions. We perform a simulation study in which we assess the finite sample properties of our proposed estimators. We present the results of an illustrative study where we assess the effect of participation in a sports team on the body mass index among children, using mediators such as exercise habits, daily consumption of snacks and overweight status."
15,A flexible framework for hypothesis testing in high dimensions,"Adel Javanmard, Jason D. Lee",https://onlinelibrary.wiley.com/doi/10.1111/rssb.12373,"Hypothesis testing in the linear regression model is a fundamental statistical problem. We consider linear regression in the high dimensional regime where the number of parameters exceeds the number of samples (p>n). To make informative inference, we assume that the model is approximately sparse, i.e. the effect of covariates on the response can be well approximated by conditioning on a relatively small number of covariates whose identities are unknown. We develop a framework for testing very general hypotheses regarding the model parameters. Our framework encompasses testing whether the parameter lies in a convex cone, testing the signal strength, and testing arbitrary functionals of the parameter. We show that the procedure proposed controls the type I error, and we also analyse the power of the procedure. Our numerical experiments confirm our theoretical findings and demonstrate that we control the false positive rate (type I error) near the nominal level and have high power. By duality between hypotheses testing and confidence intervals, the framework proposed can be used to obtain valid confidence intervals for various functionals of the model parameters. For linear functionals, the length of confidence intervals is shown to be minimax rate optimal."
16,Causal isotonic regression,"Ted Westling, Peter Gilbert, Marco Carone",https://onlinelibrary.wiley.com/doi/10.1111/rssb.12372,"In observational studies, potential confounders may distort the causal relationship between an exposure and an outcome. However, under some conditions, a causal dose–response curve can be recovered by using the G‐computation formula. Most classical methods for estimating such curves when the exposure is continuous rely on restrictive parametric assumptions, which carry significant risk of model misspecification. Non‐parametric estimation in this context is challenging because in a non‐parametric model these curves cannot be estimated at regular rates. Many available non‐parametric estimators are sensitive to the selection of certain tuning parameters, and performing valid inference with such estimators can be difficult. We propose a non‐parametric estimator of a causal dose–response curve known to be monotone. We show that our proposed estimation procedure generalizes the classical least squares isotonic regression estimator of a monotone regression function. Specifically, it does not involve tuning parameters and is invariant to strictly monotone transformations of the exposure variable. We describe theoretical properties of our proposed estimator, including its irregular limit distribution and the potential for doubly robust inference. Furthermore, we illustrate its performance via numerical studies and use it to assess the relationship between body mass index and immune response in human immunodeficiency virus vaccine trials."
17,"Optimal, two‐stage, adaptive enrichment designs for randomized trials, using sparse linear programming","Michael Rosenblum, Ethan X. Fang, Han Liu",https://onlinelibrary.wiley.com/doi/10.1111/rssb.12366,"Adaptive enrichment designs involve preplanned rules for modifying enrolment criteria based on accruing data in a randomized trial. We focus on designs where the overall population is partitioned into two predefined subpopulations, e.g. based on a biomarker or risk score measured at baseline. The goal is to learn which populations benefit from an experimental treatment. Two critical components of adaptive enrichment designs are the decision rule for modifying enrolment, and the multiple‐testing procedure. We provide a general method for simultaneously optimizing these components for two‐stage, adaptive enrichment designs. We minimize the expected sample size under constraints on power and the familywise type I error rate. It is computationally infeasible to solve this optimization problem directly because of its non‐convexity. The key to our approach is a novel, discrete representation of this optimization problem as a sparse linear program, which is large but computationally feasible to solve by using modern optimization techniques. We provide an R package that implements our method and is compatible with linear program solvers in several software languages. Our approach produces new, approximately optimal trial designs."
18,Goodness‐of‐fit testing in high dimensional generalized linear models,"Jana Janková, Rajen D. Shah, Peter Bühlmann, Richard J. Samworth",https://onlinelibrary.wiley.com/doi/10.1111/rssb.12371,"We propose a family of tests to assess the goodness of fit of a high dimensional generalized linear model. Our framework is flexible and may be used to construct an omnibus test or directed against testing specific non‐linearities and interaction effects, or for testing the significance of groups of variables. The methodology is based on extracting left‐over signal in the residuals from an initial fit of a generalized linear model. This can be achieved by predicting this signal from the residuals by using modern powerful regression or machine learning methods such as random forests or boosted trees. Under the null hypothesis that the generalized linear model is correct, no signal is left in the residuals and our test statistic has a Gaussian limiting distribution, translating to asymptotic control of type I error. Under a local alternative, we establish a guarantee on the power of the test. We illustrate the effectiveness of the methodology on simulated and real data examples by testing goodness of fit in logistic regression models. Software implementing the methodology is available in the R package GRPtests."
19,Inference for two‐stage sampling designs,"Guillaume Chauvet, Audrey‐Anne Vallée",https://onlinelibrary.wiley.com/doi/10.1111/rssb.12368,"Two‐stage sampling designs are commonly used for household and health surveys. To produce reliable estimators with associated confidence intervals, some basic statistical properties like consistency and asymptotic normality of the Horvitz–Thompson estimator are desirable, along with the consistency of associated variance estimators. These properties have been mainly studied for single‐stage sampling designs. In this work, we prove the consistency of the Horvitz–Thompson estimator and of associated variance estimators for a general class of two‐stage sampling designs, under mild assumptions. We also study two‐stage sampling with a large entropy sampling design at the first stage and prove that the Horvitz–Thompson estimator is asymptotically normally distributed through a coupling argument. When the first‐stage sampling fraction is negligible, simplified variance estimators which do not require estimating the variance within the primary sampling units are proposed and shown to be consistent. An application to a panel for urban policy, which is the initial motivation for this work, is also presented."
20,On bandwidth choice for spatial data density estimation,"Zhenyu Jiang, Nengxiang Ling, Zudi Lu, Dag Tj⊘stheim, Qiang Zhang",https://onlinelibrary.wiley.com/doi/10.1111/rssb.12367,"Bandwidth choice is crucial in spatial kernel estimation in exploring non‐Gaussian complex spatial data. The paper investigates the choice of adaptive and non‐adaptive bandwidths for density estimation given data on a spatial lattice. An adaptive bandwidth depends on local data and hence adaptively conforms with local features of the spatial data. We propose a spatial cross‐validation (SCV) choice of a global bandwidth. This is done first with a pilot density involved in the expression for the adaptive bandwidth. The optimality of the procedure is established, and it is shown that a non‐adaptive bandwidth choice comes out as a special case. Although the cross‐validation idea has been popular for choosing a non‐adaptive bandwidth in data‐driven smoothing of independent and time series data, its theory and application have not been much investigated for spatial data. For the adaptive case, there is little theory even for independent data. Conditions that ensure asymptotic optimality of the SCV‐selected bandwidth are derived, actually, also extending time series and independent data optimality results. Further, for the adaptive bandwidth with an estimated pilot density, oracle properties of the resultant density estimator are obtained asymptotically as if the true pilot were known. Numerical simulations show that finite sample performance of the SCV adaptive bandwidth choice works quite well. It outperforms the existing R routines such as the ‘rule of thumb’ and the so‐called ‘second‐generation’ Sheather–Jones bandwidths for moderate and big data sets. An empirical application to a set of spatial soil data is further implemented with non‐Gaussian features significantly identified."
21,Robust testing in generalized linear models by sign flipping score contributions,"Jesse Hemerik, Jelle J. Goeman, Livio Finos",https://onlinelibrary.wiley.com/doi/10.1111/rssb.12369,"Generalized linear models are often misspecified because of overdispersion, heteroscedasticity and ignored nuisance variables. Existing quasi‐likelihood methods for testing in misspecified models often do not provide satisfactory type I error rate control. We provide a novel semiparametric test, based on sign flipping individual score contributions. The parameter tested is allowed to be multi‐dimensional and even high dimensional. Our test is often robust against the mentioned forms of misspecification and provides better type I error control than its competitors. When nuisance parameters are estimated, our basic test becomes conservative. We show how to take nuisance estimation into account to obtain an asymptotically exact test. Our proposed test is asymptotically equivalent to its parametric counterpart."
22,Reply to the correction by Grover and Kaur: a new randomized response model,Sarjinder Singh,https://onlinelibrary.wiley.com/doi/10.1111/rssb.12376,"Acknowledgements
The author is grateful to the Joint Editor and a learned referee for very valuable comments on the original version of this reply. The author is also grateful to Professor Stephen A. Sedory, Department of Mathematics, Texas A&M University, for editing the original version of this reply."
23,Issue Information,no author,https://onlinelibrary.wiley.com/doi/10.1111/rssb.12329,no abstract
24,Functional models for time‐varying random objects,"Paromita Dubey, Hans‐Georg Müller",https://onlinelibrary.wiley.com/doi/10.1111/rssb.12337,"Functional data analysis provides a popular toolbox of functional models for the analysis of samples of random functions that are real valued. In recent years, samples of time‐varying object data such as time‐varying networks that are not in a vector space have been increasingly collected. These data can be viewed as elements of a general metric space that lacks local or global linear structure and therefore common approaches that have been used with great success for the analysis of functional data, such as functional principal component analysis, cannot be applied. We propose metric covariance, a novel association measure for paired object data lying in a metric space (Ω,d) that we use to define a metric autocovariance function for a sample of random Ω‐valued curves, where Ω generally will not have a vector space or manifold structure. The proposed metric autocovariance function is non‐negative definite when the squared semimetric d2 is of negative type. Then the eigenfunctions of the linear operator with the autocovariance function as kernel can be used as building blocks for an object functional principal component analysis for Ω‐valued functional data, including time‐varying probability distributions, covariance matrices and time dynamic networks. Analogues of functional principal components for time‐varying objects are obtained by applying Fréchet means and projections of distance functions of the random object trajectories in the directions of the eigenfunctions, leading to real‐valued Fréchet scores. Using the notion of generalized Fréchet integrals, we construct object functional principal components that lie in the metric space Ω. We establish asymptotic consistency of the sample‐based estimators for the corresponding population targets under mild metric entropy conditions on Ω and continuity of the Ω‐valued random curves. These concepts are illustrated with samples of time‐varying probability distributions for human mortality, time‐varying covariance matrices derived from trading patterns and time‐varying networks that arise from New York taxi trips."
25,Sparse principal component analysis via axis‐aligned random projections,"Milana Gataric, Tengyao Wang, Richard J. Samworth",https://onlinelibrary.wiley.com/doi/10.1111/rssb.12360,"We introduce a new method for sparse principal component analysis, based on the aggregation of eigenvector information from carefully selected axis‐aligned random projections of the sample covariance matrix. Unlike most alternative approaches, our algorithm is non‐iterative, so it is not vulnerable to a bad choice of initialization. We provide theoretical guarantees under which our principal subspace estimator can attain the minimax optimal rate of convergence in polynomial time. In addition, our theory provides a more refined understanding of the statistical and computational trade‐off in the problem of sparse principal component estimation, revealing a subtle interplay between the effective sample size and the number of random projections that are required to achieve the minimax optimal rate. Numerical studies provide further insight into the procedure and confirm its highly competitive finite sample performance."
26,Right singular vector projection graphs: fast high dimensional covariance matrix estimation under latent confounding,"Rajen D. Shah, Benjamin Frot, Gian‐Andrea Thanei, Nicolai Meinshausen",https://onlinelibrary.wiley.com/doi/10.1111/rssb.12359,"We consider the problem of estimating a high dimensional p×p covariance matrix Σ, given n observations of confounded data with covariance , where Γ is an unknown p×q matrix of latent factor loadings. We propose a simple and scalable estimator based on the projection onto the right singular vectors of the observed data matrix, which we call right singular vector projection (RSVP). Our theoretical analysis of this method reveals that, in contrast with approaches based on the removal of principal components, RSVP can cope well with settings where the smallest eigenvalue of  is relatively close to the largest eigenvalue of Σ, as well as when the eigenvalues of  are diverging fast. RSVP does not require knowledge or estimation of the number of latent factors q, but it recovers Σ only up to an unknown positive scale factor. We argue that this suffices in many applications, e.g. if an estimate of the correlation matrix is desired. We also show that, by using subsampling, we can further improve the performance of the method. We demonstrate the favourable performance of RSVP through simulation experiments and an analysis of gene expression data sets collated by the GTEX consortium."
27,Semisupervised inference for explained variance in high dimensional linear regression and its applications,"T. Tony Cai, Zijian Guo",https://onlinelibrary.wiley.com/doi/10.1111/rssb.12357,"The paper considers statistical inference for the explained variance  under the high dimensional linear model Y=Xβ+ε in the semisupervised setting, where β is the regression vector and Σ is the design covariance matrix. A calibrated estimator, which efficiently integrates both labelled and unlabelled data, is proposed. It is shown that the estimator achieves the minimax optimal rate of convergence in the general semisupervised framework. The optimality result characterizes how the unlabelled data contribute to the estimation accuracy. Moreover, the limiting distribution for the proposed estimator is established and the unlabelled data have also proved useful in reducing the length of the confidence interval for the explained variance. The method proposed is extended to semisupervised inference for the unweighted quadratic functional . The inference results obtained are then applied to a range of high dimensional statistical problems, including signal detection and global testing, prediction accuracy evaluation and confidence ball construction. The numerical improvement of incorporating the unlabelled data is demonstrated through simulation studies and an analysis of estimating heritability for a yeast segregant data set with multiple traits."
28,Model misspecification in approximate Bayesian computation: consequences and diagnostics,"David T. Frazier, Christian P. Robert, Judith Rousseau",https://onlinelibrary.wiley.com/doi/10.1111/rssb.12356,"We analyse the behaviour of approximate Bayesian computation (ABC) when the model generating the simulated data differs from the actual data‐generating process, i.e. when the data simulator in ABC is misspecified. We demonstrate both theoretically and in simple, but practically relevant, examples that when the model is misspecified different versions of ABC can yield substantially different results. Our theoretical results demonstrate that even though the model is misspecified, under regularity conditions, the accept–reject ABC approach concentrates posterior mass on an appropriately defined pseudotrue parameter value. However, under model misspecification the ABC posterior does not yield credible sets with valid frequentist coverage and has non‐standard asymptotic behaviour. In addition, we examine the theoretical behaviour of the popular local regression adjustment to ABC under model misspecification and demonstrate that this approach concentrates posterior mass on a pseudotrue value that is completely different from accept–reject ABC. Using our theoretical results, we suggest two approaches to diagnose model misspecification in ABC. All theoretical results and diagnostics are illustrated in a simple running example."
29,Doubly robust inference when combining probability and non‐probability samples with high dimensional data,"Shu Yang, Jae Kwang Kim, Rui Song",https://onlinelibrary.wiley.com/doi/10.1111/rssb.12354,"We consider integrating a non‐probability sample with a probability sample which provides high dimensional representative covariate information of the target population. We propose a two‐step approach for variable selection and finite population inference. In the first step, we use penalized estimating equations with folded concave penalties to select important variables and show selection consistency for general samples. In the second step, we focus on a doubly robust estimator of the finite population mean and re‐estimate the nuisance model parameters by minimizing the asymptotic squared bias of the doubly robust estimator. This estimating strategy mitigates the possible first‐step selection error and renders the doubly robust estimator root n consistent if either the sampling probability or the outcome model is correctly specified."
30,"Sumca: simple, unified, Monte‐Carlo‐assisted approach to second‐order unbiased mean‐squared prediction error estimation","Jiming Jiang, Mahmoud Torabi",https://onlinelibrary.wiley.com/doi/10.1111/rssb.12358,"We propose a simple, unified, Monte‐Carlo‐assisted approach (called ‘Sumca’) to second‐order unbiased estimation of the mean‐squared prediction error (MSPE) of a small area predictor. The MSPE estimator proposed is easy to derive, has a simple expression and applies to a broad range of predictors that include the traditional empirical best linear unbiased predictor, empirical best predictor and post‐model‐selection empirical best linear unbiased predictor and empirical best predictor as special cases. Furthermore, the leading term of the MSPE estimator proposed is guaranteed positive; the lower order term corresponds to a bias correction, which can be evaluated via a Monte Carlo method. The computational burden for the Monte Carlo evaluation is much less, compared with other Monte‐Carlo‐based methods that have been used for producing second‐order unbiased MSPE estimators, such as the double bootstrap and Monte Carlo jackknife. The Sumca estimator also has a nice stability feature. Theoretical and empirical results demonstrate properties and advantages of the Sumca estimator."
31,Exchangeable random measures for sparse and modular graphs with overlapping communities,"Adrien Todeschini, Xenia Miscouridou, François Caron",https://onlinelibrary.wiley.com/doi/10.1111/rssb.12363,"We propose a novel statistical model for sparse networks with overlapping community structure. The model is based on representing the graph as an exchangeable point process and naturally generalizes existing probabilistic models with overlapping block structure to the sparse regime. Our construction builds on vectors of completely random measures and has interpretable parameters, each node being assigned a vector representing its levels of affiliation to some latent communities. We develop methods for efficient simulation of this class of random graphs and for scalable posterior inference. We show that the approach proposed can recover interpretable structure of real world networks and can handle graphs with thousands of nodes and tens of thousands of edges."
32,Multiply robust causal inference with double‐negative control adjustment for categorical unmeasured confounding,"Xu Shi, Wang Miao, Jennifer C. Nelson, Eric J. Tchetgen Tchetgen",https://onlinelibrary.wiley.com/doi/10.1111/rssb.12361,"Unmeasured confounding is a threat to causal inference in observational studies. In recent years, the use of negative controls to mitigate unmeasured confounding has gained increasing recognition and popularity. Negative controls have a long‐standing tradition in laboratory sciences and epidemiology to rule out non‐causal explanations, although they have been used primarily for bias detection. Recently, Miao and colleagues have described sufficient conditions under which a pair of negative control exposure and outcome variables can be used to identify non‐parametrically the average treatment effect (ATE) from observational data subject to uncontrolled confounding. We establish non‐parametric identification of the ATE under weaker conditions in the case of categorical unmeasured confounding and negative control variables. We also provide a general semiparametric framework for obtaining inferences about the ATE while leveraging information about a possibly large number of measured covariates. In particular, we derive the semiparametric efficiency bound in the non‐parametric model, and we propose multiply robust and locally efficient estimators when non‐parametric estimation may not be feasible. We assess the finite sample performance of our methods in extensive simulation studies. Finally, we illustrate our methods with an application to the post‐licensure surveillance of vaccine safety among children."
33,Issue Information,no author,https://onlinelibrary.wiley.com/doi/10.1111/rssb.12328,no abstract
34,Report of the Editors—2019,"David Dunson, Simon Wood",https://onlinelibrary.wiley.com/doi/10.1111/rssb.12350,no abstract
35,Multiscale inference and long‐run variance estimation in non‐parametric regression with time series errors,"Marina Khismatullina, Michael Vogt",https://onlinelibrary.wiley.com/doi/10.1111/rssb.12347,"We develop new multiscale methods to test qualitative hypotheses about the function m in the non‐parametric regression model Yt,T=m(t/T)+ɛt with time series errors ɛt. In time series applications, m represents a non‐parametric time trend. Practitioners are often interested in whether the trend m has certain shape properties. For example, they would like to know whether m is constant or whether it is increasing or decreasing in certain time intervals. Our multiscale methods enable us to test for such shape properties of the trend m. To perform the methods, we require an estimator of the long‐run error variance . We propose a new difference‐based estimator of σ2 for the case that {ɛt} belongs to the class of auto‐regressive AR(∞) processes. In the technical part of the paper, we derive asymptotic theory for the proposed multiscale test and the estimator of the long‐run error variance. The theory is complemented by a simulation study and an empirical application to climate data."
36,Making sense of sensitivity: extending omitted variable bias,"Carlos Cinelli, Chad Hazlett",https://onlinelibrary.wiley.com/doi/10.1111/rssb.12348,"We extend the omitted variable bias framework with a suite of tools for sensitivity analysis in regression models that does not require assumptions on the functional form of the treatment assignment mechanism nor on the distribution of the unobserved confounders, naturally handles multiple confounders, possibly acting non‐linearly, exploits expert knowledge to bound sensitivity parameters and can be easily computed by using only standard regression results. In particular, we introduce two novel sensitivity measures suited for routine reporting. The robustness value describes the minimum strength of association that unobserved confounding would need to have, both with the treatment and with the outcome, to change the research conclusions. The partial R2 of the treatment with the outcome shows how strongly confounders explaining all the residual outcome variation would have to be associated with the treatment to eliminate the estimated effect. Next, we offer graphical tools for elaborating on problematic confounders, examining the sensitivity of point estimates and t‐values, as well as ‘extreme scenarios’. Finally, we describe problems with a common ‘benchmarking’ practice and introduce a novel procedure to bound the strength of confounders formally on the basis of a comparison with observed covariates. We apply these methods to a running example that estimates the effect of exposure to violence on attitudes toward peace."
37,Renewable estimation and incremental inference in generalized linear models with streaming data sets,"Lan Luo, Peter X.‐K. Song",https://onlinelibrary.wiley.com/doi/10.1111/rssb.12352,"The paper presents an incremental updating algorithm to analyse streaming data sets using generalized linear models. The method proposed is formulated within a new framework of renewable estimation and incremental inference, in which the maximum likelihood estimator is renewed with current data and summary statistics of historical data. Our framework can be implemented within a popular distributed computing environment, known as Apache Spark, to scale up computation. Consisting of two data‐processing layers, the rho architecture enables us to accommodate inference‐related statistics and to facilitate sequential updating of the statistics used in both estimation and inference. We establish estimation consistency and asymptotic normality of the proposed renewable estimator, in which the Wald test is utilized for an incremental inference. Our methods are examined and illustrated by various numerical examples from both simulation experiments and a real world data analysis."
38,Targeted sampling from massive block model graphs with personalized PageRank,"Fan Chen, Yini Zhang, Karl Rohe",https://onlinelibrary.wiley.com/doi/10.1111/rssb.12349,"The paper provides statistical theory and intuition for personalized PageRank (called ‘PPR’): a popular technique that samples a small community from a massive network. We study a setting where the entire network is expensive to obtain thoroughly or to maintain, but we can start from a seed node of interest and ‘crawl’ the network to find other nodes through their connections. By crawling the graph in a designed way, the PPR vector can be approximated without querying the entire massive graph, making it an alternative to snowball sampling. Using the degree‐corrected stochastic block model, we study whether the PPR vector can select nodes that belong to the same block as the seed node. We provide a simple and interpretable form for the PPR vector, highlighting its biases towards high degree nodes outside the target block. We examine a simple adjustment based on node degrees and establish consistency results for PPR clustering that allows for directed graphs. These results are enabled by recent technical advances showing the elementwise convergence of eigenvectors. We illustrate the method with the massive Twitter friendship graph, which we crawl by using the Twitter application programming interface. We find that the adjusted and unadjusted PPR techniques are complementary approaches, where the adjustment makes the results particularly localized around the seed node, and that the bias adjustment greatly benefits from degree regularization."
39,A Bayesian hierarchical model for related densities by using Pólya trees,"Jonathan Christensen, Li Ma",https://onlinelibrary.wiley.com/doi/10.1111/rssb.12346,"Bayesian hierarchical models are used to share information between related samples and to obtain more accurate estimates of sample level parameters, common structure and variation between samples. When the parameter of interest is the distribution or density of a continuous variable, a hierarchical model for continuous distributions is required. Various such models have been described in the literature using extensions of the Dirichlet process and related processes, typically as a distribution on the parameters of a mixing kernel. We propose a new hierarchical model based on the Pólya tree, which enables direct modelling of densities and enjoys some computational advantages over the Dirichlet process. The Pólya tree also enables more flexible modelling of the variation between samples, providing more informed shrinkage and permitting posterior inference on the dispersion function, which quantifies the variation between sample densities. We also show how the model can be extended to cluster samples in situations where the observed samples are believed to have been drawn from several latent populations."
40,Bayesian empirical likelihood inference with complex survey data,"Puying Zhao, Malay Ghosh, J. N. K. Rao, Changbao Wu",https://onlinelibrary.wiley.com/doi/10.1111/rssb.12342,We propose a Bayesian empirical likelihood approach to survey data analysis on a vector of finite population parameters defined through estimating equations. Our method allows overidentified estimating equation systems and is applicable to both smooth and non‐differentiable estimating functions. Our proposed Bayesian estimator is design consistent for general sampling designs and the Bayesian credible intervals are calibrated in the sense of having asymptotically valid design‐based frequentist properties under single‐stage unequal probability sampling designs with small sampling fractions. Large sample properties of the Bayesian inference proposed are established for both non‐informative and informative priors under the design‐based framework. We also propose a Bayesian model selection procedure with complex survey data and show that it works for general sampling designs. An efficient Markov chain Monte Carlo procedure is described for the required computation of the posterior distribution for general vector parameters. Simulation studies and an application to a real survey data set are included to examine the finite sample performances of the methods proposed as well as the effect of different types of prior and different types of sampling design.
41,The conditional permutation test for independence while controlling for confounders,"Thomas B. Berrett, Yi Wang, Rina Foygel Barber, Richard J. Samworth",https://onlinelibrary.wiley.com/doi/10.1111/rssb.12340,"We propose a general new method, the conditional permutation test, for testing the conditional independence of variables X and Y given a potentially high dimensional random vector Z that may contain confounding factors. The test permutes entries of X non‐uniformly, to respect the existing dependence between X and Z and thus to account for the presence of these confounders. Like the conditional randomization test of Candès and co‐workers in 2018, our test relies on the availability of an approximation to the distribution of X|Z—whereas their test uses this estimate to draw new X‐values, for our test we use this approximation to design an appropriate non‐uniform distribution on permutations of the X‐values already seen in the true data. We provide an efficient Markov chain Monte Carlo sampler for the implementation of our method and establish bounds on the type I error in terms of the error in the approximation of the conditional distribution of X|Z, finding that, for the worst‐case test statistic, the inflation in type I error of the conditional permutation test is no larger than that of the conditional randomization test. We validate these theoretical results with experiments on simulated data and on the Capital Bikeshare data set."
42,Robust inference on population indirect causal effects: the generalized front door criterion,"Isabel R. Fulcher, Ilya Shpitser, Stella Marealle, Eric J. Tchetgen Tchetgen",https://onlinelibrary.wiley.com/doi/10.1111/rssb.12345,"Standard methods for inference about direct and indirect effects require stringent no‐unmeasured‐confounding assumptions which often fail to hold in practice, particularly in observational studies. The goal of the paper is to introduce a new form of indirect effect, the population intervention indirect effect, that can be non‐parametrically identified in the presence of an unmeasured common cause of exposure and outcome. This new type of indirect effect captures the extent to which the effect of exposure is mediated by an intermediate variable under an intervention that holds the component of exposure directly influencing the outcome at its observed value. The population intervention indirect effect is in fact the indirect component of the population intervention effect, introduced by Hubbard and Van der Laan. Interestingly, our identification criterion generalizes Judea Pearl's front door criterion as it does not require no direct effect of exposure not mediated by the intermediate variable. For inference, we develop both parametric and semiparametric methods, including a novel doubly robust semiparametric locally efficient estimator, that perform very well in simulation studies. Finally, the methods proposed are used to measure the effectiveness of monetary saving recommendations among women enrolled in a maternal health programme in Tanzania."
43,Multivariate type G Matérn stochastic partial differential equation random fields,"David Bolin, Jonas Wallin",https://onlinelibrary.wiley.com/doi/10.1111/rssb.12351,"For many applications with multivariate data, random‐field models capturing departures from Gaussianity within realizations are appropriate. For this reason, we formulate a new class of multivariate non‐Gaussian models based on systems of stochastic partial differential equations with additive type G noise whose marginal covariance functions are of Matérn type. We consider four increasingly flexible constructions of the noise, where the first two are similar to existing copula‐based models. In contrast with these, the last two constructions can model non‐Gaussian spatial data without replicates. Computationally efficient methods for likelihood‐based parameter estimation and probabilistic prediction are proposed, and the flexibility of the models suggested is illustrated by numerical examples and two statistical applications."
44,Rerandomization and regression adjustment,"Xinran Li, Peng Ding",https://onlinelibrary.wiley.com/doi/10.1111/rssb.12353,"Randomization is a basis for the statistical inference of treatment effects without strong assumptions on the outcome‐generating process. Appropriately using covariates further yields more precise estimators in randomized experiments. R. A. Fisher suggested blocking on discrete covariates in the design stage or conducting analysis of covariance in the analysis stage. We can embed blocking in a wider class of experimental design called rerandomization, and extend the classical analysis of covariance to more general regression adjustment. Rerandomization trumps complete randomization in the design stage, and regression adjustment trumps the simple difference‐in‐means estimator in the analysis stage. It is then intuitive to use both rerandomization and regression adjustment. Under the randomization inference framework, we establish a unified theory allowing the designer and analyser to have access to different sets of covariates. We find that asymptotically, for any given estimator with or without regression adjustment, rerandomization never hurts either the sampling precision or the estimated precision, and, for any given design with or without rerandomization, our regression‐adjusted estimator never hurts the estimated precision. Therefore, combining rerandomization and regression adjustment yields better coverage properties and thus improves statistical inference. To quantify these statements theoretically, we discuss optimal regression‐adjusted estimators in terms of the sampling precision and the estimated precision, and then measure the additional gains of the designer and the analyser. We finally suggest the use of rerandomization in the design and regression adjustment in the analysis followed by the Huber–White robust standard error."
45,Correction: ‘A new randomized response model’,"Lovleen Kumar Grover, Amanpreet Kaur",https://onlinelibrary.wiley.com/doi/10.1111/rssb.12341,"We point out a minor mistake in published in 2006, ‘A new randomized response model’, which as been cited by various researchers, though no one has pointed out the mistake."
