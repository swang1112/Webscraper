,title,author,link,abstract
0,Issue Information,no author,https://onlinelibrary.wiley.com/doi/10.1111/rssa.12480,no abstract
1,Multiple‐systems analysis for the quantification of modern slavery: classical and Bayesian approaches,Bernard W. Silverman,https://onlinelibrary.wiley.com/doi/10.1111/rssa.12505,"Multiple‐systems estimation is a key approach for quantifying hidden populations such as the number of victims of modern slavery. The UK Government published an estimate of 10000–13000 victims, constructed by the present author, as part of the strategy leading to the Modern Slavery Act 2015. This estimate was obtained by a stepwise multiple‐systems method based on six lists. Further investigation shows that a small proportion of the possible models give rather different answers, and that other model fitting approaches may choose one of these. Three data sets collected in the field of modern slavery, together with a data set about the death toll in the Kosovo conflict, are used to investigate the stability and robustness of various multiple‐systems‐estimate methods. The crucial aspect is the way that interactions between lists are modelled, because these can substantially affect the results. Model selection and Bayesian approaches are considered in detail, in particular to assess their stability and robustness when applied to real modern slavery data. A new Markov chain Monte Carlo Bayesian approach is developed; overall, this gives robust and stable results at least for the examples considered. The software and data sets are freely and publicly available to facilitate wider implementation and further research."
2,Gender differences in the perception of safety in public transport,"Laila Ait Bihi Ouali, Daniel J. Graham, Alexander Barron, Mark Trompet",https://onlinelibrary.wiley.com/doi/10.1111/rssa.12558,"Concerns over women's safety on public transport systems are commonly reported in the media. We develop statistical models to test for gender differences in the perception of safety and satisfaction on urban metros and buses by using large‐scale unique customer satisfaction data for 28 world cities over the period 2009–2018. Results indicate a significant gender gap in the perception of safety, with women being 10% more likely than men to feel unsafe in metros (6% for buses). This gender gap is larger for safety than for overall satisfaction (3% in metros and 2.5% in buses), which is consistent with safety being one dimension of overall satisfaction. Results are stable across specifications and robust to inclusion of city level and time controls. We find heterogeneous responses by sociodemographic characteristics. Data indicate that 45% of women feel secure in trains and metro stations (and 55% in buses). Thus the gender gap encompasses more differences in transport perception between men and women rather than an intrinsic network fear. Additional models test for the influence of metro characteristics on perceived safety levels and find that more acts of violence, larger carriages and emptier vehicles decrease women's feeling of safety."
3,Simple rules to guide expert classifications,"Jongbin Jung, Connor Concannon, Ravi Shroff, Sharad Goel, Daniel G. Goldstein",https://onlinelibrary.wiley.com/doi/10.1111/rssa.12576,"Judges, doctors and managers are among those decision makers who must often choose a course of action under limited time, with limited knowledge and without the aid of a computer. Because data‐driven methods typically outperform unaided judgements, resource‐constrained practitioners can benefit from simple, statistically derived rules that can be applied mentally. In this work, we formalize long‐standing observations about the efficacy of improper linear models to construct accurate yet easily applied rules. To test the performance of this approach, we conduct a large‐scale evaluation in 22 domains and focus in detail on one: judicial decisions to release or detain defendants while they await trial. In these domains, we find that simple rules rival the accuracy of complex prediction models that base decisions on considerably more information. Further, comparing with unaided judicial decisions, we find that simple rules substantially outperform the human experts. To conclude, we present an analytical framework that sheds light on why simple rules perform as well as they do."
4,A similarity‐based approach for macroeconomic forecasting,"Y. Dendramis, G. Kapetanios, M. Marcellino",https://onlinelibrary.wiley.com/doi/10.1111/rssa.12574,"In the aftermath of the recent financial crisis there has been considerable focus on methods for predicting macroeconomic variables when their behaviour is subject to abrupt changes, associated for example with crisis periods. We propose similarity‐based approaches as a way to handle parameter instability and apply them to macroeconomic forecasting. The rationale is that clusters of past data that match the current economic conditions can be more informative for forecasting than the entire past behaviour of the variable of interest. We apply our methods to predict both simulated data in a set of Monte Carlo experiments, and a broad set of key US macroeconomic indicators. The forecast evaluation exercises indicate that similarity‐based approaches perform well, in general, in comparison with other common time‐varying forecasting methods, and particularly well during crisis episodes."
5,Forecasting of cohort fertility under a hierarchical Bayesian approach,"Joanne Ellison, Erengul Dodd, Jonathan J. Forster",https://onlinelibrary.wiley.com/doi/10.1111/rssa.12566,"Fertility projections are a key determinant of population forecasts, which are widely used by government policy makers and planners. In keeping with the recent literature, we propose an intuitive and transparent hierarchical Bayesian model to forecast cohort fertility. Using Hamiltonian Monte Carlo methods and a data set from the human fertility database, we obtain fertility forecasts for 30 countries. We use scoring rules to assess the predictive accuracy of the forecasts quantitatively; these indicate that our model predicts with an accuracy comparable with that of the best‐performing models in the current literature overall, with stronger performance for countries without a recent structural shift. Our findings support the position of hierarchical Bayesian modelling at the forefront of population forecasting methods."
6,Fulfilling the information need after an earthquake: statistical modelling of citizen science seismic reports for predicting earthquake parameters in near realtime,Francesco Finazzi,https://onlinelibrary.wiley.com/doi/10.1111/rssa.12577,"When an earthquake affects an inhabited area, a need for information immediately arises among the population. In general, this need is not immediately fulfilled by official channels which usually release expert‐validated information with delays of many minutes. Seismology is among the research fields where citizen science projects succeeded in collecting useful scientific information. More recently, the ubiquity of smartphones is giving the opportunity to involve even more citizens. This paper focuses on seismic intensity reports collected through smartphone applications while an earthquake is occurring. The aim is to provide a framework for predicting and updating in near realtime earthquake parameters that are useful for assessing the effect of the earthquake. This is done by using a multivariate space–time model based on time‐varying coefficients and a spatial latent variable. As a case‐study, the model is applied to more than 200000 seismic reports globally collected over a period of around 4 years by the Earthquake Network citizen science project. It is shown how the time‐varying coefficients are needed to adapt the model to an information content that changes with time, and how the spatial latent variable can capture the local seismicity and the heterogeneity in the people's response across the globe."
7,How does temperature vary over time?: evidence on the stationary and fractal nature of temperature fluctuations,"John K. Dagsvik, Mariachiara Fortuna, Sigmund Hov Moen",https://onlinelibrary.wiley.com/doi/10.1111/rssa.12557,"The paper analyses temperature data from 96 selected weather stations world wide, and from reconstructed northern hemisphere temperature data over the last two millennia. Using a non‐parametric test, we find that the stationarity hypothesis is not rejected by the data. Subsequently, we investigate further properties of the data by means of a statistical model known as the fractional Gaussian noise (FGN) model. Under stationarity FGN follows from the fact that the observed data are obtained as temporal aggregates of data generated at a finer (basic) timescale where temporal aggregation is taken over a ‘large’ number of basic units. The FGN process exhibits long‐range dependence. Several tests show that both the reconstructed and most of the observed data are consistent with the FGN model."
8,Change point analysis of historical battle deaths,"Brennen T. Fagan, Marina I. Knight, Niall J. MacKay, A. Jamie Wood",https://onlinelibrary.wiley.com/doi/10.1111/rssa.12578,"It has been claimed and disputed that World War II has been followed by a ‘long peace’: an unprecedented decline of war. We conduct a full change point analysis of well‐documented, publicly available battle deaths data sets, using new techniques that enable the robust detection of changes in the statistical properties of such heavy‐tailed data. We first test and calibrate these techniques. We then demonstrate the existence of changes, independent of data presentation, in the early to mid‐19th century, as the Congress of Vienna system moved towards its collapse, in the early to mid‐20th century, bracketing the World Wars, and in the late 20th century, as the world reconfigured around the end of the Cold War. Our analysis provides a methodology for future investigations and an empirical basis for political and historical discussions."
9,Finding the strength in a weak instrument in a study of cognitive outcomes produced by Catholic high schools,"Siyu Heng, Dylan S. Small, Paul R. Rosenbaum",https://onlinelibrary.wiley.com/doi/10.1111/rssa.12559,"We show that the strength of an instrument is incompletely characterized by the proportion of compliers, and we propose and evaluate new methods that extract more information from certain settings with comparatively few compliers. Specifically, we demonstrate that, for a fixed small proportion of compliers, the presence of an equal number of always‐takers and never‐takers weakens an instrument, whereas the absence of always‐takers or, equivalently, the absence of never‐takers strengthens an instrument. In this statement, the strength of an instrument refers to its ability to recognize and reject a false hypothesis about a structural parameter. Equivalently, the strength of an instrument refers to its ability to exclude from a confidence interval a false value of a structural parameter. This ability is measured by the Bahadur efficiency of a test that assumes that the instrument is flawless, or the Bahadur efficiency of a sensitivity analysis that assumes that the instrument may be somewhat biased. When there are few compliers, the outcomes for most people are unaffected by fluctuations in the instrument, so most of the information about the treatment effect is contained in the tail of the distribution of the outcomes. Exploiting this fact, we propose new methods that emphasize the affected portion of the distribution of outcomes, thereby extracting more information from studies with few compliers. Studies of the effects of Catholic high schools on academic test performance have used ‘being Catholic’ as an instrument for ‘attending a Catholic high school’, and the application concerns such a comparison using the US National Educational Longitudinal Study. Most Catholics did not attend Catholic school, so there are few compliers, but it was rare for non‐Catholics to attend Catholic school, so there are very few always‐takers."
10,On quantifying expert opinion about multinomial models that contain covariates,"Fadlalla G. Elfadaly, Paul H. Garthwaite",https://onlinelibrary.wiley.com/doi/10.1111/rssa.12546,"The paper addresses the task of forming a prior distribution to represent expert opinion about a multinomial model that contains covariates. The task has not previously been addressed. We suppose that the sampling model is a multinomial logistic regression and represent expert opinion about the regression coefficients by a multivariate normal distribution. This logistic–normal model gives a flexible prior distribution that can capture a broad variety of expert opinion. The challenge is to find meaningful assessment tasks that an expert can perform and which should yield appropriate information to determine the values of parameters in the prior distribution, and to develop theory for determining the parameter values from the assessments. A method is proposed that meets this challenge. The method is implemented in interactive easy‐to‐use software that is freely available. It provides a graphical interface that the expert uses to assess quartiles of sets of proportions and the method determines a mean vector and a positive definite covariance matrix to represent the expert's opinions. The assessment tasks chosen yield parameter values that satisfy the usual laws of probability without the expert being aware of the constraints that this imposes. Special attention is given to feedback that encourages the expert to consider his or her opinions from a different perspective. The method is illustrated in an example that shows its viability and usefulness."
11,A novel approach to latent class modelling: identifying the various types of body mass index individuals,"Sarah Brown, William Greene, Mark Harris",https://onlinelibrary.wiley.com/doi/10.1111/rssa.12552,"Given the increasing prevalence of adult obesity, furthering understanding of the determinants of measures such as the body mass index (BMI) remains high on the policy agenda. We contribute to existing literature on modelling the BMI by proposing an extension to latent class modelling, which serves to unveil a more detailed picture of the determinants of BMI. Interest here lies in latent class analysis with a regression model and predictor variables explaining class membership, a regression model and predictor variables explaining the outcome variable within BMI classes and instances where the BMI classes are naturally ordered and labelled by expected values within class. A simple and generic way of parameterizing both the class probabilities and the statistical representation of behaviours within each class is proposed, that simultaneously preserves the ranking according to class‐specific expected values and yields a parsimonious representation of the class probabilities. Based on a wide range of metrics, the newly proposed approach is found to dominate the prevailing approach and, moreover, results are often quite different across the two."
12,Quantifying the association between discrete event time series with applications to digital forensics,"Christopher Galbraith, Padhraic Smyth, Hal S. Stern",https://onlinelibrary.wiley.com/doi/10.1111/rssa.12549,"We consider the problem of quantifying the degree of association between pairs of discrete event time series, with potential applications in forensic and cybersecurity settings. We focus in particular on the case where two associated event series exhibit temporal clustering such that the occurrence of one type of event at a particular time increases the likelihood that an event of the other type will also occur nearby in time. We pursue a non‐parametric approach to the problem and investigate various score functions to quantify association, including characteristics of marked point processes and summary statistics of interevent times. Two techniques are proposed for assessing the significance of the measured degree of association: a population‐based approach to calculating score‐based likelihood ratios when a sample from a relevant population is available, and a resampling approach to computing coincidental match probabilities when only a single pair of event series is available. The methods are applied to simulated data and to two real world data sets consisting of logs of computer activity and achieve accurate results across all data sets."
13,"A multilevel structural equation model for the interrelationships between multiple latent dimensions of childhood socio‐economic circumstances, partnership transitions and mid‐life health","Yajing Zhu, Fiona Steele, Irini Moustaki",https://onlinelibrary.wiley.com/doi/10.1111/rssa.12554,"We propose a multilevel structural equation model to investigate the interrelationships between childhood socio‐economic circumstances, partnership formation and stability, and mid‐life health, using data from the 1958 British birth cohort. The structural equation model comprises latent class models that characterize the patterns of change in four dimensions of childhood socio‐economic circumstances and a joint regression model that relates these categorical latent variables to partnership transitions in adulthood and mid‐life health, while allowing for informative dropout. The model can be extended to handle multiple outcomes of mixed types and at different levels in a hierarchical data structure."
14,Spatial confounding in hurdle multilevel beta models: the case of the Brazilian Mathematical Olympics for Public Schools,"João B. M. Pereira, Widemberg S. Nobre, Igor F. L. Silva, Alexandra M. Schmidt",https://onlinelibrary.wiley.com/doi/10.1111/rssa.12551,"Among the many disparities for which Brazil is known is the difference in performance across students who attend the three administrative levels of Brazilian public schools: federal, state and municipal. Our main goal is to investigate whether student performance in the Brazilian Mathematical Olympics for Public Schools is associated with school administrative level and student gender. For this, we propose a hurdle hierarchical beta model for the scores of students who took the examination in the second phase of these Olympics, in 2013. The mean of the beta model incorporates fixed and random effects at the student and school levels. We explore different distributions for the random school effect. As the posterior distributions of some fixed effects change in the presence, and distribution, of the random school effects, we also explore models that constrain random school effects to the orthogonal complement of the fixed effects. We conclude that male students perform slightly better than female students and that, on average, federal schools perform substantially better than state or municipal schools. However, some of the best municipal and state schools perform as well as some federal schools. We hypothesize that this is due to individual teachers who successfully motivate and prepare their students to perform well in the mathematical Olympics."
15,A placebo design to detect spillovers from an education–entertainment experiment in Uganda,"Anna M. Wilke, Donald P. Green, Jasper Cooper",https://onlinelibrary.wiley.com/doi/10.1111/rssa.12571,"Education–entertainment refers to dramatizations designed to convey information and to change attitudes. Buoyed by observational studies suggesting that education–entertainment strongly influences beliefs, attitudes and behaviours, scholars have recently assessed education–entertainment by using rigorous experimental designs in field settings. Studies conducted in developing countries have repeatedly shown the effectiveness of radio and film dramatizations on outcomes ranging from health to group conflict. One important gap in the literature is estimation of social spillover effects from those exposed to the dramatizations to others in the audience members’ social network. In theory, the social diffusion of media effects could greatly amplify their policy impact. The current study uses a novel placebo‐controlled design that gauges both the direct effects of the treatment on audience members as well as the indirect effects of the treatment on others in their family and in the community. We implement this design in two large cluster‐randomized experiments set in rural Uganda using video dramatizations on the topics of violence against women, teacher absenteeism and abortion stigma. We find several instances of sizable and highly significant direct effects on the attitudes of audience members, but we find little evidence that these effects diffused to others in the villages where the videos were aired."
16,Freight rates in downside and upside markets: pricing of own and spillover risks from other shipping segments,"Panayiotis Theodossiou, Dimitris Tsouknidis, Christos Savva",https://onlinelibrary.wiley.com/doi/10.1111/rssa.12553,"Shipping freight rates are notoriously volatile and shipping investors are perceived to be risk loving. The paper explores the stochastic properties of freight rates in the shipping industry and derives the analytical equations for their moments in downside and upside markets by using a two‐piece extension of the generalized error distribution. Pricing equations developed across shipping segments show how conditional risk and conditional skewness are priced along with their risk spillover effects. Results reveal the existence of a positive skewness premium, suggesting that shipping investors are willing to accept lower expected returns for the opportunity to earn high pay‐offs in the future."
17,Selecting a scale for spatial confounding adjustment,"Joshua P. Keller, Adam A. Szpiro",https://onlinelibrary.wiley.com/doi/10.1111/rssa.12556,"Unmeasured, spatially structured factors can confound associations between spatial environmental exposures and health outcomes. Adding flexible splines to a regression model is a simple approach for spatial confounding adjustment, but the spline degrees of freedom do not provide an easily interpretable spatial scale. We describe a method for quantifying the extent of spatial confounding adjustment in terms of the Euclidean distance at which variation is removed. We develop this approach for confounding adjustment with splines and using Fourier and wavelet filtering. We demonstrate differences in the spatial scales that these bases can represent and provide a comparison of methods for selecting the amount of confounding adjustment. We find the best performance for selecting the amount of adjustment by using an information criterion evaluated on an outcome model without exposure. We apply this method to spatial adjustment in an analysis of fine particulate matter and blood pressure in a cohort of US women."
18,New statistical metrics for multisite replication projects,"Maya B. Mathur, Tyler J. VanderWeele",https://onlinelibrary.wiley.com/doi/10.1111/rssa.12572,"Increasingly, researchers are attempting to replicate published original studies by using large, multisite replication projects, at least 134 of which have been completed or are on going. These designs are promising to assess whether the original study is statistically consistent with the replications and to reassess the strength of evidence for the scientific effect of interest. However, existing analyses generally focus on single replications; when applied to multisite designs, they provide an incomplete view of aggregate evidence and can lead to misleading conclusions about replication success. We propose new statistical metrics representing firstly the probability that the original study's point estimate would be at least as extreme as it actually was, if in fact the original study were statistically consistent with the replications, and secondly the estimated proportion of population effects agreeing in direction with the original study. Generalized versions of the second metric enable consideration of only meaningfully strong population effects that agree in direction, or alternatively that disagree in direction, with the original study. These metrics apply when there are at least 10 replications (unless the heterogeneity estimate 
            
            


τ
^

=
0

, in which case the metrics apply regardless of the number of replications). The first metric assumes normal population effects but appears robust to violations in simulations; the second is distribution free. We provide R packages (Replicate and MetaUtility)."
19,Longevity forecasting by socio‐economic groups using compositional data analysis,"S⊘ren Kjærgaard, Yunus Emre Ergemen, Marie‐Pier Bergeron‐Boucher, Jim Oeppen, Malene Kallestrup‐Lamb",https://onlinelibrary.wiley.com/doi/10.1111/rssa.12555,"Several Organisation for Economic Co‐operation and Development countries have recently implemented an automatic link between the statutory retirement age and life expectancy for the total population to ensure sustainability in their pension systems due to increasing life expectancy. As significant mortality differentials are observed across socio‐economic groups, future changes in these differentials will determine whether some socio‐economic groups drive increases in the retirement age, leaving other groups with fewer pensionable years. We forecast life expectancy by socio‐economic groups and compare the forecast performance of competing models by using Danish mortality data and find that the most accurate model assumes a common mortality trend. Life expectancy forecasts are used to analyse the consequences of a pension system where the statutory retirement age is increased when total life expectancy is increasing."
20,Multilevel network meta‐regression for population‐adjusted treatment comparisons,"David M. Phillippo, Sofia Dias, A. E. Ades, Mark Belger, Alan Brnabic, Alexander Schacht, Daniel Saure, Zbigniew Kadziola, Nicky J. Welton",https://onlinelibrary.wiley.com/doi/10.1111/rssa.12579,"Standard network meta‐analysis (NMA) and indirect comparisons combine aggregate data from multiple studies on treatments of interest, assuming that any effect modifiers are balanced across populations. Population adjustment methods relax this assumption using individual patient data from one or more studies. However, current matching‐adjusted indirect comparison and simulated treatment comparison methods are limited to pairwise indirect comparisons and cannot predict into a specified target population. Existing meta‐regression approaches incur aggregation bias. We propose a new method extending the standard NMA framework. An individual level regression model is defined, and aggregate data are fitted by integrating over the covariate distribution to form the likelihood. Motivated by the complexity of the closed form integration, we propose a general numerical approach using quasi‐Monte‐Carlo integration. Covariate correlation structures are accounted for by using copulas. Crucially for decision making, comparisons may be provided in any target population with a given covariate distribution. We illustrate the method with a network of plaque psoriasis treatments. Estimated population‐average treatment effects are similar across study populations, as differences in the distributions of effect modifiers are small. A better fit is achieved than a random effects NMA, uncertainty is substantially reduced by explaining within‐ and between‐study variation, and estimates are more interpretable."
21,Temporal disaggregation of overlapping noisy quarterly data: estimation of monthly output from UK value‐added tax data,"Paul Labonne, Martin Weale",https://onlinelibrary.wiley.com/doi/10.1111/rssa.12568,"The paper derives monthly estimates of business sector output in the UK from rolling quarterly value‐added tax based turnover data. The administrative nature of the value‐added tax data implies that their use could ultimately yield a more precise and granular picture of output across the economy. However, they show two particular features which complicate their exploitation: they are overlapping and subject to substantial noise. This motivates our choice of a multivariate unobserved components model for filtering and disaggregating temporally the aggregate figures. After illustrating our method by using one industry as a case‐study, we estimate monthly seasonally adjusted gross output figures for the 75 industries for which the data are available. Our results show material differences from the existing output profile."
22,Model‐based clustering and analysis of life history data,"Marc A. Scott, Kaushik Mohan, Jacques‐Antoine Gauthier",https://onlinelibrary.wiley.com/doi/10.1111/rssa.12575,"Methods and models for longitudinal data with categorical, multi‐dimensional outcomes are quite limited, but they are essential to the study of life histories. For example, in the Swiss Household Panel, information on the co‐residence and professional status of several thousand individuals is available through to age 45 years. Interest centres on the time and order of life course events such as having children and working full or part time and the duration of the phases that they delineate. With data of this type, optimal matching and clustering algorithms relying on a distance metric or parametric models of duration in a competing risks framework are used; the appropriateness of each derives from competing goals and orientation. We prefer model‐based approaches when certain goals are paramount: simulation of individual trajectories; adjusting for time‐dependent covariates; handling multistate trajectories and missing outcomes. Several of these goals are particularly challenging when the number of states is of moderate size, and many transitions are infrequent and/or time inhomogeneous. Using the Swiss Household Panel, we demonstrate the appropriateness of latent class growth curve models for analysing sequence data. In particular, models including heterogeneous dependence structure provide new techniques for assessing goodness of fit as well as yield insights into social processes."
23,A causal inference framework for cancer cluster investigations using publicly available data,"Rachel C. Nethery, Yue Yang, Anna J. Brown, Francesca Dominici",https://onlinelibrary.wiley.com/doi/10.1111/rssa.12567,"Often, a community becomes alarmed when high rates of cancer are noticed, and residents suspect that the cancer cases could be caused by a known source of hazard. In response, the US Centers for Disease Control and Prevention recommend that departments of health perform a standardized incidence ratio (SIR) analysis to determine whether the observed cancer incidence is higher than expected. This approach has several limitations that are well documented in the existing literature. We propose a novel causal inference framework for cancer cluster investigations, rooted in the potential outcomes framework. Assuming that a source of hazard representing a potential cause of increased cancer rates in the community is identified a priori, we focus our approach on a causal inference estimand which we call the causal SIR. The causal SIR is a ratio defined as the expected cancer incidence in the exposed population divided by the expected cancer incidence for the same population under the (counterfactual) scenario of no exposure. To estimate the causal SIR we need to overcome two main challenges: first, we must identify unexposed populations that are as similar as possible to the exposed population to inform estimation of the expected cancer incidence under the counterfactual scenario of no exposure, and, second, publicly available data on cancer incidence for these unexposed populations are often available at a much higher level of spatial aggregation (e.g. county) than what is desired (e.g. census block group). We overcome the first challenge by relying on matching. We overcome the second challenge by building a Bayesian hierarchical model that borrows information from other sources to impute cancer incidence at the desired level of spatial aggregation. In simulations, our statistical approach was shown to provide dramatically improved results, i.e. less bias and better coverage, than the current approach to SIR analyses. We apply our proposed approach to investigate whether trichloroethylene vapour exposure has caused increased cancer incidence in Endicott, New York."
24,A functional approach to small area estimation of the relative median poverty gap,"Enrico Fabrizi, Maria Rosaria Ferrante, Carlo Trivisano",https://onlinelibrary.wiley.com/doi/10.1111/rssa.12562,"We consider the estimation of the relative median poverty gap (RMPG) at the level of Italian provinces by using data from the European Union Survey on Income and Living Conditions. The overall sample size does not allow reliable estimation of income‐distribution‐related parameters at the provincial level; therefore, small area estimation techniques must be used. The specific challenge in estimating the RMPG is that, as it summarizes the income distribution of the poor, samples for estimating it for small subpopulations are even smaller than those available in other parameters. We propose a Bayesian strategy where various parameters summarizing the distribution of income at the provincial level are modelled by means of a multivariate small area model. To estimate the RMPG, we relate these parameters to a distribution describing income, namely the generalized beta distribution of the second kind. Posterior draws from the multivariate model are then used to generate draws for the distribution's area‐specific parameters and then of the RMPG defined as their functional."
25,Improving external validity of epidemiologic cohort analyses: a kernel weighting approach,"Lingxiao Wang, Barry I. Graubard, Hormuzd A. Katki, and Yan Li",https://onlinelibrary.wiley.com/doi/10.1111/rssa.12564,"For various reasons, cohort studies generally forgo probability sampling required to obtain population representative samples. However, such cohorts lack population representativeness, which invalidates estimates of population prevalences for novel health factors that are only available in cohorts. To improve external validity of estimates from cohorts, we propose a kernel weighting (KW) approach that uses survey data as a reference to create pseudoweights for cohorts. A jackknife variance is proposed for the KW estimates. In simulations, the KW method outperformed two existing propensity‐score‐based weighting methods in mean‐squared error while maintaining confidence interval coverage. We applied all methods to estimating US population mortality and prevalences of various diseases from the non‐representative US National Institutes of Health–American Association of Retired Persons cohort, using the sample from the US‐representative National Health Interview Survey as the reference. Assuming that the survey estimates are correct, the KW approach yielded generally less biased estimates compared with the existing propensity‐score‐based weighting methods."
26,"T. M. F. Smith, 1934–2019; Harvey Goldstein, 1939–2020; Allan Henry Seheult, 1942–2019;John Francis Bithell, 1939–2020; M. H. A. Davis, 1945–2020",no author,https://onlinelibrary.wiley.com/doi/10.1111/rssa.12580,no abstract
27,Book reviews,no author,https://onlinelibrary.wiley.com/doi/10.1111/rssa.12585,no abstract
28,Issue Information,no author,https://onlinelibrary.wiley.com/doi/10.1111/rssa.12479,no abstract
29,Knowing the signs: a direct and generalizable motivation of two‐sided tests,"Kenneth Rice, Tyler Bonnett, Chloe Krakauer",https://onlinelibrary.wiley.com/doi/10.1111/rssa.12496,"Many well‐known problems with two‐sided p‐values are due to their use in hypothesis tests, with ‘reject–accept’ conclusions about point null hypotheses. We present an alternative motivation for p‐value‐based tests, viewing them as assessments of only the sign of an underlying parameter, where we can conclude that the parameter is positive or negative, or simply say nothing either way. Our approach is decision theoretic, but—unusually—we consider the whole set of possible utility functions available. Doing this we show how, in a specific sense, close analogues of familiar one‐ and two‐sided tests are always the optimal decision. We argue that this simplicity could aid non‐experts’ understanding and use of tests—and help them to think critically about whether or not tests are appropriate tools for answering their questions of interest. Several extensions are also considered, showing that the simple idea of determining the signs of parameters yields a rich framework for inference."
30,A new standard for the analysis and design of replication studies,Leonhard Held,https://onlinelibrary.wiley.com/doi/10.1111/rssa.12493,"A new standard is proposed for the evidential assessment of replication studies. The approach combines a specific reverse Bayes technique with prior‐predictive tail probabilities to define replication success. The method gives rise to a quantitative measure for replication success, called the sceptical p‐value. The sceptical p‐value integrates traditional significance of both the original and the replication study with a comparison of the respective effect sizes. It incorporates the uncertainty of both the original and the replication effect estimates and reduces to the ordinary p‐value of the replication study if the uncertainty of the original effect estimate is ignored. The framework proposed can also be used to determine the power or the required replication sample size to achieve replication success. Numerical calculations highlight the difficulty of achieving replication success if the evidence from the original study is only suggestive. An application to data from the Open Science Collaboration project on the replicability of psychological science illustrates the methodology proposed."
31,Discussion on the meeting on ‘Signs and sizes:understanding and replicating statistical findings’,no author,https://onlinelibrary.wiley.com/doi/10.1111/rssa.12544,no abstract
32,Identifying the effect of public holidays on daily demand for gas,"Sarah E. Heaps, Malcolm Farrow, Kevin J. Wilson",https://onlinelibrary.wiley.com/doi/10.1111/rssa.12504,"To reduce operational costs and to ensure security of supply, gas distribution networks require accurate forecasts of the demand for gas. Among domestic and commercial customers, demand relates primarily to the weather and patterns of life and work. Public holidays have a pronounced effect which often spreads into neighbouring days. We call this spread the ‘proximity effect’. Traditionally, the days over which the proximity effect is felt are prespecified in fixed windows around each holiday, allowing no uncertainty in their identification. We are motivated by an application to modelling daily gas demand in two large British regions. We introduce a novel model which does not fix the days on which the proximity effect is felt. Our approach uses a four‐state, non‐homogeneous hidden Markov model, with cyclic dynamics, where the classification of days as public holidays is observed, but the assignment of days as ‘pre‐holiday’, ‘post‐holiday’ or ‘normal’ days is unknown. The number of days to the preceding and succeeding holidays guide transitions between states. We apply Bayesian inference and illustrate the benefit of our modelling approach. A version of the model is now being used by one of the UK's regional distribution networks."
33,Modelling the socio‐economic determinants of fertility: a mediation analysis using the parametric g‐formula,"Maarten J. Bijlsma, Ben Wilson",https://onlinelibrary.wiley.com/doi/10.1111/rssa.12520,"Theories predict that the timing of childbearing and number of children born are determined by multiple socio‐economic factors. Despite this, many methods cannot investigate the interrelationships between these determinants, including the direct and indirect influence that they have on fertility over the life course. Here we use the parametric g‐formula to examine the interdependent influences of time‐varying socio‐economic processes—education, employment status and partnership status—on fertility. To demonstrate this approach, we study a cohort of women who were born in the UK in 1970. Our results show that socio‐economic processes play an important role in determining fertility, not only directly but also indirectly. We show that increasing attendance in higher education has a largely direct effect on early childbearing up to age 25 years, resulting in a substantial increase in childlessness. However, childbearing at later ages is dominated by an indirect effect of education on fertility, via partnership status and employment status, that is twice as large as the direct effect. We also use the g‐formula to examine bias due to unobserved heterogeneity, and we demonstrate that our results appear to be robust. We conclude that the method provides a valuable tool for mediation analysis in studies of interdependent life course processes."
34,A quantitative framework to inform extrapolation decisions in children,"Ian Wadsworth, Lisa V. Hampson, Thomas Jaki, Graeme J. Sills, Anthony G. Marson, Richard Appleton",https://onlinelibrary.wiley.com/doi/10.1111/rssa.12532,"When developing a new medicine for children, the potential to extrapolate from adult efficacy data is well recognized. However, significant assumptions about the similarity of adults and children are needed for extrapolations to be biologically plausible. One such assumption is that of similar exposure–response (E–R‐) relationships. Motivated by applications to antiepileptic drug development, we consider how data that are available from existing trials of adults and adolescents can be used to quantify prior uncertainty about whether E–R‐relationships are similar in adults and younger children. A Bayesian multivariate meta‐analytic model is fitted to existing E–R‐data and adjusted for external biases that arise because these data are not perfectly relevant to the comparison of interest. We propose a strategy for eliciting expert prior opinion on external biases. From the bias‐adjusted meta‐analysis, we derive prior distributions quantifying our uncertainty about the degree of similarity between E–R‐relationships for adults and younger children. Using these we calculate the prior probability that average pharmacodynamic responses in adults and younger children, both on placebo and at an effective concentration, are sufficiently similar to justify a complete extrapolation of efficacy data. A simulation study is performed to evaluate the operating characteristics of the approach proposed."
35,A Bayesian spatial categorical model for prediction to overlapping geographical areas in sample surveys,"K. Shuvo Bakar, Nicholas Biddle, Philip Kokic, Huidong Jin",https://onlinelibrary.wiley.com/doi/10.1111/rssa.12526,"Motivated by the Australian National University poll, we consider a situation where survey data have been collected from respondents for several categorical variables and a primary geographic classification, e.g. postcode. Here, a common and important problem is to obtain estimates for a second target geography that overlaps with the primary geography but has not been collected from the respondents. We examine this problem when areal level census information is available for both geographic classifications. Such a situation is challenging from a small area estimation perspective for several reasons: there is a misalignment between the census and survey information as well as the geographical classifications; the geographic areas are potentially small and so prediction can be difficult because of the sparse or spatially missing data issue; and there is the possibility of non‐stationary spatial dependence. To address these problems we develop a Bayesian model using latent processes, underpinned by a non‐stationary spatial basis that combines Moran's I and multiresolution basis functions with a small but representative set of knots. The study results based on simulated data demonstrate that the model can be highly effective and gives more accurate estimates for areas defined by the target geography than several existing models. The model also performs well for the Australian National University poll data to predict on a second geographic classification: statistical area level 2."
36,The hot hand in professional darts,"Marius Ötting, Roland Langrock, Christian Deutscher, Vianey Leos‐Barajas",https://onlinelibrary.wiley.com/doi/10.1111/rssa.12527,"We investigate the hot hand hypothesis in professional darts in a nearly ideal setting with minimal to no interaction between players. Considering almost 1 year of tournament data, corresponding to 167492 dart throws in total, we use state space models to investigate serial dependence in throwing performance. In our models, a latent state process serves as a proxy for a player's underlying form, and we use auto‐regressive processes to model how this process evolves over time. Our results regarding the persistence of the latent process indicate a weak hot hand effect, but the evidence is inconclusive."
37,Information asymmetry and leverage adjustments: a semiparametric varying‐coefficient approach,"Man Jin, Shunan Zhao, Subal C. Kumbhakar",https://onlinelibrary.wiley.com/doi/10.1111/rssa.12524,"Information asymmetry reflects the risk and uncertainty faced by investors and is a measure of a firm's transparency. High information asymmetry could increase the cost of external financing, which in turn impedes a firm's leverage (debt–asset ratio) adjustment. The paper studies the adjustment speed towards the target leverage in the presence of information asymmetry by using microlevel data from China. In contrast with previous studies, we allow heterogeneity in the adjustment speed coefficient by modelling it as a non‐parametric function of information asymmetry and other firm characteristics. This refinement not only allows for more flexibility in the model, but it also facilitates further exploration into the differences and determinants of firms’ financing behaviour. We uniquely build the firm level measure of information asymmetry into the traditional partial leverage adjustment framework. Based on our firm level measure of the adjustment speed, our paper explores why the leverage adjustment speed matters by examining its association with corporate performance indicators. We find that China's firms do have leverage targets and they slowly adjust towards these targets. We also find that the adjustment speed decreases with an increase in information asymmetry. Overall, firms which converge towards their targets faster perform better in value, profitability, investment and costs."
38,A Bayesian parametric approach to handle missing longitudinal outcome data in trial‐based health economic evaluations,"Andrea Gabrio, Michael J. Daniels, Gianluca Baio",https://onlinelibrary.wiley.com/doi/10.1111/rssa.12522,"Trial‐based economic evaluations are typically performed on cross‐sectional variables, derived from the responses for only the completers in the study, using methods that ignore the complexities of utility and cost data (e.g. skewness and spikes). We present an alternative and more efficient Bayesian parametric approach to handle missing longitudinal outcomes in economic evaluations, while accounting for the complexities of the data. We specify a flexible parametric model for the observed data and partially identify the distribution of the missing data with partial identifying restrictions and sensitivity parameters. We explore alternative non‐ignorable missingness scenarios through different priors for the sensitivity parameters, calibrated on the observed data. Our approach is motivated by, and applied to, data from a trial assessing the cost‐effectiveness of a new treatment for intellectual disability and challenging behaviour."
39,Inferring the outcomes of rejected loans: an application of semisupervised clustering,"Zhiyong Li, Xinyi Hu, Ke Li, Fanyin Zhou, Feng Shen",https://onlinelibrary.wiley.com/doi/10.1111/rssa.12534,"Rejection inference aims to reduce sample bias and to improve model performance in credit scoring. We propose a semisupervised clustering approach as a new rejection inference technique. K‐prototype clustering can deal with mixed types of numeric and categorical characteristics, which are common in consumer credit data. We identify homogeneous acceptances and rejections and assign labels to part of the rejections according to the label of acceptances. We test the performance of various rejection inference methods in logit, support vector machine and random‐forests models based on data sets of real consumer loans. The predictions of clustering rejection inference show advantages over other traditional rejection inference methods. Inferring the label of the rejection from semisupervised clustering is found to help to mitigate the sample bias problem and to improve the predictive accuracy."
40,Crime against women in India: unveiling spatial patterns and temporal trends of dowry deaths in the districts of Uttar Pradesh,"G. Vicente, T. Goicoa, P. Fernandez‐Rasines, M. D. Ugarte",https://onlinelibrary.wiley.com/doi/10.1111/rssa.12545,"Crimes against women in India have been continuously increasing lately as reported by the National Crime Records Bureau. Gender‐based violence has become a serious issue to such an extent that it has been catalogued as a high impact health problem by the World Health Organization. However, there is a lack of spatiotemporal analyses to reveal a complete picture of the geographical and temporal patterns of crimes against women. We focus on analysing how the geographical pattern of ‘dowry deaths’ changes over time in the districts of Uttar Pradesh during the period 2001–2014. The study of the geographical distribution of dowry death incidence and its evolution over time aims to identify specific regions that exhibit high risks and to hypothesize on potential risk factors. We also look into different spatial priors and their effects on final risk estimates. Various priors for the hyperparameters are also reviewed. The risk estimates seem to be robust in terms of the spatial prior and hyperprior choices and final results highlight several districts with extreme risks of dowry death incidence. Statistically significant associations are also found between dowry deaths, sex ratio and some forms of overall crime."
41,Can genetics reveal the causes and consequences of educational attainment?,"Marcus Munafò, Neil M. Davies, George Davey Smith",https://onlinelibrary.wiley.com/doi/10.1111/rssa.12543,"There is an extensive literature on the causes of educational inequalities, and the life course consequences of educational attainment. Mendelian randomization, where genetic variants associated with exposures of interest are used as proxies for those exposures, often within an instrumental variables framework, has proven highly effective at elucidating the causal effects of several risk factors in the biomedical sciences. We discuss the potential for this approach to be used in the context of social and socio‐economic exposures and outcomes, such as educational attainment."
42,Issue Information,no author,https://onlinelibrary.wiley.com/doi/10.1111/rssa.12478,no abstract
43,The effect of general health checks on healthcare utilization: accounting for self‐selection bias,"Sungwook Yoon, Duk Bin Jun, Sungho Park",https://onlinelibrary.wiley.com/doi/10.1111/rssa.12482,"The general health check is one of the most common preventive healthcare measures in many countries. In this study, we propose an empirical approach which jointly models the decision to obtain a general health check and healthcare utilization, tackling the self‐selection problem by using eligibility to obtain a health check for free as an instrumental variable. Eligibility has some exogenous variations by design and this helps us to partial out the effect of general health checks from self‐selection biases. We apply the model to a large 12‐year panel data set provided by the Korean National Health Insurance Service. We find that participation in the general health check increases healthcare utilization and ignored self‐selection generates substantial upward bias in the estimates. We also find that the health check effect shows noteworthy heterogeneity across gender and income groups. Before health checks, healthcare utilization of males and people in low income groups is lower than those of females and people in high income groups respectively. However, these become comparable across different groups after health checks. This finding implies that general health checks can be an effective vehicle for health equity."
44,Improved secondary analysis of linked data: a framework and an illustration,"Ray Chambers, Andrea Diniz da Silva",https://onlinelibrary.wiley.com/doi/10.1111/rssa.12477,"Applications that use linked data are now part of mainstream social science research, though they generally do not take linkage error into consideration. Solutions that correct for the bias caused by these errors have been proposed but are not yet embedded in the various analysis procedures in common use. Secondary analyses based on linked data can therefore be potentially misleading. We review some recent approaches to non‐deterministic data linkage together with a framework for secondary analysis of the linked data which makes use of paradata produced by the linkage process to correct this bias. We also describe a new method for secondary analysis of linked data that builds on this framework and show how it can be used for estimation of a set of domain means based on linked data. We then illustrate this approach via an empirical study based on record linkage of agricultural producers in four states of Brazil aimed at producing estimates of agricultural output by industry. Our study considers register‐to‐register linkage as well as sample‐to‐register linkage, and we show results for the traditional Fellegi–Sunter approach to record linkage as well as for a newer linkage procedure based on the use of classification trees and bagging."
45,A data‐driven supply‐side approach for estimating cross‐border Internet purchases within the European Union,"Q. A. Meertens, C. G. H. Diks, H. J. van den Herik, F. W. Takes",https://onlinelibrary.wiley.com/doi/10.1111/rssa.12487,"The digital economy is a highly relevant item on the European Union's policy agenda. We focus on cross‐border Internet purchases, as part of the digital economy, the total value of which cannot be accurately estimated by using existing consumer survey approaches. In fact, they lead to a serious underestimation. To obtain an accurate estimate, we propose a three‐step data‐driven approach based on supply‐side data. For the first step, we develop a data‐driven generic method for firm level probabilistic record linkage of tax data and business registers. In the second step, we use machine learning to identify webshops based on website data. Then, in the third step, we implement recently developed bias correction techniques that have hitherto been overlooked by the machine learning community. Subsequently, we claim that our three‐step approach can be applied to any European Union member state, leading to more accurate estimates of cross‐border Internet purchases than those obtained by currently existing approaches. To justify the claim, we apply our approach to the Netherlands for the year 2016 and find an estimate that is six times as high as current estimates, having a standard deviation of 8%. Hence, we may conclude that our new approach deserves more investigation and applications."
46,UK regional nowcasting using a mixed frequency vector auto‐regressive model with entropic tilting,"Gary Koop, Stuart McIntyre, James Mitchell",https://onlinelibrary.wiley.com/doi/10.1111/rssa.12491,"Output growth data for the UK regions are available at only annual frequency and are released with significant delay. Regional policy makers would benefit from more frequent and timely data. We develop a stacked, mixed frequency vector auto‐regression to provide, each quarter, nowcasts of annual output growth for the UK regions. The information that we use to update our regional nowcasts includes output growth data for the UK as a whole, as these aggregate data are released in a more timely and frequent (quarterly) fashion than the regional disaggregates which they comprise. We show how entropic tilting methods can be adapted to exploit the restriction that UK output growth is a weighted average of regional growth. In our realtime nowcasting application we find that the stacked mixed frequency vector‐autoregressive model, with entropic tilting, provides an effective means of nowcasting the regional disaggregates exploiting known information on the aggregate."
47,Data‐driven transformations in small area estimation,"Natalia Rojas‐Perilla, Sören Pannier, Timo Schmid, Nikos Tzavidis",https://onlinelibrary.wiley.com/doi/10.1111/rssa.12488,"Small area models typically depend on the validity of model assumptions. For example, a commonly used version of the empirical best predictor relies on the Gaussian assumptions of the error terms of the linear mixed regression model: a feature rarely observed in applications with real data. The paper tackles the potential lack of validity of the model assumptions by using data‐driven scaled transformations as opposed to ad hoc chosen transformations. Different types of transformations are explored, the estimation of the transformation parameters is studied in detail under the linear mixed regression model and transformations are used in small area prediction of linear and non‐linear parameters. The use of scaled transformations is crucial as it enables fitting the linear mixed regression model with standard software and hence it simplifies the work of the data analyst. Mean‐squared error estimation that accounts for the uncertainty due to the estimation of the transformation parameters is explored by using the parametric and semiparametric (wild) bootstrap. The methods proposed are illustrated by using real survey and census data for estimating income deprivation parameters for municipalities in the Mexican state of Guerrero. Simulation studies and the results from the application show that using carefully selected, data‐driven transformations can improve small area estimation."
48,Tracking the evolution of literary style via Dirichlet–multinomial change point regression,Gordon J. Ross,https://onlinelibrary.wiley.com/doi/10.1111/rssa.12492,"It is typical in stylometry to assume that authors have a unique writing style which is common to all their published writings and is constant over time. Based on this assumption, statistical techniques can be used to answer literary questions, such as authorship attribution, in a quantitative manner. However, the claim that authors have a constant literary style has not received much investigation or validation. We propose a collection of statistical models based on Dirichlet–multinomial change point regression which can capture the evolution of writing style over time, including both gradual changes in style as the author matures, and abrupt changes which can be caused by extreme events in the author's life. To illustrate our framework, we study the literary output of the celebrated British author Sir Terry Pratchett, who was tragically diagnosed with Alzheimer's disease during the last years of his life. Contrary to the usual assumptions made in stylometry, we find evidence of both gradual changes in style over his lifetime, and an abrupt change which corresponds to his Alzheimer's diagnosis. We also investigate the published writings of Agatha Christie, who is also rumoured to have suffered from Alheizmer's disease towards the end of her life, and find evidence of gradual drift, but no corresponding abrupt change. The implications for stylometry and authorship attribution are discussed."
49,Spatial hedonic modelling adjusted for preferential sampling,"Lucia Paci, Alan E. Gelfand, and María Asunción Beamonte, Pilar Gargallo, Manuel Salvador",https://onlinelibrary.wiley.com/doi/10.1111/rssa.12489,"Hedonic models are widely used to predict selling prices of properties. Originally, they were proposed as simple spatial regressions, i.e. a spatially referenced response regressed on spatially referenced predictors. Subsequently, spatial random effects were introduced to serve as surrogates for unmeasured or unobservable predictors and were shown to provide better out‐of‐sample prediction. However, what has been ignored in the literature is the fact that the locations (and times) of the sales are random and, in fact, are an observation of a random point pattern. Here, we first consider whether there is stochastic dependence between the point pattern of locations and the set of responses. If so, a second question is whether incorporating a log‐intensity for the point pattern of locations in the hedonic modelling enables improvement in the prediction of selling price. We connect this problem to what is referred to as preferential sampling. Through model comparison we illuminate the role of the point pattern data in the prediction of selling price. Using two different years of property sales from Zaragoza, Spain, we employ both the full database as well as an intentionally biased subset to elaborate this story."
50,Adjusting trial results for biases in meta‐analysis: combining data‐based evidence on bias with detailed trial assessment,"K. M. Rhodes, J. Savović, R. Elbers, H. E. Jones, J. P. T. Higgins, J. A. C. Sterne, N. J. Welton, R. M. Turner",https://onlinelibrary.wiley.com/doi/10.1111/rssa.12485,"Flaws in the conduct of randomized trials can lead to biased estimation of the intervention effect. Methods for adjustment of within‐trial biases in meta‐analysis include the use of empirical evidence from an external collection of meta‐analyses, and the use of expert opinion informed by the assessment of detailed trial information. Our aim is to present methods to combine these two approaches to gain the advantages of both. We make use of the risk of bias information that is routinely available in Cochrane reviews, by obtaining empirical distributions for the bias associated with particular bias profiles (combinations of risk of bias judgements). We propose three methods: a formal combination of empirical evidence and opinion in a Bayesian analysis; asking experts to give an opinion on bias informed by both summary trial information and a bias distribution from the empirical evidence, either numerically or by selecting areas of the empirical distribution. The methods are demonstrated through application to two example binary outcome meta‐analyses. Bias distributions based on opinion informed by trial information alone were most dispersed on average, and those based on opinions obtained by selecting areas of the empirical distribution were narrowest. Although the three methods for combining empirical evidence with opinion vary in ease and speed of implementation, they yielded similar results in the two examples."
51,"Productivity, infrastructure and urban density—an allometric comparison of three European city regions across scales","Hadi Arbabi, Martin Mayfield, Philip McCann",https://onlinelibrary.wiley.com/doi/10.1111/rssa.12490,"Agglomeration‐based arguments citing Dutch and German city regions have been a primary driver in advocating intercity transport strategies in the north of England. We adopt an allometric urban model investigating the applicability and transferability of these transport‐led agglomerative strategies promoted to address England's regional economic underperformance. This is undertaken through a comparative study of the size–cost performance balance of three city regions and the overall urban networks in the Netherlands and Germany, and England and Wales by using city units defined at different spatial scales. Although our results support a case for better mobility and transport comparing the three urban networks regardless of the spatial scales, comparisons of specific city regions indicate a more nuanced interplay of productivity, mobility infrastructure and urban density."
52,Parametric modelling of M‐quantile regression coefficient functions with application to small area estimation,"Paolo Frumento, Nicola Salvati",https://onlinelibrary.wiley.com/doi/10.1111/rssa.12495,"Small area estimation methods can be used to obtain reliable estimates of a parameter of interest within an unplanned domain or subgroup of the population for which only a limited sample size is available. A standard approach to small area estimation is to use a linear mixed model in which the heterogeneity between areas is accounted for by area level effects. An alternative solution, which has gained popularity in recent years, is to use M‐quantile regression models. This approach requires much weaker assumptions than the standard linear mixed model and enables computing outlier robust estimators of the area means. We introduce a new framework for M‐quantile regression, in which the model coefficients, β(τ), are described by (flexible) parametric functions of τ. We illustrate the advantages of this approach and its application to small area estimation. Using the European Union Survey on Income and Living Conditions data, we estimate the average equivalized household income in three Italian regions. The paper is accompanied by an R package Mqrcm that implements the necessary procedures for estimation, inference and prediction."
53,‘What drives commuter behaviour?': a Bayesian clustering approach for understanding opposing behaviours in social surveys,"Laura C. Dawkins, Daniel B. Williamson, Stewart W. Barr, Sally R. Lampkin",https://onlinelibrary.wiley.com/doi/10.1111/rssa.12499,"The city of Exeter, UK, is experiencing unprecedented growth, putting pressure on traffic infrastructure. As well as traffic network management, understanding and influencing commuter behaviour is important for reducing congestion. Information about current commuter behaviour has been gathered through a large on‐line survey, and similar individuals have been grouped to explore distinct behaviour profiles to inform intervention design to reduce commuter congestion. Statistical analysis within societal applications benefit from incorporating available social scientist expert knowledge. Current clustering approaches for the analysis of social surveys assume that the number of groups and the within‐group narratives are unknown a priori. Here, however, informed by valuable expert knowledge, we develop a novel Bayesian approach for creating a clear opposing transport mode group narrative within survey respondents, simplifying communication with project partners and the general public. Our methodology establishes groups characterizing opposing behaviours based on a key multinomial survey question by constraining parts of our prior judgement within a Bayesian finite mixture model. Drivers of group membership and within‐group behavioural differences are modelled hierarchically by using further information from the survey. In applying the methodology we demonstrate how it can be used to understand the key drivers of opposing behaviours in any wider application."
54,Estimation of proportions in small areas: application to the labour force using the Swiss Census Structural Survey,"Isabel Molina, Ewa Strzalkowska‐Kominiak",https://onlinelibrary.wiley.com/doi/10.1111/rssa.12498,"The main objectives of this paper are to find efficient but computationally simple estimators for the proportions of people in the labour force (economic activity rates) in Swiss communes and to estimate their mean‐squared error (MSE) over the sampling replication mechanism (the design MSE). This will be done by combining survey data with administrative data provided by the Swiss Federal Statistical Office. We find estimators with considerably greater efficiency than currently used direct estimators and that are easy to implement. We show that, under a generalized linear mixed model with logit link, the computationally expensive empirical best predictor does not perform appreciably better than a plug‐in estimator. Moreover, for moderate proportions of active workers, the empirical best linear unbiased predictor (EBLUP) based on a much simpler linear mixed model performs similarly to the above estimators. We propose new bootstrap estimators of the design MSE of the EBLUPs, which ‘borrow strength’ similarly to EBLUPs. Realistic simulation studies carried out under both model‐ and design‐based set‐ups indicate great gains in efficiency of the selected small area estimators over the traditional direct estimators and acceptable performance of the proposed bootstrap MSE estimators. In the application using the Swiss data, coefficient‐of‐variation reductions of the estimates obtained for the communes are remarkable."
55,Regression‐with‐residuals estimation of marginal effects: a method of adjusting for treatment‐induced confounders that may also be effect modifiers,"Geoffrey T. Wodtke, Zahide Alaca, Xiang Zhou",https://onlinelibrary.wiley.com/doi/10.1111/rssa.12497,"When making causal inferences, treatment‐induced confounders complicate analyses of time‐varying treatment effects. Conditioning on these variables naively to estimate marginal effects may inappropriately block causal pathways and may induce spurious associations between the treatment and the outcome, leading to bias. Although several methods for estimating marginal effects avoid these complications, including inverse probability of treatment weighted estimation of marginal structural models as well as g‐ and regression‐with‐residuals estimation of highly constrained structural nested mean models, each suffers from a set of non‐trivial limitations, among them an inability to accommodate effect modification. In this study, we adapt the method of regression with residuals to estimate marginal effects with a set of moderately constrained structural nested mean models that easily accommodate several types of treatment‐by‐confounder interaction. With this approach, the confounders at each time point are first residualized with respect to the observed past, which involves centring them at their estimated means given prior treatments and confounders. The outcome is then regressed on all prior variables, including a set of treatment‐by‐confounder interaction terms, with these residuals substituted for the untransformed confounders both as ‘main effects’ and as part of any interaction terms. Through a series of simulation experiments and empirical examples, we show that this approach outperforms other methods for estimating the marginal effects of time‐varying treatments."
56,Inclusion of time‐varying covariates in cure survival models with an application in fertility studies,"Philippe Lambert, Vincent Bremhorst",https://onlinelibrary.wiley.com/doi/10.1111/rssa.12501,Cure survival models are used when we desire to acknowledge explicitly that an unknown proportion of the population studied will never experience the event of interest. An extension of the promotion time cure model enabling the inclusion of time‐varying covariates as regressors when modelling (simultaneously) the probability and the timing of the monitored event is presented. Our proposal enables us to handle non‐monotone population hazard functions without a specific parametric assumption on the baseline hazard. This extension is motivated by and illustrated on data from the German Socio‐Economic Panel by studying the transition to second and third births in West Germany.
57,Optimally balanced Gaussian process propensity scores for estimating treatment effects,"Brian G. Vegetabile, Daniel L. Gillen, Hal S. Stern",https://onlinelibrary.wiley.com/doi/10.1111/rssa.12502,"Propensity scores are commonly employed in observational study settings where the goal is to estimate average treatment effects. The paper introduces a flexible propensity score modelling approach, where the probability of treatment is modelled through a Gaussian process framework. To evaluate the effectiveness of the estimated propensity score, a metric of covariate imbalance is developed that quantifies the discrepancy between the distributions of covariates in the treated and control groups. It is demonstrated that this metric is ultimately a function of the hyperparameters of the covariance matrix of the Gaussian process and therefore it is possible to select the hyperparameters to optimize the metric and to minimize overall covariate imbalance. The effectiveness of the Gaussian process method is compared in a simulation against other methods of estimating the propensity score and the method is applied to data from a study of Dehejia and Wahba in 1999 to demonstrate benchmark performance within a relevant policy application."
58,Outcome‐dependent sampling in cluster‐correlated data settings with application to hospital profiling,"Glen McGee, Jonathan Schildcrout, Sharon‐Lise Normand, Sebastien Haneuse",https://onlinelibrary.wiley.com/doi/10.1111/rssa.12503,"Hospital readmission is a key marker of quality of healthcare and an important policy measure, used by the Centers for Medicare and Medicaid Services to determine, in part, reimbursement rates. Currently, analyses of readmissions are based on a logistic–normal generalized linear mixed model that permits estimation of hospital‐specific measures while adjusting for case mix differences. Recent moves to identify and address healthcare disparities call for expanding case mix adjustment to include measures of socio‐economic status while minimizing additional burden to hospitals associated with collecting data on such measures. Towards resolving this dilemma, we propose that detailed socio‐economic data be collected on a subsample of patients via an outcome‐dependent sampling scheme, specifically the cluster‐stratified case–control design. Estimation and inference, for both the fixed and the random‐effects components, are performed via pseudo‐maximum‐likelihood wherein inverse probability weights are incorporated in the usual integrated likelihood to account for the design. In comprehensive simulations, cluster‐stratified case–control sampling proves to be an efficient design whenever interest lies in fixed or random effects of a generalized linear mixed model and covariates are unobserved or expensive to collect. The methods are motivated by and illustrated with an analysis of N = 889661 Medicare beneficiaries hospitalized between 2011 and 2013 with congestive heart failure at one of K = 3116 hospitals. Results highlight that the framework proposed provides a means of mitigating disparities in terms of which hospitals are indicated as being poor performers, relative to a naive analysis that fails to adjust for missing case mix variables."
59,Book reviews,no author,https://onlinelibrary.wiley.com/doi/10.1111/rssa.12535,no abstract
60,Regression Analysis in Medical Research for Starters and 2nd Levelers,Kuldeep Kumar,https://onlinelibrary.wiley.com/doi/10.1111/rssa.12536,no abstract
61,"Foundations of Info‐metrics: Modeling, Inference, and Imperfect Information",Rosa Bernardini Papalia,https://onlinelibrary.wiley.com/doi/10.1111/rssa.12537,no abstract
62,Practical R for Mass Communication and Journalism,Thomas King,https://onlinelibrary.wiley.com/doi/10.1111/rssa.12538,no abstract
63,"Classification, (Big) Data Analysis and Statistical Learning",Kuldeep Kumar,https://onlinelibrary.wiley.com/doi/10.1111/rssa.12539,no abstract
64,A Multivariate Claim Count Model for Applications in Insurance,Morteza Aalabaf‐Sabaghi,https://onlinelibrary.wiley.com/doi/10.1111/rssa.12540,no abstract
65,"Philogenetic Inference, Selection Theory, and History of Science",R Allan Reese,https://onlinelibrary.wiley.com/doi/10.1111/rssa.12541,no abstract
66,The Beauty of Mathematics in Computer Science,Johnny Hopkins,https://onlinelibrary.wiley.com/doi/10.1111/rssa.12542,no abstract
