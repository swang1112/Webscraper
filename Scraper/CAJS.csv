,title,author,link,abstract
0,Issue Information,no author,https://onlinelibrary.wiley.com/doi/10.1002/cjs.11505,no abstract
1,Validity and efficiency in analyzing ordinal responses with missing observations,"Xichen She, Changbao Wu",https://onlinelibrary.wiley.com/doi/10.1002/cjs.11523,"This article addresses issues in creating public‐use data files in the presence of missing ordinal responses and subsequent statistical analyses of the dataset by users. The authors propose a fully efficient fractional imputation (FI) procedure for ordinal responses with missing observations. The proposed imputation strategy retrieves the missing values through the full conditional distribution of the response given the covariates and results in a single imputed data file that can be analyzed by different data users with different scientific objectives. Two most critical aspects of statistical analyses based on the imputed data set,  validity  and  efficiency, are examined through regression analysis involving the ordinal response and a selected set of covariates. It is shown through both theoretical development and simulation studies that, when the ordinal responses are missing at random, the proposed FI procedure leads to valid and highly efficient inferences as compared to existing methods. Variance estimation using the fractionally imputed data set is also discussed. The Canadian Journal of Statistics 48: 138–151; 2020 © 2019 Statistical Society of Canada"
2,Partial order relations for classification comparisons,Lo‐Bin Chang,https://onlinelibrary.wiley.com/doi/10.1002/cjs.11524,"The Bayes classification rule offers the optimal classifier, minimizing the classification error rate, whereas the Neyman–Pearson lemma offers the optimal family of classifiers to maximize the detection rate for any given false alarm rate. These motivate studies on comparing classifiers based on similarities between the classifiers and the optimal. In this article, we define partial order relations on classifiers and families of classifiers, based on rankings of rate function values and rankings of test function values, respectively. Each partial order relation provides a sufficient condition, which yields better classification error rates or better performance on the receiver operating characteristic analysis. Various examples and applications of the partial order theorems are discussed to provide comparisons of classifiers and families of classifiers, including the comparison of cross‐validation methods, training data that contains outliers, and labelling errors in training data. The Canadian Journal of Statistics 48: 152–166; 2020 © 2019 Statistical Society of Canada"
3,A new distribution‐free k‐sample test: Analysis of kernel density functionals,Su Chen,https://onlinelibrary.wiley.com/doi/10.1002/cjs.11525,"A novel distribution‐free k‐sample test of differences in location shifts based on the analysis of kernel density functional estimation is introduced and studied. The proposed test parallels one‐way analysis of variance and the Kruskal–Wallis (KW) test aiming at testing locations of unknown distributions. In contrast to the rank (score)‐transformed non‐parametric approach, such as the KW test, the proposed F‐test uses the measurement responses along with well‐known kernel density estimation (KDE) to estimate the locations and construct the test statistic. A practical optimal bandwidth selection procedure is also provided. Our simulation studies and real data example indicate that the proposed analysis of kernel density functional estimate (ANDFE) test is superior to existing competitors for fat‐tailed or heavy‐tailed distributions when the k groups differ mainly in location rather than shape, especially with unbalanced data. ANDFE is also highly recommended when it is unclear whether test groups differ mainly in shape or location. The Canadian Journal of Statistics 48: 167–186; 2020 © 2019 Statistical Society of Canada"
4,Direct estimation of differential networks under high‐dimensional nonparanormal graphical models,Qingyang Zhang,https://onlinelibrary.wiley.com/doi/10.1002/cjs.11526,"In genomics, it is often of interest to study the structural change of a genetic network between two phenotypes. Under Gaussian graphical models, the problem can be transformed to estimating the difference between two precision matrices, and several approaches have been recently developed for this task such as joint graphical lasso and fused graphical lasso. However, the multivariate Gaussian assumptions made in the existing approaches are often violated in reality. For instance, most RNA‐Seq data follow non‐Gaussian distributions even after log‐transformation or other variance‐stabilizing transformations. In this work, we consider the problem of directly estimating differential networks under a flexible semiparametric model, namely the nonparanormal graphical model, where the random variables are assumed to follow a multivariate Gaussian distribution after a set of monotonically increasing transformations. We propose to use a novel rank‐based estimator to directly estimate the differential network, together with a parametric simplex algorithm for fast implementation. Theoretical properties of the new estimator are established under a high‐dimensional setting where 
            
            
p
 grows with 
            
            
n
 almost exponentially fast. In particular, we show that the proposed estimator is consistent in both parameter estimation and support recovery. Both synthetic data and real genomic data are used to illustrate the promise of the new approach. The Canadian Journal of Statistics 48: 187–203; 2020 © 2019 Statistical Society of Canada"
5,Estimating prediction error for complex samples,"Andrew Holbrook, Thomas Lumley, Daniel Gillen",https://onlinelibrary.wiley.com/doi/10.1002/cjs.11527,"With a growing interest in using non‐representative samples to train prediction models for numerous outcomes it is necessary to account for the sampling design that gives rise to the data in order to assess the generalized predictive utility of a proposed prediction rule. After learning a prediction rule based on a non‐uniform sample, it is of interest to estimate the rule's error rate when applied to unobserved members of the population. Efron (1986) proposed a general class of covariance penalty inflated prediction error estimators that assume the available training data are representative of the target population for which the prediction rule is to be applied. We extend Efron's estimator to the complex sample context by incorporating Horvitz–Thompson sampling weights and show that it is consistent for the true generalization error rate when applied to the underlying superpopulation. The resulting Horvitz–Thompson–Efron estimator is equivalent to dAIC, a recent extension of Akaike's information criteria to survey sampling data, but is more widely applicable. The proposed methodology is assessed with simulations and is applied to models predicting renal function obtained from the large‐scale National Health and Nutrition Examination Study survey. The Canadian Journal of Statistics 48: 204–221; 2020 © 2019 Statistical Society of Canada"
6,Semiparametric regression methods for temporal processes subject to multiple sources of censoring,"Tianyu Zhan, Douglas E. Schaubel",https://onlinelibrary.wiley.com/doi/10.1002/cjs.11528,"Process regression methodology is underdeveloped relative to the frequency with which pertinent data arise. In this article, the response‐190 is a binary indicator process representing the joint event of being alive and remaining in a specific state. The process is indexed by time (e.g., time since diagnosis) and observed continuously. Data of this sort occur frequently in the study of chronic disease. A general area of application involves a recurrent event with non‐negligible duration (e.g., hospitalization and associated length of hospital stay) and subject to a terminating event (e.g., death). We propose a semiparametric multiplicative model for the process version of the probability of being alive and in the (transient) state of interest. Under the proposed methods, the regression parameter is estimated through a procedure that does not require estimating the baseline probability. Unlike the majority of process regression methods, the proposed methods accommodate multiple sources of censoring. In particular, we derive a computationally convenient variant of inverse probability of censoring weighting based on the additive hazards model. We show that the regression parameter estimator is asymptotically normal, and that the baseline probability function estimator converges to a Gaussian process. Simulations demonstrate that our estimators have good finite sample performance. We apply our method to national end‐stage liver disease data. The Canadian Journal of Statistics 48: 222–237; 2020 © 2019 Statistical Society of Canada"
7,Functional measurement error in functional regression,"Sneha Jadhav, Shuangge Ma",https://onlinelibrary.wiley.com/doi/10.1002/cjs.11529,"Measurement error is an important problem that has not been studied very well in the context of functional data analysis. To the best of our knowledge, there are no existing methods that address the presence of functional measurement errors in generalized functional linear models. In this article, a novel approach is proposed to estimate the slope function in the presence of measurement error in the generalized functional linear model with a scalar response. This work significantly advances the existing conditional score method to accommodate the case where both the measurement error and independent variables lie in infinite dimensional spaces. Asymptotic results are established for the proposed estimate, and its behaviour is studied via simulations, where the response is continuous or binary. Analysis of Canadian Weather data highlights the practical utility of our method. The Canadian Journal of Statistics 48: 238–258; 2020 © 2020 Statistical Society of Canada"
8,Improved methods for moment restriction models with data combination and an application to two‐sample instrumental variable estimation,"Heng Shu, Zhiqiang Tan",https://onlinelibrary.wiley.com/doi/10.1002/cjs.11530,"Combining‐100 information from multiple samples is often needed in biomedical and economic studies, but differences between these samples must be appropriately taken into account in the analysis of the combined data. We study the estimation for moment restriction models with data combined from two samples under an ignorability‐type assumption while allowing for different marginal distributions of variables common to both samples. Suppose that an outcome regression (OR) model and a propensity score (PS) model are specified. By leveraging semi‐parametric efficiency theory, we derive an augmented inverse probability‐weighted (AIPW) estimator that is locally efficient and doubly robust with respect to these models. Furthermore, we develop calibrated regression and likelihood estimators that are not only locally efficient and doubly robust but also intrinsically efficient in achieving smaller variances than the AIPW estimator when the PS model is correctly specified but the OR model may be mispecified. As an important application, we study the two‐sample instrumental variable problem and derive the corresponding estimators while allowing for incompatible distributions of variables common to the two samples. Finally, we provide a simulation study and an econometric application on public housing projects to demonstrate the superior performance of our improved estimators. The Canadian Journal of Statistics 48: 259–284; 2020 © 2019 Statistical Society of Canada"
9,Optimal design for classification of functional data,"Cai Li, Luo Xiao",https://onlinelibrary.wiley.com/doi/10.1002/cjs.11531,"We study the design problem for the optimal classification of functional data. The goal is to select sampling time points so that functional data observed at these time points can be classified accurately. We propose optimal designs that are applicable to either dense or sparse functional data. Using linear discriminant analysis, we formulate our design objectives as explicit functions of the sampling points. We study the theoretical properties of the proposed design objectives and provide a practical implementation. The performance of the proposed design is evaluated through simulations and real data applications. The Canadian Journal of Statistics 48: 285–307; 2020 © 2019 Statistical Society of Canada"
10,High‐dimensional covariance matrix estimation using a low‐rank and diagonal decomposition,"Yilei Wu, Yingli Qin, Mu Zhu",https://onlinelibrary.wiley.com/doi/10.1002/cjs.11532,"We study high‐dimensional covariance/precision matrix estimation under the assumption that the covariance/precision matrix can be decomposed into a low‐rank component 
            
            
L
 and a diagonal component 
            
            
D
. The rank of 
            
            
L
 can either be chosen to be small or controlled by a penalty function. Under moderate conditions on the population covariance/precision matrix itself and on the penalty function, we prove some consistency results for our estimators. A block‐wise coordinate descent algorithm, which iteratively updates 
            
            
L
 and 
            
            
D
, is then proposed to obtain the estimator in practice. Finally, various numerical experiments are presented; using simulated data, we show that our estimator performs quite well in terms of the Kullback–Leibler loss; using stock return data, we show that our method can be applied to obtain enhanced solutions to the Markowitz portfolio selection problem. The Canadian Journal of Statistics 48: 308–337; 2020 © 2019 Statistical Society of Canada"
11,Issue Information,no author,https://onlinelibrary.wiley.com/doi/10.1002/cjs.11504,no abstract
12,"Special issue on Stochastic Models, Statistics and Finance: Guest Editor's Introduction",Cody Hyndman,https://onlinelibrary.wiley.com/doi/10.1002/cjs.11539,no abstract
13,Price bias and common practice in option pricing,"Jean‐François Bégin, Geneviève Gauthier",https://onlinelibrary.wiley.com/doi/10.1002/cjs.11495,"Generally, the semiclosed‐form option pricing formula for complex financial models depends on unobservable factors such as stochastic volatility and jump intensity. A popular practice is to use an estimate of these latent factors to compute the option price. However, in many situations this plug‐and‐play approximation does not yield the appropriate price. This article examines this bias and quantifies its impacts. We decompose the bias into terms that are related to the bias on the unobservable factors and to the precision of their point estimators. The approximated price is found to be highly biased when only the history of the stock price is used to recover the latent states. This bias is corrected when option prices are added to the sample used to recover the states' best estimate. We also show numerically that such a bias is propagated on calibrated parameters, leading to erroneous values. The Canadian Journal of Statistics 48: 8–35; 2020 © 2019 Statistical Society of Canada"
14,Common‐factor stochastic volatility modelling with observable proxy,"Yizhou Fang, Martin Lysy, Don Mcleish",https://onlinelibrary.wiley.com/doi/10.1002/cjs.11536,"Multi‐asset modelling is of fundamental importance to financial applications such as risk management and portfolio selection. In this article, we propose a multivariate stochastic volatility modelling framework with a parsimonious and interpretable correlation structure. Building on well‐established evidence of common volatility factors among individual assets, we consider a multivariate diffusion process with a common‐factor structure in the volatility innovations. Upon substituting an observable market proxy for the common volatility factor, we markedly improve the estimation of several model parameters and latent volatilities. The model is applied to a portfolio of several important constituents of the S&P500 in the financial sector, with the VIX index as the common‐factor proxy. We find that the prediction intervals for asset forecasts are comparable to those of more complex dependence models, but that option‐pricing uncertainty can be greatly reduced by adopting a common‐volatility structure. The Canadian Journal of Statistics 48: 36–61; 2020 © 2020 Statistical Society of Canada"
15,Inference for a change‐point problem under an OU setting with unequal and unknown volatilities,"Fuqi Chen, Rogemar Mamon, Sévérien Nkurunziza",https://onlinelibrary.wiley.com/doi/10.1002/cjs.11522,"An Ornstein–Uhlenbeck (OU) process is employed as a versatile model to capture the mean‐reverting and stochastic evolution of many variables in various fields of applications including finance and economics. Within the OU setting, we develop a new estimation method to determine the unknown change‐point location under the assumption that the volatilities before and after the change point in a time series are unequal. Our method hinges on the concept of a weighted least sum of squared errors approach and enhanced by a fusion of an iterative algorithm. The consistency of the change‐point estimator is established. This article highlights a numerical implementation on simulated and observed financial market data demonstrating the significant flexibility and accuracy of our proposed modelling and estimation method. The Canadian Journal of Statistics 48: 62–78; 2020 © 2019 Statistical Society of Canada"
16,Goodness‐of‐fit for regime‐switching copula models with application to option pricing,"Bouchra R. Nasri, Bruno N. Rémillard, Mamadou Y. Thioub",https://onlinelibrary.wiley.com/doi/10.1002/cjs.11534,"We consider several time series, and for each of them, we fit an appropriate dynamic parametric model. This produces serially independent error terms for each time series. The dependence between these error terms is then modelled by a regime‐switching copula. The EM algorithm is used for estimating the parameters and a sequential goodness‐of‐fit procedure based on Cramér–von Mises statistics is proposed to select the appropriate number of regimes. Numerical experiments are performed to assess the validity of the proposed methodology. As an example of application, we evaluate a European put‐on‐max option on the returns of two assets. To facilitate the use of our methodology, we have built a R package HMMcopula available on CRAN. The Canadian Journal of Statistics 48: 79–96; 2020 © 2020 Statistical Society of Canada"
17,The entropic measure transform,"Renjie Wang, Cody Hyndman, Anastasis Kratsios",https://onlinelibrary.wiley.com/doi/10.1002/cjs.11537,"We introduce the entropic measure transform (EMT) problem for a general process and prove the existence of a unique optimal measure characterizing the solution. The density process of the optimal measure is characterized using a semimartingale BSDE under general conditions. The EMT is used to reinterpret the conditional entropic risk‐measure and to obtain a convenient formula for the conditional expectation of a process that admits an affine representation under a related measure. The EMT is then used to provide a new characterization of defaultable bond prices, forward prices and futures prices when a jump‐diffusion drives the asset. The characterization of these pricing problems in terms of the EMT provides economic interpretations as maximizing the returns subject to a penalty for removing financial risk as expressed through the aggregate relative entropy. The EMT is shown to extend the optimal stochastic control characterization of default‐free bond prices of Gombani & Runggaldier (2013). These methods are illustrated numerically with an example in the defaultable bond setting. The Canadian Journal of Statistics 48: 97–129; 2020 © 2020 Statistical Society of Canada"
18,Acknowledgement of Referees' Services Remerciements aux lecteurs critiques,no author,https://onlinelibrary.wiley.com/doi/10.1002/cjs.11538,no abstract
