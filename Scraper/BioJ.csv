,title,author,link,abstract
0,Cover Picture: Biometrical Journal 4'20,no author,https://onlinelibrary.wiley.com/doi/10.1002/bimj.202070041,no abstract
1,Editorial Board: Biometrical Journal 4'20,no author,https://onlinelibrary.wiley.com/doi/10.1002/bimj.202070042,no abstract
2,Masthead: Biometrical Journal 4'20,no author,https://onlinelibrary.wiley.com/doi/10.1002/bimj.202070043,no abstract
3,Contents: Biometrical Journal 4'20,no author,https://onlinelibrary.wiley.com/doi/10.1002/bimj.202070045,no abstract
4,Editorial: Year 2019 Report,"Marco Alfò, Dankmar Böhning",https://onlinelibrary.wiley.com/doi/10.1002/bimj.202000074,no abstract
5,Regressive models for risk prediction of repeated multinomial outcomes: An illustration using Health and Retirement Study data,"Rafiqul I. Chowdhury, Mohammed Ataharul Islam",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201800101,"Life expectancy is increasing in many countries and this may lead to a higher frequency of adverse health outcomes. Therefore, there is a growing demand for predicting the risk of a sequence of events based on specified factors from repeated outcomes. We proposed regressive models and a framework to predict the joint probabilities of a sequence of events for multinomial outcomes from longitudinal studies. The Markov chain is used to link marginal and sequence of conditional probabilities to predict the joint probability. Marginal and sequence of conditional probabilities are estimated using marginal and regressive models. An application is shown using the Health and Retirement Study data. The bias of parameter estimates for all models from all bootstrap simulation is less than 1% in most of the cases. The estimated mean squared error is also very low. Results from the simulation study show negligible bias and the usefulness of the proposed model. The proposed model and framework would be useful to solve real‐life problems from various fields and big data analysis."
6,Modeling tails for collinear data with outliers in the English Longitudinal Study of Ageing: Quantile profile regression,"Xi Liu, Silvia Liverani, Kimberley J. Smith, Keming Yu",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201900146,"Research has shown that high blood glucose levels are important predictors of incident diabetes. However, they are also strongly associated with other cardiometabolic risk factors such as high blood pressure, adiposity, and cholesterol, which are also highly correlated with one another. The aim of this analysis was to ascertain how these highly correlated cardiometabolic risk factors might be associated with high levels of blood glucose in older adults aged 50 or older from wave 2 of the English Longitudinal Study of Ageing (ELSA). Due to the high collinearity of predictor variables and our interest in extreme values of blood glucose we proposed a new method, called quantile profile regression, to answer this question. Profile regression, a Bayesian nonparametric model for clustering responses and covariates simultaneously, is a powerful tool to model the relationship between a response variable and covariates, but the standard approach of using a mixture of Gaussian distributions for the response model will not identify the underlying clusters correctly, particularly with outliers in the data or heavy tail distribution of the response. Therefore, we propose quantile profile regression to model the response variable with an asymmetric Laplace distribution, allowing us to model more accurately clusters that are asymmetric and predict more accurately for extreme values of the response variable and/or outliers. Our new method performs more accurately in simulations when compared to Normal profile regression approach as well as robustly when outliers are present in the data. We conclude with an analysis of the ELSA."
7,Developing risk models for multicenter data using standard logistic regression produced suboptimal predictions: A simulation study,"Nora Falconieri, Ben Van Calster, Dirk Timmerman, Laure Wynants",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201900075,"Although multicenter data are common, many prediction model studies ignore this during model development. The objective of this study is to evaluate the predictive performance of regression methods for developing clinical risk prediction models using multicenter data, and provide guidelines for practice. We compared the predictive performance of standard logistic regression, generalized estimating equations, random intercept logistic regression, and fixed effects logistic regression. First, we presented a case study on the diagnosis of ovarian cancer. Subsequently, a simulation study investigated the performance of the different models as a function of the amount of clustering, development sample size, distribution of center‐specific intercepts, the presence of a center‐predictor interaction, and the presence of a dependency between center effects and predictors. The results showed that when sample sizes were sufficiently large, conditional models yielded calibrated predictions, whereas marginal models yielded miscalibrated predictions. Small sample sizes led to overfitting and unreliable predictions. This miscalibration was worse with more heavily clustered data. Calibration of random intercept logistic regression was better than that of standard logistic regression even when center‐specific intercepts were not normally distributed, a center‐predictor interaction was present, center effects and predictors were dependent, or when the model was applied in a new center. Therefore, to make reliable predictions in a specific center, we recommend random intercept logistic regression."
8,Population size estimation with interval censored counts and external information: Prevalence of multiple sclerosis in Rome,Alessio Farcomeni,https://onlinelibrary.wiley.com/doi/10.1002/bimj.201900268,"We discuss Bayesian log‐linear models for incomplete contingency tables with both missing and interval censored cells, with the aim of obtaining reliable population size estimates. We also discuss use of external information on the censoring probability, which may substantially reduce uncertainty. We show in simulation that information on lower bounds and external information can each improve the mean squared error of population size estimates, even when the external information is not completely accurate. We conclude with an original example on estimation of prevalence of multiple sclerosis in the metropolitan area of Rome, where five out of six lists have interval censored counts. External information comes from mortality rates of multiple sclerosis patients."
9,Bayesian latent class models for capture–recapture in the presence of missing data,"Davide Di Cecco, Marco Di Zio, Brunero Liseo",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201900111,"We propose a method for estimating the size of a population in a multiple record system in the presence of missing data. The method is based on a latent class model where the parameters and the latent structure are estimated using a Gibbs sampler. The proposed approach is illustrated through the analysis of a data set already known in the literature, which consists of five registrations of neural tube defects."
10,A nonparametric method of estimation of the population size in capture–recapture experiments,"María Dolores Jiménez‐Gamero, Pedro Puig",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201900185,"A recent method for estimating a lower bound of the population size in capture–recapture samples is studied. Specifically, some asymptotic properties, such as strong consistency and asymptotic normality, are provided. The introduced estimator is based on the empirical probability generating function (pgf) of the observed data, and it is consistent for count distributions having a log‐convex pgf (‐class). This is a large family that includes mixed and compound Poisson distributions, and their independent sums and finite mixtures as well. The finite‐sample performance of the lower bound estimator is assessed via simulation showing a better behavior than some close competitors. Several examples of application are also analyzed and discussed."
11,Generalized parametric cure models for relative survival,"Lasse Hjort Jakobsen, Martin Bøgsted, Mark Clements",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201900056,"Cure models are used in time‐to‐event analysis when not all individuals are expected to experience the event of interest, or when the survival of the considered individuals reaches the same level as the general population. These scenarios correspond to a plateau in the survival and relative survival function, respectively. The main parameters of interest in cure models are the proportion of individuals who are cured, termed the cure proportion, and the survival function of the uncured individuals. Although numerous cure models have been proposed in the statistical literature, there is no consensus on how to formulate these. We introduce a general parametric formulation of mixture cure models and a new class of cure models, termed latent cure models, together with a general estimation framework and software, which enable fitting of a wide range of different models. Through simulations, we assess the statistical properties of the models with respect to the cure proportion and the survival of the uncured individuals. Finally, we illustrate the models using survival data on colon cancer, which typically display a plateau in the relative survival. As demonstrated in the simulations, mixture cure models which are not guaranteed to be constant after a finite time point, tend to produce accurate estimates of the cure proportion and the survival of the uncured. However, these models are very unstable in certain cases due to identifiability issues, whereas LC models generally provide stable results at the price of more biased estimates."
12,The effect of treatment delay on time‐to‐recovery in the presence of unobserved heterogeneity,"Nan van Geloven, Theodor A. Balan, Hein Putter, Saskia le Cessie",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201900131,"We study the effect of delaying treatment in the presence of (unobserved) heterogeneity. In a homogeneous population and assuming a proportional treatment effect, a treatment delay period will result in notably lower cumulative recovery percentages. We show in theoretical scenarios using frailty models that if the population is heterogeneous, the effect of a delay period is much smaller. This can be explained by the selection process that is induced by the frailty. Patient groups that start treatment later have already undergone more selection. The marginal hazard ratio for the treatment will act differently in such a more homogeneous patient group. We further discuss modeling approaches for estimating the effect of treatment delay in the presence of heterogeneity, and compare their performance in a simulation study. The conventional Cox model that fails to account for heterogeneity overestimates the effect of treatment delay. Including interaction terms between treatment and starting time of treatment or between treatment and follow up time gave no improvement. Estimating a frailty term can improve the estimation, but is sensitive to misspecification of the frailty distribution. Therefore, multiple frailty distributions should be used and the results should be compared using the Akaike Information Criterion. Non‐parametric estimation of the cumulative recovery percentages can be considered if the dataset contains sufficient long term follow up for each of the delay strategies. The methods are demonstrated on a motivating application evaluating the effect of delaying the start of treatment with assisted reproductive techniques on time‐to‐pregnancy in couples with unexplained subfertility."
13,Approaches for missing covariate data in logistic regression with MNAR sensitivity analyses,"Ralph C. Ward, Robert Neal Axon, Mulugeta Gebregziabher",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201900117,"Data with missing covariate values but fully observed binary outcomes are an important subset of the missing data challenge. Common approaches are complete case analysis (CCA) and multiple imputation (MI). While CCA relies on missing completely at random (MCAR), MI usually relies on a missing at random (MAR) assumption to produce unbiased results. For MI involving logistic regression models, it is also important to consider several missing not at random (MNAR) conditions under which CCA is asymptotically unbiased and, as we show, MI is also valid in some cases. We use a data application and simulation study to compare the performance of several machine learning and parametric MI methods under a fully conditional specification framework (MI‐FCS). Our simulation includes five scenarios involving MCAR, MAR, and MNAR under predictable and nonpredictable conditions, where “predictable” indicates missingness is not associated with the outcome. We build on previous results in the literature to show MI and CCA can both produce unbiased results under more conditions than some analysts may realize. When both approaches were valid, we found that MI‐FCS was at least as good as CCA in terms of estimated bias and coverage, and was superior when missingness involved a categorical covariate. We also demonstrate how MNAR sensitivity analysis can build confidence that unbiased results were obtained, including under MNAR‐predictable, when CCA and MI are both valid. Since the missingness mechanism cannot be identified from observed data, investigators should compare results from MI and CCA when both are plausibly valid, followed by MNAR sensitivity analysis."
14,Smoothed empirical likelihood inference for ROC curve in the presence of missing biomarker values,"Weili Cheng, Niansheng Tang",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201900121,"This paper considers statistical inference for the receiver operating characteristic (ROC) curve in the presence of missing biomarker values by utilizing estimating equations (EEs) together with smoothed empirical likelihood (SEL). Three approaches are developed to estimate ROC curve and construct its SEL‐based confidence intervals based on the kernel‐assisted EE imputation, multiple imputation, and hybrid imputation combining the inverse probability weighted imputation and multiple imputation. Under some regularity conditions, we show asymptotic properties of the proposed maximum SEL estimators for ROC curve. Simulation studies are conducted to investigate the performance of the proposed SEL approaches. An example is illustrated by the proposed methodologies. Empirical results show that the hybrid imputation method behaves better than the kernel‐assisted and multiple imputation methods, and the proposed three SEL methods outperform existing nonparametric method."
15,False discovery rate control for multiple testing based on discrete p‐values,Xiongzhi Chen,https://onlinelibrary.wiley.com/doi/10.1002/bimj.201900163,"For multiple testing based on discrete p‐values, we propose a false discovery rate (FDR) procedure “BH+” with proven conservativeness. BH+ is at least as powerful as the BH (i.e., Benjamini‐Hochberg) procedure when they are applied to superuniform p‐values. Further, when applied to mid‐p‐values, BH+ can be more powerful than it is applied to conventional p‐values. An easily verifiable necessary and sufficient condition for this is provided. BH+ is perhaps the first conservative FDR procedure applicable to mid‐p‐values and to p‐values with general distributions. It is applied to multiple testing based on discrete p‐values in a methylation study, an HIV study and a clinical safety study, where it makes considerably more discoveries than the BH procedure. In addition, we propose an adaptive version of the BH+ procedure, prove its conservativeness under certain conditions, and provide evidence on its excellent performance via simulation studies."
16,A simple method for correcting for the Will Rogers phenomenon with biometrical applications,"Mark Stander, Julian Stander",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201900199,"In its basic form, the Will Rogers phenomenon takes place when an increase in the average value of each of two sets is achieved by moving an element from one set to another. This leads to the conclusion that there has been an improvement, when in fact essentially nothing has changed. Extended versions of this phenomenon can occur in epidemiological studies, rendering their results unreliable. After describing epidemiological and clinical studies that have been affected by the Will Rogers phenomenon, this paper presents a simple method to correct for it. The method involves introducing a transition matrix between the two sets and taking probability weighted expectations. Two real‐world biometrical examples, based on migration economics and breast cancer epidemiology, are given and improvements against a naïve analysis are demonstrated. In the cancer epidemiology example, we take account of estimation uncertainty. We also discuss briefly some limitations associated with our method."
17,A design criterion for symmetric model discrimination based on flexible nominal sets,"Radoslav Harman, Werner G. Müller",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201900074,"Experimental design applications for discriminating between models have been hampered by the assumption to know beforehand which model is the true one, which is counter to the very aim of the experiment. Previous approaches to alleviate this requirement were either symmetrizations of asymmetric techniques, or Bayesian, minimax, and sequential approaches. Here we present a genuinely symmetric criterion based on a linearized distance between mean‐value surfaces and the newly introduced tool of flexible nominal sets. We demonstrate the computational efficiency of the approach using the proposed criterion and provide a Monte‐Carlo evaluation of its discrimination performance on the basis of the likelihood ratio. An application for a pair of competing models in enzyme kinetics is given."
18,A Bayesian spatiotemporal statistical analysis of out‐of‐hospital cardiac arrests,"Stefano Peluso, Antonietta Mira, Håvard Rue, Nicholas John Tierney, Claudio Benvenuti, Roberto Cianella, Maria Luce Caputo, Angelo Auricchio",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201900166,"We propose a Bayesian spatiotemporal statistical model for predicting out‐of‐hospital cardiac arrests (OHCAs). Risk maps for Ticino, adjusted for demographic covariates, are built for explaining and forecasting the spatial distribution of OHCAs and their temporal dynamics. The occurrence intensity of the OHCA event in each area of interest, and the cardiac risk‐based clustering of municipalities are efficiently estimated, through a statistical model that decomposes OHCA intensity into overall intensity, demographic fixed effects, spatially structured and unstructured random effects, time polynomial dependence, and spatiotemporal random effect. In the studied geography, time evolution and dependence on demographic features are robust over different categories of OHCAs, but with variability in their spatial and spatiotemporal structure. Two main OHCA incidence‐based clusters of municipalities are identified."
19,"Model‐based clustering and classification for data science: With applications in R Bouveyron, Charles Celeux, Gilles Murphy, T. Brendan Raftery, Adrian E. (2019).  New York, NY:  Cambridge University Press.  446 pages. CDN$91.95 (hardback). ISBN: 9781108494205.",Li‐Pang Chen,https://onlinelibrary.wiley.com/doi/10.1002/bimj.201900390,no abstract
20,,Andreas Ziegler,https://onlinelibrary.wiley.com/doi/10.1002/bimj.202000088,no abstract
21,"Clinical prediction models: A practical approach to development, validation, and updating Steyerberg, Ewout W. (2019).  Second Edition, Springer Series Statistics for Biology and Health.  Cham:  Springer.  558 pages. ISBN: 978‐3‐030‐16398‐3.",Andreas Ziegler,https://onlinelibrary.wiley.com/doi/10.1002/bimj.202000088,no abstract
22,"Dose‐response analysis using R Ritz, C. Jensen, S. M. Gerhard, D. Streibig, J. C. (2019).  Boca Raton, FL:  CRC Press, 214 pages. ISBN: 978‐1‐138‐03431‐0.","Franziska Kappenberg, Jörg Rahnenführer",https://onlinelibrary.wiley.com/doi/10.1002/bimj.202000099,no abstract
23,List of Reviewers for 2019,no author,https://onlinelibrary.wiley.com/doi/10.1002/bimj.202070044,no abstract
24,Cover Picture: Biometrical Journal 3'20,no author,https://onlinelibrary.wiley.com/doi/10.1002/bimj.202070031,no abstract
25,Editorial Board: Biometrical Journal 3'20,no author,https://onlinelibrary.wiley.com/doi/10.1002/bimj.202070032,no abstract
26,Masthead: Biometrical Journal 3'20,no author,https://onlinelibrary.wiley.com/doi/10.1002/bimj.202070033,no abstract
27,Contents: Biometrical Journal 3'20,no author,https://onlinelibrary.wiley.com/doi/10.1002/bimj.202070034,no abstract
28,Statistical models for complex data in clinical and epidemiological research,"Jan Beyersmann, Willi Sauerbrei, Claudia Schmoor",https://onlinelibrary.wiley.com/doi/10.1002/bimj.202000079,no abstract
29,Time‐dependent mediators in survival analysis: Modeling direct and indirect effects with the additive hazards model,"Odd O. Aalen, Mats J. Stensrud, Vanessa Didelez, Rhian Daniel, Kjetil Røysland, Susanne Strohmaier",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201800263,"We discuss causal mediation analyses for survival data and propose a new approach based on the additive hazards model. The emphasis is on a dynamic point of view, that is, understanding how the direct and indirect effects develop over time. Hence, importantly, we allow for a time varying mediator. To define direct and indirect effects in such a longitudinal survival setting we take an interventional approach (Didelez, 2018) where treatment is separated into one aspect affecting the mediator and a different aspect affecting survival. In general, this leads to a version of the nonparametric g‐formula (Robins, 1986). In the present paper, we demonstrate that combining the g‐formula with the additive hazards model and a sequential linear model for the mediator process results in simple and interpretable expressions for direct and indirect effects in terms of relative survival as well as cumulative hazards. Our results generalize and formalize the method of dynamic path analysis (Fosen, Ferkingstad, Borgan, & Aalen, 2006; Strohmaier et al., 2015). An application to data from a clinical trial on blood pressure medication is given."
30,A multistate model for early decision‐making in oncology,"Ulrich Beyer, David Dejardin, Matthias Meller, Kaspar Rufibach, Hans Ulrich Burger",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201800250,"The development of oncology drugs progresses through multiple phases, where after each phase, a decision is made about whether to move a molecule forward. Early phase efficacy decisions are often made on the basis of single‐arm studies based on a set of rules to define whether the tumor improves (“responds”), remains stable, or progresses (response evaluation criteria in solid tumors [RECIST]). These decision rules are implicitly assuming some form of surrogacy between tumor response and long‐term endpoints like progression‐free survival (PFS) or overall survival (OS). With the emergence of new therapies, for which the link between RECIST tumor response and long‐term endpoints is either not accessible yet, or the link is weaker than with classical chemotherapies, tumor response‐based rules may not be optimal. In this paper, we explore the use of a multistate model for decision‐making based on single‐arm early phase trials. The multistate model allows to account for more information than the simple RECIST response status, namely, the time to get to response, the duration of response, the PFS time, and time to death. We propose to base the decision on efficacy on the OS hazard ratio (HR) comparing historical control to data from the experimental treatment, with the latter predicted from a multistate model based on early phase data with limited survival follow‐up. Using two case studies, we illustrate feasibility of the estimation of such an OS HR. We argue that, in the presence of limited follow‐up and small sample size, and making realistic assumptions within the multistate model, the OS prediction is acceptable and may lead to better early decisions within the development of a drug."
31,"Comparison of complex modeling strategies for prediction of a binary outcome based on a few, highly correlated predictors","Marco Chiabudini, Martin Schumacher, Erika Graf",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201800243,"Motivated by a clinical prediction problem, a simulation study was performed to compare different approaches for building risk prediction models. Robust prediction models for hospital survival in patients with acute heart failure were to be derived from three highly correlated blood parameters measured up to four times, with predictive ability having explicit priority over interpretability. Methods that relied only on the original predictors were compared with methods using an expanded predictor space including transformations and interactions. Predictors were simulated as transformations and combinations of multivariate normal variables which were fitted to the partly skewed and bimodally distributed original data in such a way that the simulated data mimicked the original covariate structure. Different penalized versions of logistic regression as well as random forests and generalized additive models were investigated using classical logistic regression as a benchmark. Their performance was assessed based on measures of predictive accuracy, model discrimination, and model calibration. Three different scenarios using different subsets of the original data with different numbers of observations and events per variable were investigated. In the investigated setting, where a risk prediction model should be based on a small set of highly correlated and interconnected predictors, Elastic Net and also Ridge logistic regression showed good performance compared to their competitors, while other methods did not lead to substantial improvements or even performed worse than standard logistic regression. Our work demonstrates how simulation studies that mimic relevant features of a specific data set can support the choice of a good modeling strategy."
32,The population‐attributable fraction for time‐dependent exposures using dynamic prediction and landmarking,"Maja von Cube, Martin Schumacher, Hein Putter, Jéan‐François Timsit, Cornelis van de Velde, Martin Wolkewitz",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201800252,"The public health impact of a harmful exposure can be quantified by the population‐attributable fraction (PAF). The PAF describes the attributable risk due to an exposure and is often interpreted as the proportion of preventable cases if the exposure was extinct. Difficulties in the definition and interpretation of the PAF arise when the exposure of interest depends on time. Then, the definition of exposed and unexposed individuals is not straightforward. We propose dynamic prediction and landmarking to define and estimate a PAF in this data situation. Two estimands are discussed which are based on two hypothetical interventions that could prevent the exposure in different ways. Considering the first estimand, at each landmark the estimation problem is reduced to a time‐independent setting. Then, estimation is simply performed by using a generalized‐linear model accounting for the current exposure state and further (time‐varying) covariates. The second estimand is based on counterfactual outcomes, estimation can be performed using pseudo‐values or inverse‐probability weights. The approach is explored in a simulation study and applied on two data examples. First, we study a large French database of intensive care unit patients to estimate the population‐benefit of a pathogen‐specific intervention that could prevent ventilator‐associated pneumonia caused by the pathogen Pseudomonas aeruginosa. Moreover, we quantify the population‐attributable burden of locoregional and distant recurrence in breast cancer patients."
33,On the use of comparison regions in visualizing stochastic uncertainty in some two‐parameter estimation problems,"Maren Eckert, Werner Vach",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201800232,"When considering simultaneous inference for two parameters, it is very common to visualize stochastic uncertainty by plotting two‐dimensional confidence regions. This allows us to test post hoc null hypotheses about a single point in a simple manner. However, in some applications the interest is not in rejecting hypotheses on single points, but in demonstrating evidence for the two parameters to be in a convex subset of the parameter space. The specific convex subset to be considered may vary from one post hoc analysis to another. Then it is of interest to have a visualization allowing to perform corresponding analyses. We suggest comparison regions as a simple tool for this task."
34,Marginal variable screening for survival endpoints,"Dominic Edelmann, Manuela Hummel, Thomas Hielscher, Maral Saadati, Axel Benner",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201800269,"When performing survival analysis in very high dimensions, it is often required to reduce the number of covariates using preliminary screening. During the last years, a large number of variable screening methods for the survival context have been developed. However, guidance is missing for choosing an appropriate method in practice. The aim of this work is to provide an overview of marginal variable screening methods for survival and develop recommendations for their use. For this purpose, a literature review is given, offering a comprehensive and structured introduction to the topic. In addition, a novel screening procedure based on distance correlation and martingale residuals is proposed, which is particularly useful in detecting nonmonotone associations. For evaluating the performance of the discussed approaches, a simulation study is conducted, comparing the true positive rates of competing variable screening methods in different settings. A real data example on mantle cell lymphoma is provided."
35,Adjustment for exploratory cut‐off selection in randomized clinical trials with survival endpoint,"Heiko Götte, Marietta Kirchner, Meinhard Kieser",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201800302,"Defining the target population based on predictive biomarkers plays an important role during clinical development. After establishing a relationship between a biomarker candidate and response to treatment in exploratory phases, a subsequent confirmatory trial ideally involves only subjects with high potential of benefiting from the new compound. In order to identify those subjects in case of a continuous biomarker, a cut‐off is needed. Usually, a cut‐off is chosen that resulted in a subgroup with a large observed treatment effect in an exploratory trial. However, such a data‐driven selection may lead to overoptimistic expectations for the subsequent confirmatory trial. Treatment effect estimates, probability of success, and posterior probabilities are useful measures for deciding whether or not to conduct a confirmatory trial enrolling the biomarker‐defined population. These measures need to be adjusted for selection bias. We extend previously introduced Approximate Bayesian Computation techniques for adjustment of subgroup selection bias to a time‐to‐event setting with cut‐off selection. Challenges in this setting are that treatment effects become time‐dependent and that subsets are defined by the biomarker distribution. Simulation studies show that the proposed method provides adjusted statistical measures which are superior to naïve Maximum Likelihood estimators as well as simple shrinkage estimators."
36,Validation of discrete time‐to‐event prediction models in the presence of competing risks,"Rachel Heyard, Jean‐François Timsit, Leonhard Held, COMBACTE‐MAGNET consortium",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201800293,"Clinical prediction models play a key role in risk stratification, therapy assignment and many other fields of medical decision making. Before they can enter clinical practice, their usefulness has to be demonstrated using systematic validation. Methods to assess their predictive performance have been proposed for continuous, binary, and time‐to‐event outcomes, but the literature on validation methods for discrete time‐to‐event models with competing risks is sparse. The present paper tries to fill this gap and proposes new methodology to quantify discrimination, calibration, and prediction error (PE) for discrete time‐to‐event outcomes in the presence of competing risks. In our case study, the goal was to predict the risk of ventilator‐associated pneumonia (VAP) attributed to Pseudomonas aeruginosa in intensive care units (ICUs). Competing events are extubation, death, and VAP due to other bacteria. The aim of this application is to validate complex prediction models developed in previous work on more recently available validation data."
37,Quantitative assessment of adverse events in clinical trials: Comparison of methods at an interim and the final analysis,"Norbert Hollaender, Juan Gonzalez‐Maffe, Valentine Jehl",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201800234,"In clinical study reports (CSRs), adverse events (AEs) are commonly summarized using the incidence proportion (IP). IPs can be calculated for all types of AEs and are often interpreted as the probability that a treated patient experiences specific AEs. Exposure time can be taken into account with time‐to‐event methods. Using one minus Kaplan–Meier (1‐KM) is known to overestimate the AE probability in the presence of competing events (CEs). The use of a nonparametric estimator of the cumulative incidence function (CIF) has therefore been advocated as more appropriate. In this paper, we compare different methods to estimate the probability of one selected AE. In particular, we investigate whether the proposed methods provide a reasonable estimate of the AE probability at an interim analysis (IA). The characteristics of the methods in the presence of a CE are illustrated using data from a breast cancer study and we quantify the potential bias in a simulation study. At the final analysis performed for the CSR, 1‐KM systematically overestimates and in most cases IP slightly underestimates the given AE probability. CIF has the lowest bias in most simulation scenarios. All methods might lead to biased estimates at the IA except for AEs with early onset. The magnitude of the bias varies with the time‐to‐AE and/or CE occurrence, the selection of event‐specific hazards and the amount of censoring. In general, reporting AE probabilities for prespecified fixed time points is recommended."
38,Sampling uncertainty versus method uncertainty: A general framework with applications to omics biomarker selection,"Simon Klau, Marie‐Laure Martin‐Magniette, Anne‐Laure Boulesteix, Sabine Hoffmann",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201800309,"Uncertainty is a crucial issue in statistics which can be considered from different points of view. One type of uncertainty, typically referred to as sampling uncertainty, arises through the variability of results obtained when the same analysis strategy is applied to different samples. Another type of uncertainty arises through the variability of results obtained when using the same sample but different analysis strategies addressing the same research question. We denote this latter type of uncertainty as method uncertainty. It results from all the choices to be made for an analysis, for example, decisions related to data preparation, method choice, or model selection. In medical sciences, a large part of omics research is focused on the identification of molecular biomarkers, which can either be performed through ranking or by selection from among a large number of candidates. In this paper, we introduce a general resampling‐based framework to quantify and compare sampling and method uncertainty. For illustration, we apply this framework to different scenarios related to the selection and ranking of omics biomarkers in the context of acute myeloid leukemia: variable selection in multivariable regression using different types of omics markers, the ranking of biomarkers according to their predictive performance, and the identification of differentially expressed genes from RNA‐seq data. For all three scenarios, our findings suggest highly unstable results when the same analysis strategy is applied to two independent samples, indicating high sampling uncertainty and a comparatively smaller, but non‐negligible method uncertainty, which strongly depends on the methods being compared."
39,IV estimation without distributional assumptions,"Theis Lange, Aksel K. G. Jensen",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201800277,"It is widely known that Instrumental Variable (IV) estimation allows the researcher to estimate causal effects between an exposure and an outcome even in face of serious uncontrolled confounding. The key requirement for IV estimation is the existence of a variable, the instrument, which only affects the outcome through its effects on the exposure and that the instrument–outcome relationship is unconfounded. Countless papers have employed such techniques and carefully addressed the validity of the IV assumption just mentioned. However, less appreciated is that fact that the IV estimation also depends on a number of distributional assumptions in particular linearities. In this paper, we propose a novel bounding procedure which can bound the true causal effect relying only on the key IV assumption and not on any distributional assumptions. For a purely binary case (instrument, exposure, and outcome all binary), such boundaries have been proposed by Balke and Pearl in 1997. We extend such boundaries to non‐binary settings. In addition, our procedure offers a tuning parameter such that one can go from the traditional IV analysis, which provides a point estimate, to a completely unrestricted bound and anything in between. Subject matter knowledge can be used when setting the tuning parameter. To the best of our knowledge, no such methods exist elsewhere. The method is illustrated using a pivotal study which introduced IV estimation to epidemiologists. Here, we demonstrate that the conclusion of this paper indeed hinges on these additional distributional assumptions. R‐code is provided in the Supporting Information."
40,Estimating the distribution of heterogeneous treatment effects from treatment responses and from a predictive biomarker in a parallel‐group RCT: A structural model approach,"Ruediger P. Laubender, Ulrich Mansmann, Michael Lauseker",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201800370,"When the objective is to administer the best of two treatments to an individual, it is necessary to know his or her individual treatment effects (ITEs) and the correlation between the potential responses (PRs)  and  under treatments 1 and 0. Data that are generated in a parallel‐group design RCT does not allow the ITE to be determined because only two samples from the marginal distributions of these PRs are observed and not the corresponding joint distribution. This is due to the “fundamental problem of causal inference.” Here, we present a counterfactual approach for estimating the joint distribution of two normally distributed responses to two treatments. This joint distribution of the PRs  and  can be estimated by assuming a bivariate normal distribution for the PRs and by using a normally distributed baseline biomarker  functionally related to the sum . Such a functional relationship is plausible since a biomarker  and the sum  encode for the same information in an RCT, namely the variation between subjects. The estimation of the joint trivariate distribution is subjected to some constraints. These constraints can be framed in the context of linear regressions with regard to the proportions of variances in the responses explained and with regard to the residual variation. This presents new insights on the presence of treatment–biomarker interactions. We applied our approach to example data on exercise and heart rate and extended the approach to survival data."
41,Meta‐analysis of clinical trials with competing time‐to‐event endpoints,"Alessandra Meddis, Aurélien Latouche, Bingqing Zhou, Stefan Michiels, Jason Fine",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201900103,"Recommendations for the analysis of competing risks in the context of randomized clinical trials are well established. Meta‐analysis of individual patient data (IPD) is the gold standard for synthesizing evidence for clinical interpretation based on multiple studies. Surprisingly, no formal guidelines have been yet proposed to conduct an IPD meta‐analysis with competing risk endpoints. To fill this gap, this work details (i) how to handle the heterogeneity between trials via a stratified regression model for competing risks and (ii) that the usual metrics of inconsistency to assess heterogeneity can readily be employed. Our proposal is illustrated by the re‐analysis of a recently published meta‐analysis in nasopharyngeal carcinoma, aiming at quantifying the benefit of the addition of chemotherapy to radiotherapy on each competing endpoint."
42,Construction and assessment of prediction rules for binary outcome in the presence of missing predictor data using multiple imputation and cross‐validation: Methodological approach and data‐based evaluation,"Bart J. A. Mertens, Erika Banzato, Liesbeth C. de Wreede",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201800289,"We investigate calibration and assessment of predictive rules when missing values are present in the predictors. Our paper has two key objectives. The first is to investigate how the calibration of the prediction rule can be combined with use of multiple imputation to account for missing predictor observations. The second objective is to propose such methods that can be implemented with current multiple imputation software, while allowing for unbiased predictive assessment through validation on new observations for which outcome is not yet available.
We commence with a review of the methodological foundations of multiple imputation as a model estimation approach as opposed to a purely algorithmic description. We specifically contrast application of multiple imputation for parameter (effect) estimation with predictive calibration. Based on this review, two approaches are formulated, of which the second utilizes application of the classical Rubin's rules for parameter estimation, while the first approach averages probabilities from models fitted on single imputations to directly approximate the predictive density for future observations. We present implementations using current software that allow for validation and estimation of performance measures by cross‐validation, as well as imputation of missing data in predictors on the future data where outcome is missing by definition.
To simplify, we restrict discussion to binary outcome and logistic regression throughout. Method performance is verified through application on two real data sets. Accuracy (Brier score) and variance of predicted probabilities are investigated. Results show substantial reductions in variation of calibrated probabilities when using the first approach."
43,On the interpretation of the hazard ratio in Cox regression,"Jan De Neve, Thomas A. Gerds",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201800255,We argue that the term “relative risk” should not be used as a synonym for “hazard ratio” and encourage to use the probabilistic index as an alternative effect measure for Cox regression. The probabilistic index is the probability that the event time of an exposed or treated subject exceeds the event time of an unexposed or untreated subject conditional on the other covariates. It arises as a well known and simple transformation of the hazard ratio and nicely reveals the interpretational limitations. We demonstrate how the probabilistic index can be obtained using the R‐package Publish.
44,On the estimation of average treatment effects with right‐censored time to event outcome and competing risks,"Brice Maxime Hugues Ozenne, Thomas Harder Scheike, Laila Stærk, Thomas Alexander Gerds",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201800298,"We are interested in the estimation of average treatment effects based on right‐censored data of an observational study. We focus on causal inference of differences between t‐year absolute event risks in a situation with competing risks. We derive doubly robust estimation equations and implement estimators for the nuisance parameters based on working regression models for the outcome, censoring, and treatment distribution conditional on auxiliary baseline covariates. We use the functional delta method to show that these estimators are regular asymptotically linear estimators and estimate their variances based on estimates of their influence functions. In empirical studies, we assess the robustness of the estimators and the coverage of confidence intervals. The methods are further illustrated using data from a Danish registry study."
45,Estimating the decision curve and its precision from three study designs,"Ruth M. Pfeiffer, Mitchell H. Gail",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201800240,"The decision curve plots the net benefit  of a risk model for making decisions over a range of risk thresholds, corresponding to different ratios of misclassification costs. We discuss three methods to estimate the decision curve, together with corresponding methods of inference and methods to compare two risk models at a given risk threshold. One method uses risks (R) and a binary event indicator (Y) on the entire validation cohort. This method makes no assumptions on how well‐calibrated the risk model is nor on the incidence of disease in the population and is comparatively robust to model miscalibration. If one assumes that the model is well‐calibrated, one can compute a much more precise estimate of  based on risks R alone. However, if the risk model is miscalibrated, serious bias can result. Case–control data can also be used to estimate  if the incidence (or prevalence) of the event () is known. This strategy has comparable efficiency to using the full  data, and its efficiency is only modestly less than that for the full  data if the incidence is estimated from the mean of Y. We estimate variances using influence functions and propose a bootstrap procedure to obtain simultaneous confidence bands around the decision curve for a range of thresholds. The influence function approach to estimate variances can also be applied to cohorts derived from complex survey samples instead of simple random samples."
46,Integrated evaluation of targeted and non‐targeted therapies in a network meta‐analysis,"Tanja Proctor, Katrin Jensen, Meinhard Kieser",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201800322,"Individualized therapies for patients with biomarkers are moving more and more into the focus of research interest when developing new treatments. Hereby, the term individualized (or targeted) therapy denotes a treatment specifically developed for biomarker‐positive patients. A network meta‐analysis model for a binary endpoint combining the evidence for a targeted therapy from individual patient data with the evidence for a non‐targeted therapy from aggregate data is presented and investigated. The biomarker status of the patients is either available at patient‐level in individual patient data or at study‐level in aggregate data. Both types of biomarker information have to be included. The evidence synthesis model follows a Bayesian approach and applies a meta‐regression to the studies with aggregate data. In a simulation study, we address three treatment arms, one of them investigating a targeted therapy. The bias and the root‐mean‐square error of the treatment effect estimate for the subgroup of biomarker‐positive patients based on studies with aggregate data are investigated. Thereby, the meta‐regression approach is compared to approaches applying alternative solutions. The regression approach has a surprisingly small bias even in the presence of few studies. By contrast, the root‐mean‐square error is relatively greater. An illustrative example is provided demonstrating implementation of the presented network meta‐analysis model in a clinical setting."
47,On the relation between the cause‐specific hazard and the subdistribution rate for competing risks data: The Fine–Gray model revisited,"Hein Putter, Martin Schumacher, Hans C. van Houwelingen",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201800274,"The Fine–Gray proportional subdistribution hazards model has been puzzling many people since its introduction. The main reason for the uneasy feeling is that the approach considers individuals still at risk for an event of cause 1 after they fell victim to the competing risk of cause 2. The subdistribution hazard and the extended risk sets, where subjects who failed of the competing risk remain in the risk set, are generally perceived as unnatural . One could say it is somewhat of a riddle why the Fine–Gray approach yields valid inference. To take away these uneasy feelings, we explore the link between the Fine–Gray and cause‐specific approaches in more detail. We introduce the reduction factor as representing the proportion of subjects in the Fine–Gray risk set that has not yet experienced a competing event. In the presence of covariates, the dependence of the reduction factor on a covariate gives information on how the effect of the covariate on the cause‐specific hazard and the subdistribution hazard relate. We discuss estimation and modeling of the reduction factor, and show how they can be used in various ways to estimate cumulative incidences, given the covariates. Methods are illustrated on data of the European Society for Blood and Marrow Transplantation."
48,Network meta‐analysis of multicomponent interventions,"Gerta Rücker, Maria Petropoulou, Guido Schwarzer",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201800167,"In network meta‐analysis (NMA), treatments can be complex interventions, for example, some treatments may be combinations of others or of common components. In standard NMA, all existing (single or combined) treatments are different nodes in the network. However, sometimes an alternative model is of interest that utilizes the information that some treatments are combinations of common components, called component network meta‐analysis (CNMA) model. The additive CNMA model assumes that the effect of a treatment combined of two components A and B is the sum of the effects of A and B, which is easily extended to treatments composed of more than two components. This implies that in comparisons equal components cancel out. Interaction CNMA models also allow interactions between the components. Bayesian analyses have been suggested. We report an implementation of CNMA models in the frequentist R package netmeta. All parameters are estimated using weighted least squares regression. We illustrate the application of CNMA models using an NMA of treatments for depression in primary care. Moreover, we show that these models can even be applied to disconnected networks, if the composite treatments in the subnetworks contain common components."
49,"Dynamic prediction: A challenge for biostatisticians, but greatly needed by patients, physicians and the public","Martin Schumacher, Stefanie Hieke, Gabriele Ihorst, Monika Engelhardt",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201800248,"Prognosis is usually expressed in terms of the probability that a patient will or will not have experienced an event of interest t years after diagnosis of a disease. This quantity, however, is of little informative value for a patient who is still event‐free after a number of years. Such a patient would be much more interested in the conditional probability of being event‐free in the upcoming t years, given that he/she did not experience the event in the s years after diagnosis, called “conditional survival.” It is the simplest form of a dynamic prediction and can be dealt with using straightforward extensions of standard time‐to‐event analyses in clinical cohort studies. For a healthy individual, a related problem with further complications is the so‐called “age‐conditional probability of developing cancer” in the next t years. Here, the competing risk of dying from other diseases has to be taken into account. For both situations, the hazard function provides the central dynamic concept, which can be further extended in a natural way to build dynamic prediction models that incorporate both baseline and time‐dependent characteristics. Such models are able to exploit the most current information accumulating over time in order to accurately predict the further course or development of a disease. In this article, the biostatistical challenges as well as the relevance and importance of dynamic prediction are illustrated using studies of multiple myeloma, a hematologic malignancy with a formerly rather poor prognosis which has improved over the last few years."
50,Modeling the hazard of transition into the absorbing state in the illness‐death model,"Elena Tassistro, Davide Paolo Bernasconi, Paola Rebora, Maria Grazia Valsecchi, Laura Antolini",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201800267,"The illness‐death model is the simplest multistate model where the transition from the initial state 0 to the absorbing state 2 may involve an intermediate state 1 (e.g., disease relapse). The impact of the transition into state 1 on the subsequent transition hazard to state 2 enables insight to be gained into the disease evolution. The standard approach of analysis is modeling the transition hazards from 0 to 2 and from 1 to 2, including time to illness as a time‐varying covariate and measuring time from origin even after transition into state 1. The hazard from 1 to 2 can be also modeled separately using only patients in state 1, measuring time from illness and including time to illness as a fixed covariate. A recently proposed approach is a model where time after the transition into state 1 is measured in both scales and time to illness is included as a time‐varying covariate. Another possibility is a model where time after transition into state 1 is measured only from illness and time to illness is included as a fixed covariate. Through theoretical reasoning and simulation protocols, we discuss the use of these models and we develop a practical strategy aiming to (a) validate the properties of the illness‐death process, (b) estimate the impact of time to illness on the hazard from state 1 to 2, and (c) quantify the impact that the transition into state 1 has on the hazard of the absorbing state. The strategy is also applied to a literature dataset on diabetes."
51,Nonparametric estimation of the cumulative incidences of competing risks under double truncation,Jacobo de Uña‐Álvarez,https://onlinelibrary.wiley.com/doi/10.1002/bimj.201800323,"Registry data typically report incident cases within a certain calendar time interval. Such interval sampling induces double truncation on the incidence times, which may result in an observational bias. In this paper, we introduce nonparametric estimation for the cumulative incidences of competing risks when the incidence time is doubly truncated. Two different estimators are proposed depending on whether the truncation limits are independent of the competing events or not. The asymptotic properties of the estimators are established, and their finite sample performance is investigated through simulations. For illustration purposes, the estimators are applied to childhood cancer registry data, where the target population is peculiarly defined conditional on future cancer development. Then, in our application, the cumulative incidences inform on the distribution by age of the different types of cancer."
52,Automatic variable selection for exposure‐driven propensity score matching with unmeasured confounders,"Daniela Zöller, Leesa F. Wockner, Harald Binder",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201800190,"Multivariable model building for propensity score modeling approaches is challenging. A common propensity score approach is exposure‐driven propensity score matching, where the best model selection strategy is still unclear. In particular, the situation may require variable selection, while it is still unclear if variables included in the propensity score should be associated with the exposure and the outcome, with either the exposure or the outcome, with at least the exposure or with at least the outcome. Unmeasured confounders, complex correlation structures, and non‐normal covariate distributions further complicate matters. We consider the performance of different modeling strategies in a simulation design with a complex but realistic structure and effects on a binary outcome. We compare the strategies in terms of bias and variance in estimated marginal exposure effects. Considering the bias in estimated marginal exposure effects, the most reliable results for estimating the propensity score are obtained by selecting variables related to the exposure. On average this results in the least bias and does not greatly increase variances. Although our results cannot be generalized, this provides a counterexample to existing recommendations in the literature based on simple simulation settings. This highlights that recommendations obtained in simple simulation settings cannot always be generalized to more complex, but realistic settings and that more complex simulation studies are needed."
53,Cover Picture: Biometrical Journal 2'20,no author,https://onlinelibrary.wiley.com/doi/10.1002/bimj.202070021,no abstract
54,Editorial Board: Biometrical Journal 2'20,no author,https://onlinelibrary.wiley.com/doi/10.1002/bimj.202070022,no abstract
55,Masthead: Biometrical Journal 2'20,no author,https://onlinelibrary.wiley.com/doi/10.1002/bimj.202070023,no abstract
56,Contents: Biometrical Journal 2'20,no author,https://onlinelibrary.wiley.com/doi/10.1002/bimj.202070024,no abstract
57,A potpourri of biostatistical research: Special Issue for ISCB ASC 2018,"John Carlin, Jessica Kasza, Margarita Moreno‐Betancur, Julie Simpson, Jonathan Bartlett, Chris Metcalfe, Katherine Lee",https://onlinelibrary.wiley.com/doi/10.1002/bimj.202000030,no abstract
58,Statistical strategies for the analysis of massive data sets,"Hon Hwang, Louise Ryan",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201900034,"The advent of the big data age has changed the landscape for statisticians. Public and private organizations alike these days are interested in capturing and analyzing complex customer data in order to improve their service and drive efficiency gains. However, the large volume of data involved often means that standard statistical methods fail and new ways of thinking are needed. Although great gains can be obtained through the use of more advanced computing environments or through developing sophisticated new statistical algorithms that handle data in a more efficient way, there are also many simpler things that can be done to handle large data sets in an efficient and intuitive manner. These include the use of distributed analysis methodologies, clever subsampling, data coarsening, and clever data reductions that exploit concepts such as sufficiency. These kinds of strategies represent exciting opportunities for statisticians to remain front and center in the data science world."
59,SMARTp: A SMART design for nonsurgical treatments of chronic periodontitis with spatially referenced and nonrandomly missing skewed outcomes,"Jing Xu, Dipankar Bandyopadhyay, Sedigheh Mirzaei Salehabadi, Bryan Michalowicz, Bibhas Chakraborty",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201900027,"This paper proposes dynamic treatment regimes (DTRs) as effective individualized treatment strategies for managing chronic periodontitis. The proposed DTRs are studied via SMARTp—a two‐stage sequential multiple assignment randomized trial (SMART) design. For this design, we propose a statistical analysis plan and a novel cluster‐level sample size calculation method that factors in typical features of periodontal responses such as non‐Gaussianity, spatial clustering, and nonrandom missingness. Here, each patient is viewed as a cluster, and a tooth within a patient's mouth is viewed as an individual unit inside the cluster, with the tooth‐level covariance structure described by a conditionally autoregressive structure. To accommodate possible skewness and tail behavior, the tooth‐level clinical attachment level (CAL) response is assumed to be skew‐t, with the nonrandomly missing structure captured via a shared parameter model corresponding to the missingness indicator. The proposed method considers mean comparison for the regimes with or without sharing an initial treatment, where the expected values and corresponding variances or covariance for the sample means of a pair of DTRs are derived by the inverse probability weighting and method of moments. Simulation studies are conducted to investigate the finite‐sample performance of the proposed sample size formulas under a variety of outcome‐generating scenarios. An R package SMARTp implementing our sample size formula is available at the Comprehensive R Archive Network for free download."
60,Adjusting Simon's optimal two‐stage design for heterogeneous populations based on stratification or using historical controls,"Dominic Edelmann, Christina Habermehl, Richard F. Schlenk, Axel Benner",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201800390,"In many cancer studies, the population under consideration is highly heterogeneous in terms of clinical, demographical, and biological covariates. As the covariates substantially impact the individual prognosis, the response probabilities of patients entering the study may strongly vary. In this case, the operating characteristics of classical clinical trial designs heavily depend on the covariates of patients entering the study. Notably, both type I and type II errors can be much higher than specified. In this paper, two modifications of Simon's optimal two‐stage design correcting for heterogeneous populations are derived. The first modification assumes that the patient population is divided into a finite number of subgroups, where each subgroup has a different response probability. The second approach uses a logistic regression model based on historical controls to estimate the response probabilities of patients entering the study. The performance of both approaches is demonstrated using simulation examples."
61,A Bayesian basket trial design that borrows information across strata based on the similarity between the posterior distributions of the response probability,"Kei Fujikawa, Satoshi Teramukai, Isao Yokota, Takashi Daimon",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201800404,"Basket trials simultaneously evaluate the effect of one or more drugs on a defined biomarker, genetic alteration, or molecular target in a variety of disease subtypes, often called strata. A conventional approach for analyzing such trials is an independent analysis of each of the strata. This analysis is inefficient as it lacks the power to detect the effect of drugs in each stratum. To address these issues, various designs for basket trials have been proposed, centering on designs using Bayesian hierarchical models. In this article, we propose a novel Bayesian basket trial design that incorporates predictive sample size determination, early termination for inefficacy and efficacy, and the borrowing of information across strata. The borrowing of information is based on the similarity between the posterior distributions of the response probability. In general, Bayesian hierarchical models have many distributional assumptions along with multiple parameters. By contrast, our method has prior distributions for response probability and two parameters for similarity of distributions. The proposed design is easier to implement and less computationally demanding than other Bayesian basket designs. Through a simulation with various scenarios, our proposed design is compared with other designs including one that does not borrow information and one that uses a Bayesian hierarchical model."
62,An optimal Bayesian predictive probability design for phase II clinical trials with simple and complicated endpoints,"Beibei Guo, Suyu Liu",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201900022,"Most existing phase II clinical trial designs focus on conventional chemotherapy with binary tumor response as the endpoint. The advent of novel therapies, such as molecularly targeted agents and immunotherapy, has made the endpoint of phase II trials more complicated, often involving ordinal, nested, and coprimary endpoints. We propose a simple and flexible Bayesian optimal phase II predictive probability (OPP) design that handles binary and complex endpoints in a unified way. The Dirichlet‐multinomial model is employed to accommodate different types of endpoints. At each interim, given the observed interim data, we calculate the Bayesian predictive probability of success, should the trial continue to the maximum planned sample size, and use it to make the go/no‐go decision. The OPP design controls the type I error rate, maximizes power or minimizes the expected sample size, and is easy to implement, because the go/no‐go decision boundaries can be enumerated and included in the protocol before the onset of the trial. Simulation studies show that the OPP design has satisfactory operating characteristics."
63,Longitudinal analysis of pre‐ and post‐treatment measurements with equal baseline assumptions in randomized trials,"Ikuko Funatogawa, Takashi Funatogawa",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201800389,"For continuous variables of randomized controlled trials, recently, longitudinal analysis of pre‐ and posttreatment measurements as bivariate responses is one of analytical methods to compare two treatment groups. Under random allocation, means and variances of pretreatment measurements are expected to be equal between groups, but covariances and posttreatment variances are not. Under random allocation with unequal covariances and posttreatment variances, we compared asymptotic variances of the treatment effect estimators in three longitudinal models. The data‐generating model has equal baseline means and variances, and unequal covariances and posttreatment variances. The model with equal baseline means and unequal variance–covariance matrices has a redundant parameter. In large sample sizes, these two models keep a nominal type I error rate and have high efficiency. The model with equal baseline means and equal variance–covariance matrices wrongly assumes equal covariances and posttreatment variances. Only under equal sample sizes, this model keeps a nominal type I error rate. This model has the same high efficiency with the data‐generating model under equal sample sizes. In conclusion, longitudinal analysis with equal baseline means performed well in large sample sizes. We also compared asymptotic properties of longitudinal models with those of the analysis of covariance (ANCOVA) and t‐test."
64,Power gains by using external information in clinical trials are typically not possible when requiring strict type I error control,"Annette Kopp‐Schneider, Silvia Calderazzo, Manuel Wiesenfarth",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201800395,"In the era of precision medicine, novel designs are developed to deal with flexible clinical trials that incorporate many treatment strategies for multiple diseases in one trial setting. This situation often leads to small sample sizes in disease‐treatment combinations and has fostered the discussion about the benefits of borrowing of external or historical information for decision‐making in these trials. Several methods have been proposed that dynamically discount the amount of information borrowed from historical data based on the conformity between historical and current data. Specifically, Bayesian methods have been recommended and numerous investigations have been performed to characterize the properties of the various borrowing mechanisms with respect to the gain to be expected in the trials. However, there is common understanding that the risk of type I error inflation exists when information is borrowed and many simulation studies are carried out to quantify this effect. To add transparency to the debate, we show that if prior information is conditioned upon and a uniformly most powerful test exists, strict control of type I error implies that no power gain is possible under any mechanism of incorporation of prior information, including dynamic borrowing. The basis of the argument is to consider the test decision function as a function of the current data even when external information is included. We exemplify this finding in the case of a pediatric arm appended to an adult trial and dichotomous outcome for various methods of dynamic borrowing from adult information to the pediatric arm. In conclusion, if use of relevant external data is desired, the requirement of strict type I error control has to be replaced by more appropriate metrics."
65,Extensions of the probabilistic ranking metrics of competing treatments in network meta‐analysis to reflect clinically important relative differences on many outcomes,"Dimitris Mavridis, Raphaël Porcher, Adriani Nikolakopoulou, Georgia Salanti, Philippe Ravaud",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201900026,"One of the key features of network meta‐analysis is ranking of interventions according to outcomes of interest. Ranking metrics are prone to misinterpretation because of two limitations associated with the current ranking methods. First, differences in relative treatment effects might not be clinically important and this is not reflected in the ranking metrics. Second, there are no established methods to include several health outcomes in the ranking assessments. To address these two issues, we extended the P‐score method to allow for multiple outcomes and modified it to measure the mean extent of certainty that a treatment is better than the competing treatments by a certain amount, for example, the minimum clinical important difference. We suggest to present the tradeoff between beneficial and harmful outcomes allowing stakeholders to consider how much adverse effect they are willing to tolerate for specific gains in efficacy. We used a published network of 212 trials comparing 15 antipsychotics and placebo using a random effects network meta‐analysis model, focusing on three outcomes; reduction in symptoms of schizophrenia in a standardized scale, all‐cause discontinuation, and weight gain."
66,A utility approach to individualized optimal dose selection using biomarkers,"Pin Li, Jeremy M.G. Taylor, Spring Kong, Shruti Jolly, Matthew J. Schipper",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201900030,"In many settings, including oncology, increasing the dose of treatment results in both increased efficacy and toxicity. With the increasing availability of validated biomarkers and prediction models, there is the potential for individualized dosing based on patient specific factors. We consider the setting where there is an existing dataset of patients treated with heterogenous doses and including binary efficacy and toxicity outcomes and patient factors such as clinical features and biomarkers. The goal is to analyze the data to estimate an optimal dose for each (future) patient based on their clinical features and biomarkers. We propose an optimal individualized dose finding rule by maximizing utility functions for individual patients while limiting the rate of toxicity. The utility is defined as a weighted combination of efficacy and toxicity probabilities. This approach maximizes overall efficacy at a prespecified constraint on overall toxicity. We model the binary efficacy and toxicity outcomes using logistic regression with dose, biomarkers and dose–biomarker interactions. To incorporate the large number of potential parameters, we use the LASSO method. We additionally constrain the dose effect to be non‐negative for both efficacy and toxicity for all patients. Simulation studies show that the utility approach combined with any of the modeling methods can improve efficacy without increasing toxicity relative to fixed dosing. The proposed methods are illustrated using a dataset of patients with lung cancer treated with radiation therapy."
67,A tutorial on dynamic risk prediction of a binary outcome based on a longitudinal biomarker,"Rana Dandis, Steven Teerenstra, Leon Massuger, Fred Sweep, Yalck Eysbouts, Joanna IntHout",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201900044,"Dynamic risk predictions based on all available information are useful in timely identification of high‐risk patients. However, in contrast with time to event outcomes, there is still a lack of studies that clearly demonstrate how to obtain and update predictions for a future binary outcome using a repeatedly measured biomarker. The aim of this study is to give an illustrative overview of four approaches to obtain such predictions: likelihood based two‐stage method (2SMLE), likelihood based joint model (JMMLE), Bayesian two‐stage method (2SB), and Bayesian joint model (JMB). We applied the approaches to provide weekly updated predictions of post–molar gestational trophoblastic neoplasia (GTN) based on age and repeated measurements of human chorionic gonadotropin (hCG). Discrimination and calibration measures were used to compare the accuracy of the weekly predictions. Internal validation of the models was conducted using bootstrapping. The four approaches resulted in the same predictive and discriminative performance in predicting GTN. A simulation study showed that the joint models outperform the two‐stage methods when we increase the within‐ and the between‐patients variability of the biomarker. The applicability of these models to produce dynamic predictions has been illustrated through a comprehensive explanation and accompanying syntax (R and SAS®)."
68,Detecting possible persons of interest in a physical activity program using step entries: Including a web‐based application for outlier detection and decision‐making,"S. Sandun M. Silva, Denny Meyer, Madawa Jayawardana",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201900008,"According to recent statistics from the World Health Organization, 23% of people aged 18 years and over are not sufficiently physically active. Strangely, this is at a time when, due to the improvement in sensor technology, physical activity programs that track physical activity have become popular. However, some participants who enroll in these programs cheat by manipulating the data they enter. This can be discouraging for other participants, also invalidating the overall accuracy of program outcomes. Therefore, detecting these participants and discarding their manipulated entries is important in order to maintain the quality of the program. Currently, most of these physical activity programs use manual processes to detect and reject fraudulent step entries by reviewing the participant's demographic profiles along with their longitudinal step count performance data. In this study, a process, including two parallel models for detecting person of interest characteristics and abnormal step count entries, is developed. The first model uses the penalized logistic regression with Synthetic Minority Over‐sampling Technique subsampling to address the imbalance in the proportion of genuine and persons of interest. Having a highly imbalanced distribution between genuine and person of interest profiles makes this task more challenging. The second model uses a variety of outlier detection methods to detect and reject abnormal step entries based on previously entered data. This process will be more efficient and productive compared to the current manual system and will support better decision‐making in the future. The proposed system can be applied for other fraud detection applications after suitable adjustments."
69,Estimating treatment effects with partially observed covariates using outcome regression with missing indicators,"Helen A. Blake, Clémence Leyrat, Kathryn E. Mansfield, Laurie A. Tomlinson, James Carpenter, Elizabeth J. Williamson",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201900041,"Missing data is a common issue in research using observational studies to investigate the effect of treatments on health outcomes. When missingness occurs only in the covariates, a simple approach is to use missing indicators to handle the partially observed covariates. The missing indicator approach has been criticized for giving biased results in outcome regression. However, recent papers have suggested that the missing indicator approach can provide unbiased results in propensity score analysis under certain assumptions. We consider assumptions under which the missing indicator approach can provide valid inferences, namely, (1) no unmeasured confounding within missingness patterns; either (2a) covariate values of patients with missing data were conditionally independent of treatment or (2b) these values were conditionally independent of outcome; and (3) the outcome model is correctly specified: specifically, the true outcome model does not include interactions between missing indicators and fully observed covariates. We prove that, under the assumptions above, the missing indicator approach with outcome regression can provide unbiased estimates of the average treatment effect. We use a simulation study to investigate the extent of bias in estimates of the treatment effect when the assumptions are violated and we illustrate our findings using data from electronic health records. In conclusion, the missing indicator approach can provide valid inferences for outcome regression, but the plausibility of its assumptions must first be considered carefully."
70,Multiple imputation methods for handling incomplete longitudinal and clustered data where the target analysis is a linear mixed effects model,"Md Hamidul Huque, Margarita Moreno‐Betancur, Matteo Quartagno, Julie A. Simpson, John B. Carlin, Katherine J. Lee",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201900051,"Multiple imputation (MI) is increasingly popular for handling multivariate missing data. Two general approaches are available in standard computer packages: MI based on the posterior distribution of incomplete variables under a multivariate (joint) model, and fully conditional specification (FCS), which imputes missing values using univariate conditional distributions for each incomplete variable given all the others, cycling iteratively through the univariate imputation models. In the context of longitudinal or clustered data, it is not clear whether these approaches result in consistent estimates of regression coefficient and variance component parameters when the analysis model of interest is a linear mixed effects model (LMM) that includes both random intercepts and slopes with either covariates or both covariates and outcome contain missing information. In the current paper, we compared the performance of seven different MI methods for handling missing values in longitudinal and clustered data in the context of fitting LMMs with both random intercepts and slopes. We study the theoretical compatibility between specific imputation models fitted under each of these approaches and the LMM, and also conduct simulation studies in both the longitudinal and clustered data settings. Simulations were motivated by analyses of the association between body mass index (BMI) and quality of life (QoL) in the Longitudinal Study of Australian Children (LSAC). Our findings showed that the relative performance of MI methods vary according to whether the incomplete covariate has fixed or random effects and whether there is missingnesss in the outcome variable. We showed that compatible imputation and analysis models resulted in consistent estimation of both regression parameters and variance components via simulation. We illustrate our findings with the analysis of LSAC data."
71,Multiple imputation in the presence of an incomplete binary variable created from an underlying continuous variable,"Anneke C. Grobler, Katherine Lee",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201900011,"Multiple imputation (MI) is used to handle missing at random (MAR) data. Despite warnings from statisticians, continuous variables are often recoded into binary variables. With MI it is important that the imputation and analysis models are compatible; variables should be imputed in the same form they appear in the analysis model. With an encoded binary variable more accurate imputations may be obtained by imputing the underlying continuous variable. We conducted a simulation study to explore how best to impute a binary variable that was created from an underlying continuous variable. We generated a completely observed continuous outcome associated with an incomplete binary covariate that is a categorized version of an underlying continuous covariate, and an auxiliary variable associated with the underlying continuous covariate. We simulated data with several sample sizes, and set 25% and 50% of data in the covariate to MAR dependent on the outcome and the auxiliary variable. We compared the performance of five different imputation methods: (a) Imputation of the binary variable using logistic regression; (b) imputation of the continuous variable using linear regression, then categorizing into the binary variable; (c, d) imputation of both the continuous and binary variables using fully conditional specification (FCS) and multivariate normal imputation; (e) substantive‐model compatible (SMC) FCS. Bias and standard errors were large when the continuous variable only was imputed. The other methods performed adequately. Imputation of both the binary and continuous variables using FCS often encountered mathematical difficulties. We recommend the SMC‐FCS method as it performed best in our simulation studies."
72,Multilevel regression and poststratification as a modeling approach for estimating population quantities in large population health studies: A simulation study,"Marnie Downes, John B. Carlin",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201900023,"There are now a growing number of applications of multilevel regression and poststratification (MRP) in population health and epidemiological studies. MRP uses multilevel regression to model individual survey responses as a function of demographic and geographic covariates. Estimated mean outcome values for each demographic–geographic respondent subtype are then weighted by the proportions of each subtype in the population to produce an overall population‐level estimate. We recently reported an extensive case study of a large nationwide survey and found that MRP performed favorably compared to conventional survey sampling weights for the estimation of population descriptive quantities in a highly selected sample. In this study, we aimed to evaluate, by way of a simulation experiment, both the accuracy and precision of MRP versus survey sampling weights in the context of large population health studies. While much of the research into MRP has been focused on U.S. political and social science, we considered an alternative population structure of smaller size and with notably fewer geographic subsets. We explored the impact on MRP performance of sample size, model misspecification, interactions, and the addition of a geographic‐level covariate. MRP was found to achieve generally superior performance in both accuracy and precision at both the national and state levels. Results were generally robust to model misspecification, and MRP performance was further improved by the inclusion of a geographic‐level covariate. These findings offer further evidence that MRP provides a promising analytic approach for addressing participation bias in the estimation of population descriptive quantities from large‐scale health surveys and cohort studies."
73,Nonlinear and time‐dependent effects of sparsely measured continuous time‐varying covariates in time‐to‐event analysis,"Yishu Wang, Marie‐Eve Beauchamp, Michal Abrahamowicz",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201900042,"Many flexible extensions of the Cox proportional hazards model incorporate time‐dependent (TD) and/or nonlinear (NL) effects of time‐invariant covariates. In contrast, little attention has been given to the assessment of such effects for continuous time‐varying covariates (TVCs). We propose a flexible regression B‐spline–based model for TD and NL effects of a TVC. To account for sparse TVC measurements, we added to this model the effect of time elapsed since last observation (TEL), which acts as an effect modifier. TD, NL, and TEL effects are estimated with the iterative alternative conditional estimation algorithm. Furthermore, a simulation extrapolation (SIMEX)‐like procedure was adapted to correct the estimated effects for random measurement errors in the observed TVC values. In simulations, TD and NL estimates were unbiased if the TVC was measured with a high frequency. With sparse measurements, the strength of the effects was underestimated but the TEL estimate helped reduce the bias, whereas SIMEX helped further to correct for bias toward the null due to “white noise” measurement errors. We reassessed the effects of systolic blood pressure (SBP) and total cholesterol, measured at two‐year intervals, on cardiovascular risks in women participating in the Framingham Heart Study. Accounting for TD effects of SBP, cholesterol and age, the NL effect of cholesterol, and the TEL effect of SBP improved substantially the model's fit to data. Flexible estimates yielded clinically important insights regarding the role of these risk factors. These results illustrate the advantages of flexible modeling of TVC effects."
74,Cover Picture: Biometrical Journal 1'20,no author,https://onlinelibrary.wiley.com/doi/10.1002/bimj.202070011,no abstract
75,Editorial Board: Biometrical Journal 1'20,no author,https://onlinelibrary.wiley.com/doi/10.1002/bimj.202070012,no abstract
76,Masthead: Biometrical Journal 1'20,no author,https://onlinelibrary.wiley.com/doi/10.1002/bimj.202070013,no abstract
77,Contents: Biometrical Journal 1'20,no author,https://onlinelibrary.wiley.com/doi/10.1002/bimj.202070014,no abstract
78,Joint mean–covariance random effect model for longitudinal data,"Yongxin Bai, Manling Qian, Maozai Tian",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201800311,"In this paper, we consider the inherent association between mean and covariance in the joint mean–covariance modeling and propose a joint mean–covariance random effect model based on the modified Cholesky decomposition for longitudinal data. Meanwhile, we apply M‐H algorithm to simulate the posterior distributions of model parameters. Besides, a computationally efficient Monte Carlo expectation maximization (MCEM) algorithm is developed for carrying out maximum likelihood estimation. Simulation studies show that the model taking into account the inherent association between mean and covariance has smaller standard deviations of the estimators of parameters, which makes the statistical inferences much more reliable. In the real data analysis, the estimation of parameters in the mean and covariance structure is highly efficient."
79,Joint model for recurrent event data with a cured fraction and a terminal event,Yang‐Jin Kim,https://onlinelibrary.wiley.com/doi/10.1002/bimj.201800321,"In a longitudinal study where the recurrence of an event and a terminal event such as death are observed, a certain portion of the subjects may experience no event during a long follow‐up period; this often denoted as the cure group which is assumed to be the risk‐free from both recurrent events and death. However, this assumption ignores the possibility of death, which subjects in the cure group may experience. In the present study, such misspecification is investigated with the addition of a death hazard model to the cure group. We propose a joint model using a frailty effect, which reflects the association between a recurrent event and death. For the estimation, an expectation‐maximization (EM) algorithm was developed and PROC NLMIXED in SAS was incorporated under a piecewise constant baseline. Simulation studies were performed to check the performance of the suggested method. The proposed method was applied to leukemia patients experiencing both infection and death after bone marrow transplant."
80,Latent variable models for harmonization of test scores: A case study on memory,"Edwin R. van den Heuvel, Lauren E. Griffith, Nazmul Sohel, Isabel Fortier, Graciela Muniz‐Terrera, Parminder Raina",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201800146,"Combining data from different studies has a long tradition within the scientific community. It requires that the same information is collected from each study to be able to pool individual data. When studies have implemented different methods or used different instruments (e.g., questionnaires) for measuring the same characteristics or constructs, the observed variables need to be harmonized in some way to obtain equivalent content information across studies. This paper formulates the main concepts for harmonizing test scores from different observational studies in terms of latent variable models. The concepts are formulated in terms of calibration, invariance, and exchangeability. Although similar ideas are present in measurement reliability and test equating, harmonization is different from measurement invariance and generalizes test equating. In addition, if a test score needs to be transformed to another test score, harmonization of variables is only possible under specific conditions. Observed test scores that connect all of the different studies, are necessary to be able to test the underlying assumptions of harmonization. The concepts of harmonization are illustrated on multiple memory test scores from three different Canadian studies."
81,A multiple comparison procedure for dose‐finding trials with subpopulations,"Marius Thomas, Björn Bornkamp, Martin Posch, Franz König",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201800111,"Identifying subgroups of patients with an enhanced response to a new treatment has become an area of increased interest in the last few years. When there is knowledge about possible subpopulations with an enhanced treatment effect before the start of a trial it might be beneficial to set up a testing strategy, which tests for a significant treatment effect not only in the full population, but also in these prespecified subpopulations. In this paper, we present a parametric multiple testing approach for tests in multiple populations for dose‐finding trials. Our approach is based on the MCP‐Mod methodology, which uses multiple comparison procedures (MCPs) to test for a dose–response signal, while considering multiple possible candidate dose–response shapes. Our proposed methods allow for heteroscedastic error variances between populations and control the family‐wise error rate over tests in multiple populations and for multiple candidate models. We show in simulations that the proposed multipopulation testing approaches can increase the power to detect a significant dose–response signal over the standard single‐population MCP‐Mod, when the specified subpopulation has an enhanced treatment effect."
82,Meta‐analysis of the difference of medians,"Sean McGrath, Hojoon Sohn, Russell Steele, Andrea Benedetti",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201900036,"We consider the problem of meta‐analyzing two‐group studies that report the median of the outcome. Often, these studies are excluded from meta‐analysis because there are no well‐established statistical methods to pool the difference of medians. To include these studies in meta‐analysis, several authors have recently proposed methods to estimate the sample mean and standard deviation from the median, sample size, and several commonly reported measures of spread. Researchers frequently apply these methods to estimate the difference of means and its variance for each primary study and pool the difference of means using inverse variance weighting. In this work, we develop several methods to directly meta‐analyze the difference of medians. We conduct a simulation study evaluating the performance of the proposed median‐based methods and the competing transformation‐based methods. The simulation results show that the median‐based methods outperform the transformation‐based methods when meta‐analyzing studies that report the median of the outcome, especially when the outcome is skewed. Moreover, we illustrate the various methods on a real‐life data set."
83,Distribution‐free simultaneous tests for location–scale and Lehmann alternative in two‐sample problem,"Wolfgang Kössler, Amitava Mukherjee",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201900057,"The paper deals with the classical two‐sample testing problem for the equality of two populations, one of the most fundamental problems in biomedical experiments and case–control studies. The most familiar alternatives are the difference in location parameters or the difference in scale parameters or in both the parameters of the population density. All the tests designed for classical location or scale or location–scale alternatives assume that there is no change in the shape of the distribution. Some authors also consider the Lehmann‐type alternative that addresses the change in shape. Two‐sample tests under Lehmann alternative assume that the location and scale parameters are invariant. In real life, when a shift in the distribution occurs, one or more of the location, scale, and shape parameters may change simultaneously. We refer to change of one or more of the three parameters as a versatile alternative. Noting the dearth of literature for the equality two populations against such versatile alternative, we introduce two distribution‐free tests based on the Euclidean and Mahalanobis distance. We obtain the asymptotic distributions of the two test statistics and study asymptotic power. We also discuss approximating p‐values of the proposed tests in real applications with small samples. We compare the power performance of the two tests with several popular existing distribution‐free tests against various fixed alternatives using Monte Carlo. We provide two illustrations based on biomedical experiments. Unlike existing tests which are suitable only in certain situations, proposed tests offer very good power in almost all types of shifts."
84,Beyond the proportional frailty model: Bayesian estimation of individual heterogeneity on mortality parameters,"Fernando Colchero, Burhan Y. Kiyakoglu",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201800280,"Today, we know that demographic rates can be greatly influenced by differences among individuals in their capacity to survive and reproduce. These intrinsic differences, commonly known as individual heterogeneity, can rarely be measured and are thus treated as latent variables when modeling mortality. Finite mixture models and mixed effects models have been proposed as alternative approaches for inference on individual heterogeneity in mortality. However, in general models assume that individual heterogeneity influences mortality proportionally, which limits the possibility to test hypotheses on the effect of individual heterogeneity on other aspects of mortality such as ageing rates. Here, we propose a Bayesian model that builds upon the mixture models previously developed, but that facilitates making inferences on the effect of individual heterogeneity on mortality parameters other than the baseline mortality. As an illustration, we apply this framework to the Gompertz–Makeham mortality model, commonly used in human and wildlife studies, by assuming that the Gompertz rate parameter is affected by individual heterogeneity. We provide results of a simulation study where we show that the model appropriately retrieves the parameters used for simulation, even for low variances in the heterogeneous parameter. We then apply the model to a dataset on captive chimpanzees and on a cohort life table of 1751 Swedish men, and show how model selection against a null model (i.e., without heterogeneity) can be carried out."
85,Flexible parametric model for survival data subject to dependent censoring,"Negera Wakgari Deresa, Ingrid Van Keilegom",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201800375,"When modeling survival data, it is common to assume that the (log‐transformed) survival time (T) is conditionally independent of the (log‐transformed) censoring time (C) given a set of covariates. There are numerous situations in which this assumption is not realistic, and a number of correction procedures have been developed for different models. However, in most cases, either some prior knowledge about the association between T and C is required, or some auxiliary information or data is/are supposed to be available. When this is not the case, the application of many existing methods turns out to be limited. The goal of this paper is to overcome this problem by developing a flexible parametric model, that is a type of transformed linear model. We show that the association between T and C is identifiable in this model. The performance of the proposed method is investigated both in an asymptotic way and through finite sample simulations. We also develop a formal goodness‐of‐fit test approach to assess the quality of the fitted model. Finally, the approach is applied to data coming from a study on liver transplants."
86,An approach to model clustered survival data with dependent censoring,"Silvana Schneider, Fábio Nogueira Demarqui, Enrico Antônio Colosimo, Vinícius Diniz Mayrink",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201800391,"In this study we introduce a likelihood‐based method, via the Weibull and piecewise exponential distributions, capable of accommodating the dependence between failure and censoring times. The methodology is developed for the analysis of clustered survival data and it assumes that failure and censoring times are mutually independent conditional on a latent frailty. The dependent censoring mechanism is accounted through the frailty effect and this is accomplished by means of a key parameter accommodating the correlation between failure and censored observations. The full specification of the likelihood in our work simplifies the inference procedures with respect to Huang and Wolfe since it reduces the computation burden of working with the profile likelihood. In addition, the assumptions made for the baseline distributions lead to models with continuous survival functions. In order to carry out inferences, we devise a Monte Carlo EM algorithm. The performance of the proposed models is investigated through a simulation study. Finally, we explore a real application involving patients from the Dialysis Outcomes and Practice Patterns Study observed between 1996 and 2015."
87,A comparison of the beta‐geometric model with landmarking for dynamic prediction of time to pregnancy,"Rik van Eekelen, Hein Putter, David J. McLernon, Marinus J. Eijkemans, Nan van Geloven",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201900155,"We conducted a simulation study to compare two methods that have been recently used in clinical literature for the dynamic prediction of time to pregnancy. The first is landmarking, a semi‐parametric method where predictions are updated as time progresses using the patient subset still at risk at that time point. The second is the beta‐geometric model that updates predictions over time from a parametric model estimated on all data and is specific to applications with a discrete time to event outcome. The beta‐geometric model introduces unobserved heterogeneity by modelling the chance of an event per discrete time unit according to a beta distribution. Due to selection of patients with lower chances as time progresses, the predicted probability of an event decreases over time. Both methods were recently used to develop models predicting the chance to conceive naturally. The advantages, disadvantages and accuracy of these two methods are unknown. We simulated time‐to‐pregnancy data according to different scenarios. We then compared the two methods by the following out‐of‐sample metrics: bias and root mean squared error in the average prediction, root mean squared error in individual predictions, Brier score and c statistic. We consider different scenarios including data‐generating mechanisms for which the models are misspecified. We applied the two methods on a clinical dataset comprising 4999 couples. Finally, we discuss the pros and cons of the two methods based on our results and present recommendations for use of either of the methods in different settings and (effective) sample sizes."
88,A two‐phase Bayesian methodology for the analysis of binary phenotypes in genome‐wide association studies,"Chase Joyner, Christopher McMahan, James Baurley, Bens Pardamean",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201900050,"Recent advances in sequencing and genotyping technologies are contributing to a data revolution in genome‐wide association studies that is characterized by the challenging large p small n problem in statistics. That is, given these advances, many such studies now consider evaluating an extremely large number of genetic markers (p) genotyped on a small number of subjects (n). Given the dimension of the data, a joint analysis of the markers is often fraught with many challenges, while a marginal analysis is not sufficient. To overcome these obstacles, herein, we propose a Bayesian two‐phase methodology that can be used to jointly relate genetic markers to binary traits while controlling for confounding. The first phase of our approach makes use of a marginal scan to identify a reduced set of candidate markers that are then evaluated jointly via a hierarchical model in the second phase. Final marker selection is accomplished through identifying a sparse estimator via a novel and computationally efficient maximum a posteriori estimation technique. We evaluate the performance of the proposed approach through extensive numerical studies, and consider a genome‐wide application involving colorectal cancer."
89,Parametric modal regression with varying precision,"Marcelo Bourguignon, Jeremias Leão, Diego I. Gallardo",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201900132,"In this paper, we propose a simple parametric modal linear regression model where the response variable is gamma distributed using a new parameterization of this distribution that is indexed by mode and precision parameters, that is, in this new regression model, the modal and precision responses are related to a linear predictor through a link function and the linear predictor involves covariates and unknown regression parameters. The main advantage of our new parameterization is the straightforward interpretation of the regression coefficients in terms of the mode of the positive response variable, as is usual in the context of generalized linear models, and direct inference in parametric mode regression based on the likelihood paradigm. Furthermore, we discuss residuals and influence diagnostic tools. A Monte Carlo experiment is conducted to evaluate the performances of these estimators in finite samples with a discussion of the results. Finally, we illustrate the usefulness of the new model by two applications, to biology and demography."
90,SIMEX for correction of dietary exposure effects with Box‐Cox transformed data,"Timm Intemann, Kirsten Mehlig, Stefaan De Henauw, Alfonso Siani, Tassos Constantinou, Luis A. Moreno, Dénes Molnár, Toomas Veidebaum, Iris Pigeot, on behalf of the I.Family consortium",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201900066,"Modelling dietary data, and especially 24‐hr dietary recall (24HDR) data, is a challenge. Ignoring the inherent measurement error (ME) leads to biased effect estimates when the association between an exposure and an outcome is investigated. We propose an adapted simulation extrapolation (SIMEX) algorithm for modelling dietary exposures. For this purpose, we exploit the ME model of the NCI method where we assume the assumption of normally distributed errors of the reported intake on the Box‐Cox transformed scale and of unbiased recalls on the original scale. According to the SIMEX algorithm, remeasurements of the observed data with additional ME are generated in order to estimate the association between the level of ME and the resulting effect estimate. Subsequently, this association is extrapolated to the case of zero ME to obtain the corrected estimate. We show that the proposed method fulfils the key property of the SIMEX approach, that is, that the MSE of the generated data will converge to zero if the ME variance converges to zero. Furthermore, the method is applied to real 24HDR data of the I.Family study to correct the effects of salt and alcohol intake on blood pressure. In a simulation study, the method is compared with the NCI method resulting in effect estimates with either smaller MSE or smaller bias in certain situations. In addition, we found our method to be more informative and easier to implement. Therefore, we conclude that the proposed method is useful to promote the dissemination of ME correction methods in nutritional epidemiology."
91,Berkson's paradox and weighted distributions: An application to Alzheimer's disease,"Polychronis Economou, Apostolos Batsidis, George Tzavelas, Panagiotis Alexopoulos, Alzheimer's Disease Neuroimaging Initiative",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201900046,"One reason for observing in practice a false positive or negative correlation between two random variables, which are either not correlated or correlated with a different direction, is the overrepresentation in the sample of individuals satisfying specific properties. In 1946, Berkson first illustrated the presence of a false correlation due to this last reason, which is known as Berkson's paradox and is one of the most famous paradox in probability and statistics. In this paper, the concept of weighted distributions is utilized to describe Berskon's paradox. Moreover, a proper procedure is suggested to make inference for the population given a biased sample which possesses all the characteristics of Berkson's paradox. A real data application for patients with dementia due to Alzheimer's disease demonstrates that the proposed method reveals characteristics of the population that are masked by the sampling procedure."
92,"Applied compositional data analysis: with worked examples in R.  Filzmoser, Peter,  Hron, Karel, and  Templ, Matthias (2018). Springer Series in Statistics. Cham:  Springer.  280 pages, ISBN: 978‐3‐319‐96420‐1","Jan Graffelman, Josep Antoni Martín‐Fernández",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201900263,no abstract
93,"Generalized linear models with examples in R.  Dunn, Peter K.and  Smyth, Gordon K. (2018).  Berlin, Germany:  Springer Science+Business Media, pp.  562 pages, ISBN: 978‐1‐4419‐0118‐7",Dominic Edelmann,https://onlinelibrary.wiley.com/doi/10.1002/bimj.201900264,no abstract
94,"The statistical analysis of multivariate failure time data: A marginal modeling approach.  Prentice, Ross L. and  Zhao, Shanshan (2019).  New York, NY:  Chapman and Hall/CRC Press.  240 pages. CDN $117.68 (hardback). ISBN 978-1-4822-5657-4.",Li‐Pang Chen,https://onlinelibrary.wiley.com/doi/10.1002/bimj.201900317,no abstract
95,"Computational Bayesian statistics: An introduction.  Turkman, M. Antónia Amaral,  Paulino, Carlos Daniel, and  Müller, Peter (2019).  Cambridge, UK:  Cambridge University Press.  243 pages, ISBN: 978‐1‐108‐70374‐1.","Owen Thomas, Leiv Rønneberg",https://onlinelibrary.wiley.com/doi/10.1002/bimj.201900307,no abstract
