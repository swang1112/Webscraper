,title,author,link,abstract
0,Issue Information,no author,https://onlinelibrary.wiley.com/doi/10.1111/rssc.12361,no abstract
1,Longitudinal networks of dyadic relationships using latent trajectories: evidence from the European interbank market,"Federica Bianchi, Francesco Bartolucci, Stefano Peluso, Antonietta Mira",https://onlinelibrary.wiley.com/doi/10.1111/rssc.12413,"Financial markets are ultimately seen as a collection of dyadic transactions. We study the temporal evolution of dyadic relationships in the European interbank market, as induced by monetary transactions registered in the electronic market for interbank deposits (e‐MID) during a period of 10 years (2006–2015). In particular, we keep track of how reciprocal exchange patterns have varied with macro events and exogenous shocks and with the emergence of the Global Financial Crisis in 2008. The approach adopted extends the model of Holland and Leinhardt to a longitudinal setting where individuals’ temporal trajectories for the tendency to connect and reciprocate transactions are explicitly modelled through splines or polynomials, and individual‐specific parameters. We estimate the model by an iterative algorithm that maximizes the log‐likelihood for every ordered pair of units. The empirical application shows that the methodology proposed may be applied to large networks and represents the process of exchange at a fine‐grained level. Further results are available in on‐line supplementary material."
2,Causal mechanism of extreme river discharges in the upper Danube basin network,"Linda Mhalla, Valérie Chavez‐Demoulin, Debbie J. Dupuis",https://onlinelibrary.wiley.com/doi/10.1111/rssc.12415,"Extreme hydrological events in the Danube river basin may severely impact human populations, aquatic organisms and economic activity. One often characterizes the joint structure of extreme events by using the theory of multivariate and spatial extremes and its asymptotically justified models. There is interest, however, in cascading extreme events and whether one event causes another. We argue that an improved understanding of the mechanism underlying severe events is achieved by combining extreme value modelling and causal discovery. We construct a causal inference method relying on the notion of the Kolmogorov complexity of extreme conditional quantiles. Tail quantities are derived by using multivariate extreme value models, and causal‐induced asymmetries in the data are explored through the minimum description length principle. Our method CausEV for causality for extreme values uncovers causal relationships between summer extreme river discharges in the upper Danube basin and finds significant causal links between the Danube and its Alpine tributary Lech."
3,Inference for extreme values under threshold‐based stopping rules,"Anna Maria Barlow, Chris Sherlock, Jonathan Tawn",https://onlinelibrary.wiley.com/doi/10.1111/rssc.12420,"There is a propensity for an extreme value analysis to be conducted as a consequence of a large flooding event. This timing of the analysis introduces bias and poor coverage probabilities into the associated risk assessments and leads subsequently to inefficient flood protection schemes. We explore these problems through studying stochastic stopping criteria and propose new likelihood‐based inferences that mitigate against these difficulties. Our methods are illustrated through the analysis of the river Lune, following its experiencing the UK's largest ever measured flow event in 2015. We show that without accounting for this stopping feature there would be substantial overdesign in response to the event."
4,"A hybrid approach for the stratified mark‐specific proportional hazards model with missing covariates and missing marks, with application to vaccine efficacy trials","Yanqing Sun, Li Qi, Fei Heng, Peter B. Gilbert",https://onlinelibrary.wiley.com/doi/10.1111/rssc.12417,"Deployment of the recently licensed tetravalent dengue vaccine based on a chimeric yellow fever virus, CYD‐TDV, requires understanding of how the risk of dengue disease in vaccine recipients depends jointly on a host biomarker measured after vaccination (neutralization titre—neutralizing antibodies) and on a ‘mark’ feature of the dengue disease failure event (the amino acid sequence distance of the dengue virus to the dengue sequence represented in the vaccine). The CYD14 phase 3 trial of CYD‐TDV measured neutralizing antibodies via case–cohort sampling and the mark in dengue disease failure events, with about a third missing marks. We addressed the question of interest by developing inferential procedures for the stratified mark‐specific proportional hazards model with missing covariates and missing marks. Two hybrid approaches are investigated that leverage both augmented inverse probability weighting and nearest neighbourhood hot deck multiple imputation. The two approaches differ in how the imputed marks are pooled in estimation. Our investigation shows that nearest neighbourhood hot deck imputation can lead to biased estimation without properly selected neighbourhoods. Simulations show that the hybrid methods developed perform well with unbiased nearest neighbourhood hot deck imputations from proper neighbourhood selection. The new methods applied to CYD14 show that neutralizing antibody level is strongly inversely associated with the risk of dengue disease in vaccine recipients, more strongly against dengue viruses with shorter distances."
5,Global household energy model: a multivariate hierarchical approach to estimating trends in the use of polluting and clean fuels for cooking,"Oliver Stoner, Gavin Shaddick, Theo Economou, Sophie Gumy, Jessica Lewis, Itzel Lucio, Giulia Ruggeri, Heather Adair‐Rohani",https://onlinelibrary.wiley.com/doi/10.1111/rssc.12428,"In 2017 an estimated 3 billion people used polluting fuels and technologies as their primary cooking solution, with 3.8 million deaths annually attributed to household exposure to the resulting fine particulate matter air pollution. Currently, health burdens are calculated by using aggregations of fuel types, e.g. solid fuels, as country level estimates of the use of specific fuel types, e.g. wood and charcoal, are unavailable. To expand the knowledge base about effects of household air pollution on health, we develop and implement a novel Bayesian hierarchical model, based on generalized Dirichlet–multinomial distributions, that jointly estimates non‐linear trends in the use of eight key fuel types, overcoming several data‐specific challenges including missing or combined fuel use values. We assess model fit by using within‐sample predictive analysis and an out‐of‐sample prediction experiment to evaluate the model's forecasting performance."
6,Small sample corrections for Wald tests in latent variable models,"Brice Ozenne, Patrick M. Fisher, Esben Budtz‐J⊘rgensen",https://onlinelibrary.wiley.com/doi/10.1111/rssc.12414,"Latent variable models are commonly used in psychology and increasingly used for analysing brain imaging data. Such studies typically involve a small number of participants (n<100), where standard asymptotic results often fail to control the type 1 error appropriately. The paper presents two corrections improving the control of the type 1 error of Wald tests in latent variable models estimated by using maximum likelihood. First, we derive a correction for the bias of the maximum likelihood estimator of the variance parameters. This enables us to estimate corrected standard errors for model parameters and corrected Wald statistics. Second, we use a Student t‐distribution instead of a Gaussian distribution to account for the variability of the variance estimator. The degrees of freedom of the Student t‐distributions are estimated by using a Satterthwaite approximation. A simulation study based on data from two published brain imaging studies demonstrates that combining these two corrections provides superior control of the type 1 error rate compared with the uncorrected Wald test, despite being conservative for some parameters. The methods proposed are implemented in the R package lavaSearch2, which is available from https://cran.r-project.org/web/packages/lavaSearch2."
7,Modelling fuel injector spray characteristics in jet engines by using vine copulas,"Maximilian Coblenz, Simon Holz, Hans‐Jörg Bauer, Oliver Grothe, Rainer Koch",https://onlinelibrary.wiley.com/doi/10.1111/rssc.12421,"The emission requirements for jet engines are becoming more stringent and the combustion process determines pollutant emissions. Therefore, we model the distribution of fuel drops generated by a fuel injector in a jet engine, which can be assumed to be a five‐dimensional problem in terms of drop size, x‐position, y‐position, x‐velocity and y‐velocity. The data are generated by numerical simulations of the fuel atomization process for several jet engine operating conditions. In combustion simulations, the variables are usually assumed to be independent at the start of the simulation, which is clearly not so as our data show. The dependence between some of the variables is non‐monotone and asymmetric, which makes the modelling task difficult. Our aim is to provide a realistic parametric model for the dependence structure. For this, we employ vine copulas which provide a flexible way to construct a multivariate distribution function. However, we need to use non‐standard bivariate copulas as building blocks. Using this copula representation enables us to create realistic samples of fuel spray droplets which improve the prediction of the combustion process and the pollutant emissions. Moreover, this approach is significantly faster than solving the set of differential equations describing fuel disintegration."
8,Simulating gene silencing through intervention analysis,"Vera Djordjilović, Monica Chiogna, Chiara Romualdi",https://onlinelibrary.wiley.com/doi/10.1111/rssc.12412,"We propose a novel method for simulating the effects of gene silencing. Our approach combines relevant subject matter information provided by biological pathways with gene expression levels measured in regular conditions to predict the behaviour of the system after one of the genes has been silenced. We achieve this by modelling gene silencing as an external intervention in a causal graphical model. To account for the uncertainty that is associated with the structure learning of the graphical model, we adopt a bootstrap approach. We illustrate our proposal on a Drosophila melanogaster gene silencing experiment."
9,Global forensic geolocation with deep neural networks,"Neal S. Grantham, Brian J. Reich, Eric B. Laber, Krishna Pacifici, Robert R. Dunn, Noah Fierer, Matthew Gebert, Julia S. Allwood, Seth A. Faith",https://onlinelibrary.wiley.com/doi/10.1111/rssc.12427,"An important problem in modern forensic analyses is identifying the provenance of materials at a crime scene, such as biological material on a piece of clothing. This procedure, which is known as geolocation, is conventionally guided by expert knowledge of the biological evidence and therefore tends to be application specific, labour intensive and often subjective. Purely data‐driven methods have yet to be fully realized in this domain, because in part of the lack of a sufficiently rich source of data. However, high throughput sequencing technologies can identify tens of thousands of fungi and bacteria taxa by using DNA recovered from a single swab collected from nearly any object or surface. This microbial community, or microbiome, may be highly informative of the provenance of the sample, but data on the spatial variation of microbiomes are sparse and high dimensional and have a complex dependence structure that render them difficult to model with standard statistical tools. Deep learning algorithms have generated a tremendous amount of interest within the machine learning community for their predictive performance in high dimensional problems. We present DeepSpace: a new algorithm for geolocation that aggregates over an ensemble of deep neural network classifiers trained on randomly generated Voronoi partitions of a spatial domain. The DeepSpace algorithm makes remarkably good point predictions; for example, when applied to the microbiomes of over 1300 dust samples collected across continental USA, more than half of geolocation predictions produced by this model fall less than 100 km from their true origin, which is a 60% reduction in error from competing geolocation methods. Moreover, we apply DeepSpace to a novel data set of global dust samples collected from nearly 30 countries, finding that dust‐associated fungi alone predict a sample's country of origin with nearly 90% accuracy."
10,Fault isolation for a complex decentralized waste water treatment facility,"Molly C. Klanderman, Kathryn B. Newhart, Tzahi Y. Cath, Amanda S. Hering",https://onlinelibrary.wiley.com/doi/10.1111/rssc.12429,"Decentralized waste water treatment facilities monitor many features that are complexly related. The ability to detect the onset of a fault and to identify variables accurately that have shifted because of the fault are vital to maintaining proper system operation and high quality produced water. Various multivariate methods have been proposed to perform fault detection and isolation, but the methods require data to be independent and identically distributed when the process is in control, and most require a distributional assumption. We propose a distribution‐free retrospective change‐point‐detection method for auto‐correlated and non‐stationary multivariate processes. We detrend the data by using observations from an in‐control time period to account for expected changes due to external or user‐controlled factors. Next, we perform the fused lasso, which penalizes differences in consecutive observations, to detect faults and to identify shifted variables. To account for auto‐correlation, the regularization parameter is chosen by using an estimated effective sample size in the extended Bayesian information criterion. We demonstrate the performance of our method compared with a competitor in simulation. Finally, we apply our method to waste water treatment facility data with a known fault, and the variables identified by our proposed method are consistent with the operators’ diagnosis of the fault's cause."
11,Estimating the binary endogenous effect of insurance on doctor visits by copula‐based regression additive models,"Giampiero Marra, Rosalba Radice, David M. Zimmer",https://onlinelibrary.wiley.com/doi/10.1111/rssc.12419,"The paper estimates the causal effect of having health insurance on healthcare utilization, while accounting for potential endogeneity bias. The topic has important policy implications, because health insurance reforms implemented in the USA in recent decades have focused on extending coverage to the previously uninsured. Consequently, understanding the effects of those reforms requires an accurate estimate of the causal effect of insurance on utilization. However, obtaining such an estimate is complicated by the discreteness inherent in common measures of healthcare usage. The paper presents a flexible estimation approach, based on copula functions, that consistently estimates the coefficient of a binary endogenous regressor in count data settings. The relevant numerical computations can be easily carried out by using the freely available GJRM R package. The empirical results find significant evidence of favourable selection into insurance. Ignoring such selection, insurance appears to increase doctor visit usage by 62% but, adjusting for it, the effect increases to 134%."
12,Statistical inference on tree swallow migrations with random forests,"Tim Coleman, Lucas Mentch, Daniel Fink, Frank A. La Sorte, David W. Winkler, Giles Hooker, Wesley M. Hochachka",https://onlinelibrary.wiley.com/doi/10.1111/rssc.12416,"Bird species’ migratory patterns have typically been studied through individual observations and historical records. In recent years, the eBird citizen science project, which solicits observations from thousands of bird watchers around the world, has opened the door for a data‐driven approach to understanding the large‐scale geographical movements. Here, we focus on the North American tree swallow (Tachycineta bicolor) occurrence patterns throughout the eastern USA. Migratory departure dates for this species are widely believed by both ornithologists and casual observers to vary substantially across years, but the reasons for this are largely unknown. In this work, we present evidence that maximum daily temperature is predictive of tree swallow occurrence. Because it is generally understood that species occurrence is a function of many complex, high order interactions between ecological covariates, we utilize the flexible modelling approach that is offered by random forests. Making use of recent asymptotic results, we provide formal hypothesis tests for predictive significance of various covariates and also develop and implement a permutation‐based approach for formally assessing interannual variations by treating the prediction surfaces that are generated by random forests as functional data. Each of these tests suggest that maximum daily temperature is important in predicting migration patterns."
13,The use of sampling weights in M‐quantile random‐effects regression: an application to Programme for International Student Assessment mathematics scores,"Francesco Schirripa Spagnolo, Nicola Salvati, Antonella D’Agostino, Ides Nicaise",https://onlinelibrary.wiley.com/doi/10.1111/rssc.12418,"M‐quantile random‐effects regression represents an interesting approach for modelling multilevel data when the researcher is focused on conditional quantiles. When data are obtained from complex survey designs, sampling weights must be incorporated in the analysis. A robust pseudolikelihood approach for accommodating sampling weights in M‐quantile random‐effects regression is presented. In particular, the method is based on a robustification of the estimating equations. The methodology proposed is applied to the Italian sample of the Programme for International Student Assessment 2015 survey to study the gender gap in mathematics at various quantiles of the conditional distribution. The findings offer a possible explanation of the low proportion of women in science, technology, engineering and mathematics sectors."
14,Issue Information,no author,https://onlinelibrary.wiley.com/doi/10.1111/rssc.12360,no abstract
15,Robust and adaptive anticoagulant control,"Peter Avery, Quentin Clairon, Robin Henderson, C. James Taylor, Emma Wilson",https://onlinelibrary.wiley.com/doi/10.1111/rssc.12403,"We consider a control theory approach to adaptive dose allocation of anticoagulants, based on an analysis of records of 152 patients on long‐term warfarin treatment. We consider a selection of statistical models for the relationship between the dose of drug and subsequent blood clotting speed, measured through the international normalized ratio. Our main focus is on subsequent use of the model in guiding the choice of the next dose adaptively as patient‐specific information accrues. We compare a naive long‐term approach with a proportional‐integral‐plus method, with parameters estimated by either linear quadratic optimization or by stochastic resource allocation. We demonstrate advantages of the control approaches in comparison with a naive approach in simulations and through calculation of robust stability margins for the observed data."
16,Structured penalized regression for drug sensitivity prediction,"Zhi Zhao, Manuela Zucknick",https://onlinelibrary.wiley.com/doi/10.1111/rssc.12400,"Large‐scale in vitro drug sensitivity screens are an important tool in personalized oncology to predict the effectiveness of potential cancer drugs. The prediction of the sensitivity of cancer cell lines to a panel of drugs is a multivariate regression problem with high dimensional heterogeneous multiomics data as input data and with potentially strong correlations between the outcome variables which represent the sensitivity to the different drugs. We propose a joint penalized regression approach with structured penalty terms which enable us to utilize the correlation structure between drugs with group‐lasso‐type penalties and at the same time address the heterogeneity between ‘omics’ data sources by introducing data‐source‐specific penalty factors to penalize different data sources differently. By combining integrative penalty factors (IPFs) with the tree‐guided group lasso, we create a method called ‘IPF‐tree‐lasso’. We present a unified framework to transform more general IPF‐type methods to the original penalized method. Because the structured penalty terms have multiple parameters, we demonstrate how the interval search ‘Efficient parameter selection via global optimization’ algorithm can be used to optimize multiple penalty parameters efficiently. Simulation studies show that IPF‐tree‐lasso can improve the prediction performance compared with other lasso‐type methods, in particular for heterogeneous sources of data. Finally, we employ the new methods to analyse data from the ‘Genomics of drug sensitivity in cancer’ project."
17,Multiple imputation of binary multilevel missing not at random data,"Angelina Hammon, Sabine Zinn",https://onlinelibrary.wiley.com/doi/10.1111/rssc.12401,"We introduce a selection model‐based multilevel imputation approach to be used within the fully conditional specification framework for multiple imputation. Concretely, we apply a censored bivariate probit model to describe binary variables assumed to be missing not at random. The first equation of the model defines the regression model for the missing data mechanism. The second equation specifies the regression model of the variable to be imputed. The non‐random selection of the binary data is mapped by correlations between the error terms of the two regression models. Hierarchical data structures are modelled by random intercepts in both equations. To fit the novel imputation model we use maximum likelihood and adaptive Gauss–Hermite quadrature. A comprehensive simulation study shows the overall performance of the approach. We test its usefulness for empirical research by applying it to a common problem in social scientific research: the emergence of educational aspirations. Our software is designed to be used in the R package mice."
18,Variable selection in functional linear concurrent regression,"Rahul Ghosal, Arnab Maity, Timothy Clark, Stefano B. Longo",https://onlinelibrary.wiley.com/doi/10.1111/rssc.12408,"We propose a novel method for variable selection in functional linear concurrent regression. Our research is motivated by a fisheries footprint study where the goal is to identify important time‐varying sociostructural drivers influencing patterns of seafood consumption, and hence the fisheries footprint, over time, as well as estimating their dynamic effects. We develop a variable‐selection method in functional linear concurrent regression extending the classically used scalar‐on‐scalar variable‐selection methods like the lasso, smoothly clipped absolute deviation (SCAD) and minimax concave penalty (MCP). We show that in functional linear concurrent regression the variable‐selection problem can be addressed as a group lasso, and their natural extension: the group SCAD or a group MCP problem. Through simulations, we illustrate that our method, particularly with the group SCAD or group MCP, can pick out the relevant variables with high accuracy and has minuscule false positive and false negative rate even when data are observed sparsely, are contaminated with noise and the error process is highly non‐stationary. We also demonstrate two real data applications of our method in studies of dietary calcium absorption and fisheries footprint in the selection of influential time‐varying covariates."
19,A joint confidence region for an overall ranking of populations,"Martin Klein, Tommy Wright, Jerzy Wieczorek",https://onlinelibrary.wiley.com/doi/10.1111/rssc.12402,"National statistical agencies lack statistical methodology to express uncertainty in their released estimated overall rankings. For example, the US Census Bureau produced an ‘explicit’ ranking of the states based on observed sample estimates during 2011 of mean travel time to work. Current literature provides measures of uncertainty in estimated individual ranks, but not a direct measure of uncertainty for the estimated overall ranking. We construct and visualize a joint confidence region for the true unknown overall ranking that provides a measure of uncertainty in the estimated overall ranking."
20,An optimal design for hierarchical generalized group testing,"Yaakov Malinovsky, Gregory Haber, Paul S. Albert",https://onlinelibrary.wiley.com/doi/10.1111/rssc.12409,"Choosing an optimal strategy for hierarchical group testing is an important problem for practitioners who are interested in disease screening with limited resources. For example, when screening for infectious diseases in large populations, it is important to use algorithms that minimize the cost of potentially expensive assays. Black and co‐workers described this as an intractable problem unless the number of individuals to screen is small. They proposed an approximation to an optimal strategy that is difficult to implement for large population sizes. We develop an optimal design with respect to the expected total number of tests that can be obtained by using a novel dynamic programming algorithm. We show that this algorithm is substantially more efficient than the approach that was proposed by Black and co‐workers. In addition, we compare the two designs for imperfect tests. R code is provided for practitioners."
21,A novel regularized approach for functional data clustering: an application to milking kinetics in dairy goats,"C. Denis, E. Lebarbier, C. Lévy‐Leduc, O. Martin, L. Sansonnet",https://onlinelibrary.wiley.com/doi/10.1111/rssc.12404,"Motivated by an application to the clustering of milking kinetics of dairy goats, we propose a novel approach for functional data clustering. This issue is of growing interest in precision livestock farming, which is largely based on the development of data acquisition automation and on the development of interpretative tools to capitalize on high throughput raw data and to generate benchmarks for phenotypic traits. The method that we propose in the paper falls in this context. Our methodology relies on a piecewise linear estimation of curves based on a novel regularized change‐point‐estimation method and on the k‐means algorithm applied to a vector of coefficients summarizing the curves. The statistical performance of our method is assessed through numerical experiments and is thoroughly compared with existing experiments. Our technique is finally applied to milk emission kinetics data with the aim of a better characterization of interanimal variability and towards a better understanding of the lactation process."
22,Generalized partially linear models on Riemannian manifolds,"Amelia Simó, M. Victoria Ibáñez, Irene Epifanio, Vicent Gimeno",https://onlinelibrary.wiley.com/doi/10.1111/rssc.12411,"We introduce generalized partially linear models with covariates on Riemannian manifolds. These models, like ordinary generalized linear models, are a generalization of partially linear models on Riemannian manifolds that allow for scalar response variables with error distribution models other than a normal distribution. Partially linear models are particularly useful when some of the covariates of the model are elements of a Riemannian manifold, because the curvature of these spaces makes it difficult to define parametric models. The model was developed to address an interesting application: the prediction of children's garment fit based on three‐dimensional scanning of their bodies. For this reason, we focus on logistic and ordinal models and on the important and difficult case where the Riemannian manifold is the three‐dimensional case of Kendall's shape space. An experimental study with a well‐known three‐dimensional database is carried out to check the goodness of the procedure. Finally, it is applied to a three‐dimensional database obtained from an anthropometric survey of the Spanish child population. A comparative study with related techniques is carried out."
23,A Bayesian group sequential small n sequential multiple‐assignment randomized trial,"Yan‐Cheng Chao, Thomas M. Braun, Roy N. Tamura, Kelley M. Kidwell",https://onlinelibrary.wiley.com/doi/10.1111/rssc.12406,"A small n, sequential, multiple‐assignment, randomized trial (called ‘snSMART’) is a small sample multistage design where participants may be rerandomized to treatment on the basis of intermediate end points. This design is motivated by the ‘A randomized multicenter study for isolated skin vasculitis’ trial (NCT02939573): an on‐going snSMART design focusing on the evaluation of three drugs for isolated skin vasculitis. By formulating an interim decision rule for removing one of the treatments, we use a Bayesian model and the resulting posterior distributions to provide sufficient evidence that one treatment is inferior to the other treatments before enrolling more participants. By doing so, we can remove the worst performing treatment at an interim analysis and prevent the subsequent participants from receiving the removed treatment. On the basis of simulation results, we have evidence that the treatment response rates can still be unbiasedly and efficiently estimated in our new design, especially for the treatments with higher response rates. In addition, by adjusting the decision rule criteria for the posterior probabilities, we can control the probability of incorrectly removing an effective treatment."
24,A spatially varying distributed lag model with application to an air pollution and term low birth weight study,"Joshua L. Warren, Thomas J. Luben, Howard H. Chang",https://onlinelibrary.wiley.com/doi/10.1111/rssc.12407,"Distributed lag models have been used to identify critical pregnancy periods of exposure (i.e. critical exposure windows) to air pollution in studies of pregnancy outcomes. However, much of the previous work in this area has ignored the possibility of spatial variability in the lagged health effect parameters that may result from exposure characteristics and/or residual confounding. We develop a spatially varying Gaussian process model for critical windows called ‘SpGPCW’ and use it to investigate geographic variability in the association between term low birth weight and average weekly concentrations of ozone and PM2.5 during pregnancy by using birth records from North Carolina. SpGPCW is designed to accommodate areal level spatial correlation between lagged health effect parameters and temporal smoothness in risk estimation across pregnancy. Through simulation and a real data application, we show that the consequences of ignoring spatial variability in the lagged health effect parameters include less reliable inference for the parameters and diminished ability to identify true critical window sets, and we investigate the use of existing Bayesian model comparison techniques as tools for determining the presence of spatial variability. We find that exposure to PM2.5 is associated with elevated term low birth weight risk in selected weeks and counties and that ignoring spatial variability results in null associations during these periods. An R package (SpGPCW) has been developed to implement the new method."
25,The harmonic mean χ2‐test to substantiate scientific findings,Leonhard Held,https://onlinelibrary.wiley.com/doi/10.1111/rssc.12410,"Statistical methodology plays a crucial role in drug regulation. Decisions by the US Food and Drug Administration or European Medicines Agency are typically made based on multiple primary studies testing the same medical product, where the two‐trials rule is the standard requirement, despite shortcomings. A new approach is proposed for this task based on the harmonic mean of the squared study‐specific test statistics. Appropriate scaling ensures that, for any number of independent studies, the null distribution is a χ2‐distribution with 1 degree of freedom. This gives rise to a new method for combining one‐sided p‐values and calculating confidence intervals for the overall treatment effect. Further properties are discussed and a comparison with the two‐trials rule is made, as well as with alternative research synthesis methods. An attractive feature of the new approach is that a claim of success requires each study to be convincing on its own to a certain degree depending on the overall level of significance and the number of studies. The new approach is motivated by and applied to data from five clinical trials investigating the effect of carvedilol for the treatment of patients with moderate to severe heart failure."
26,Issue Information,no author,https://onlinelibrary.wiley.com/doi/10.1111/rssc.12359,no abstract
27,Modelling the spatial extent and severity of extreme European windstorms,"Paul Sharkey, Jonathan A. Tawn, Simon J. Brown",https://onlinelibrary.wiley.com/doi/10.1111/rssc.12391,"Windstorms are a primary natural hazard affecting Europe that are commonly linked to substantial property and infrastructural damage and are responsible for the largest spatially aggregated financial losses. Such extreme winds are typically generated by extratropical cyclone systems originating in the North Atlantic and passing over Europe. Previous statistical studies tend to model extreme winds at a given set of sites, corresponding to inference in an Eulerian framework. Such inference cannot incorporate knowledge of the life cycle and progression of extratropical cyclones across the region and is forced to make restrictive assumptions about the extremal dependence structure. We take an entirely different approach which overcomes these limitations by working in a Lagrangian framework. Specifically, we model the development of windstorms over time, preserving the physical characteristics linking the windstorm and the cyclone track, the path of local vorticity maxima, and make a key finding that the spatial extent of extratropical windstorms becomes more localized as its magnitude increases irrespective of the location of the storm track. Our model allows simulation of synthetic windstorm events to derive the joint distributional features over any set of sites giving physically consistent extrapolations to rarer events. From such simulations improved estimates of this hazard can be achieved in terms of both intensity and area affected."
28,The analysis of transformations for profit‐and‐loss data,"Anthony C. Atkinson, Marco Riani, Aldo Corbellini",https://onlinelibrary.wiley.com/doi/10.1111/rssc.12389,"We analyse data on the performance of investment funds, 99 out of 309 of which report a loss, and on the profitability of 1405 firms, 407 of which report losses. The problem in both cases is to use regression to predict performance from sets of explanatory variables. In one case, it is clear from scatter plots of the data that the negative responses have a lower variance than the positive responses and a different relationship with the explanatory variables. Because the data include negative responses, the Box–Cox transformation cannot be used. We develop a robust version of an extension to the Yeo–Johnson transformation which allows different transformations for positive and negative responses. Tests and graphical methods from our robust analysis enable the detection of outliers, the assessment of values of the two transformation parameters and the building of simple regression models. Performance comparisons are made with non‐parametric transformations."
29,Efficient data augmentation for multivariate probit models with panel data: an application to general practitioner decision making about contraceptives,"Vincent Chin, David Gunawan, Denzil G. Fiebig, Robert Kohn, Scott A. Sisson",https://onlinelibrary.wiley.com/doi/10.1111/rssc.12393,"The paper considers the problem of estimating a multivariate probit model in a panel data setting with emphasis on sampling a high dimensional correlation matrix and improving the overall efficiency of the data augmentation approach. We reparameterize the correlation matrix in a principled way and then carry out efficient Bayesian inference by using Hamiltonian Monte Carlo sampling. We also propose a novel antithetic variable method to generate samples from the posterior distribution of the random effects and regression coefficients, resulting in significant gains in efficiency. We apply the methodology by analysing stated preference data obtained from Australian general practitioners evaluating alternative contraceptive products. Our analysis suggests that the joint probability of discussing combinations of contraceptive products with a patient shows medical practice variation among the general practitioners, which indicates some resistance even to discuss these products, let alone to recommend them."
30,Bayesian protein sequence and structure alignment,"Christopher J. Fallaize, Peter J. Green, Kanti V. Mardia, Stuart Barber",https://onlinelibrary.wiley.com/doi/10.1111/rssc.12394,"The structure of a protein is crucial in determining its functionality and is much more conserved than sequence during evolution. A key task in structural biology is to compare protein structures to determine evolutionary relationships, to estimate the function of newly discovered structures and to predict unknown structures. We propose a Bayesian method for protein structure alignment, with the prior on alignments based on functions which penalize ‘gaps’ in the aligned sequences. We show how a broad class of penalty functions fits into this framework, and how the resulting posterior distribution can be efficiently sampled. A commonly used gap penalty function is shown to be a special case, and we propose a new penalty function which alleviates an undesirable feature of the commonly used penalty. We illustrate our method on benchmark data sets and find that it competes well with popular tools from computational biology. Our method has the benefit of being able potentially to explore multiple competing alignments and to quantify their merits probabilistically. The framework naturally enables further information such as amino acid sequence to be included and could be adapted to other situations such as flexible proteins or domain swaps."
31,Estimating seal pup production in the Greenland Sea by using Bayesian hierarchical modelling,"Martin Jullum, Thordis Thorarinsdottir, Fabian E. Bachl",https://onlinelibrary.wiley.com/doi/10.1111/rssc.12397,"The Greenland Sea is an important breeding ground for harp and hooded seals. Estimates of annual seal pup production are critical factors in the estimation of abundance that is needed for management of the species. These estimates are usually based on counts from aerial photographic surveys. However, only a minor part of the whelping region can be photographed, because of its large extent. To estimate total seal pup production, we propose a Bayesian hierarchical modelling approach motivated by viewing the seal pup appearances as a realization of a log‐Gaussian Cox process by using covariate information from satellite imagery as a proxy for ice thickness. For inference, we utilize the stochastic partial differential equation module of the integrated nested Laplace approximation framework. In a case‐study using survey data from 2012, we compare our results with existing methodology in a comprehensive cross‐validation study. The results of the study indicate that our method improves local estimation performance, and that the increased uncertainty of prediction of our method is required to obtain calibrated count predictions. This suggests that the sampling density of the survey design may not be sufficient to obtain reliable estimates of seal pup production."
32,"Estimation and inference in mixed effect regression models using shape constraints, with application to tree height estimation","Xiyue Liao, Mary C. Meyer",https://onlinelibrary.wiley.com/doi/10.1111/rssc.12388,"Estimation of tree height given diameter is an important part of the forest inventory analysis of the US Forest Service. Existing methods use parametric models to estimate the curve. We propose a semiparametric model in which  log (height) is a smooth, increasing and concave function of diameter, with a random‐plot component and fixed effect covariates. Large sample properties and inference methods that work well in practice are derived. Proposed inference methods use approximate normal distributions for the fixed effects and a likelihood ratio test for the significance of the random effect. A closed form approximate prediction method is provided and overall it outperformed competitors for both a simulation and a real data application. The methods are implemented by the cgamm routine in the R package cgam and can be used for a wide range of mixed model applications."
33,Modelling environmental DNA data; Bayesian variable selection accounting for false positive and false negative errors,"Jim E. Griffin, Eleni Matechou, Andrew S. Buxton, Dimitrios Bormpoudakis, Richard A. Griffiths",https://onlinelibrary.wiley.com/doi/10.1111/rssc.12390,"Environmental DNA is a survey tool with rapidly expanding applications for assessing the presence of a species at surveyed sites. Environmental DNA methodology is known to be prone to false negative and false positive errors at the data collection and laboratory analysis stages. Existing models for environmental DNA data require augmentation with additional sources of information to overcome identifiability issues of the likelihood function and do not account for environmental covariates that predict the probability of species presence or the probabilities of error. We present a novel Bayesian model for analysing environmental DNA data by proposing informative prior distributions for logistic regression coefficients that enable us to overcome parameter identifiability, while performing efficient Bayesian variable selection. Our methodology does not require the use of transdimensional algorithms and provides a general framework for performing Bayesian variable selection under informative prior distributions in logistic regression models."
34,Using Cox regression to develop linear rank tests with zero‐inflated clustered data,"Stuart R. Lipsitz, Garrett M. Fitzmaurice, Debajyoti Sinha, Alexander P. Cole, Christian P. Meyer, Quoc‐Dien Trinh",https://onlinelibrary.wiley.com/doi/10.1111/rssc.12396,"Zero‐inflated data arise in many fields of study. When comparing zero‐inflated data between two groups with independent subjects, a 2 degree‐of‐freedom test has been developed, which is the sum of a 1 degree‐of‐freedom Pearson χ2‐test for the 2×2 table of group versus dichotomized outcome (0,>0) and a 1 degree‐of‐freedom Wilcoxon rank sum test for the values of the outcome ‘>0’. Here, we extend this 2 degrees‐of‐freedom test to clustered data settings. We first propose the use of an estimating equations score statistic from a time‐varying weighted Cox regression model under naive independence, with a robust sandwich variance estimator to account for clustering. Since our proposed test statistics can be put in the framework of a Cox model, to gain efficiency over naive independence, we apply a generalized estimating equations Cox model with a non‐independence ‘working correlation’ between observations in a cluster. The methods proposed are applied to a General Social Survey study of days with mental health problems in a month, in which 52.3% of subjects report that they have no days with problems: a zero‐inflated outcome. A simulation study is used to compare our proposed test statistics with previously proposed zero‐inflated test statistics."
35,Assessing heterogeneity in transition propensity in multistate capture–recapture data,"Anita Jeyam, Rachel McCrea, Roger Pradel",https://onlinelibrary.wiley.com/doi/10.1111/rssc.12392,"Multistate capture–recapture models are a useful tool to help to understand the dynamics of movement within discrete capture–recapture data. The standard multistate capture–recapture model, however, relies on assumptions of homogeneity within the population with respect to survival, capture and transition probabilities. There are many ways in which this model can be generalized so some guidance on what is really needed is highly desirable. Within the paper we derive a new test that can detect heterogeneity in transition propensity and show its good power by using simulation and application to a Canada goose data set. We also demonstrate that existing tests which have traditionally been used to diagnose memory are in fact sensitive to other forms of transition heterogeneity and we propose modified tests which can distinguish between memory and other forms of transition heterogeneity."
36,A flexible parametric modelling framework for survival analysis,"Kevin Burke, M. C. Jones, Angela Noufaily",https://onlinelibrary.wiley.com/doi/10.1111/rssc.12398,"We introduce a general, flexible, parametric survival modelling framework which encompasses key shapes of hazard functions (constant; increasing; decreasing; up then down; down then up) and various common survival distributions (log‐logistic; Burr type XII; Weibull; Gompertz) and includes defective distributions (cure models). This generality is achieved by using four distributional parameters: two scale‐type parameters—one of which relates to accelerated failure time (AFT) modelling; the other to proportional hazards (PH) modelling—and two shape parameters. Furthermore, we advocate ‘multiparameter regression’ whereby more than one distributional parameter depends on covariates—rather than the usual convention of having a single covariate‐dependent (scale) parameter. This general formulation unifies the most popular survival models, enabling us to consider the practical value of possible modelling choices. In particular, we suggest introducing covariates through just one or other of the two scale parameters (covering AFT and PH models), and through a ‘power’ shape parameter (covering more complex non‐AFT or non‐PH effects); the other shape parameter remains covariate independent and handles automatic selection of the baseline distribution. We explore inferential issues and compare with alternative models through various simulation studies, with particular focus on evidence concerning the need, or otherwise, to include both AFT and PH parameters. We illustrate the efficacy of our modelling framework by using data from lung cancer, melanoma and kidney function studies. Censoring is accommodated throughout."
37,Bayesian hierarchical modelling of growth curve derivatives via sequences of quotient differences,"Garritt L. Page, María Xosé Rodríguez‐Álvarez, Dae‐Jin Lee",https://onlinelibrary.wiley.com/doi/10.1111/rssc.12399,"Growth curve studies are typically conducted to evaluate differences between group or treatment‐specific curves. Most analyses focus solely on the growth curves, but it has been argued that the derivative of growth curves can highlight differences between groups that may be masked when considering the raw curves only. Motivated by the desire to estimate derivative curves hierarchically, we introduce a new sequence of quotient differences (empirical derivatives) which, among other things, are well behaved near the boundaries compared with other sequences in the literature. Using the sequence of quotient differences, we develop a Bayesian method to estimate curve derivatives in a multilevel setting (a common scenario in growth studies) and show how the method can be used to estimate individual and group derivative curves and to make comparisons. We apply the new methodology to data collected from a study conducted to explore the effect that radiation‐based therapies have on growth in female children diagnosed with acute lymphoblastic leukaemia."
38,Factor‐augmented Bayesian cointegration models: a case‐study on the soybean crush spread,"Maciej Marowka, Gareth W. Peters, Nikolas Kantas, Guillaume Bagnarosa",https://onlinelibrary.wiley.com/doi/10.1111/rssc.12395,"We investigate how vector auto‐regressive models can be used to study the soybean crush spread. By crush spread we mean a time series marking the difference between a weighted combination of the value of soymeal and soyoil to the value of the original soybeans. Commodity industry practitioners often use fixed prescribed values for these weights, which do not take into account any time‐varying effects or any financial‐market‐based dynamic features that can be discerned from futures price data. We address this issue by proposing an appropriate time series model with cointegration. Our model consists of an extension of a particular vector auto‐regressive model that is used widely in econometrics. Our extensions are inspired by the problem at hand and allow for a time‐varying covariance structure and a time‐varying intercept to account for seasonality. To perform Bayesian inference we design an efficient Markov chain Monte Carlo algorithm, which is based on the approach of Koop and his co‐workerss. Our investigations on prices obtained from futures contracts data confirmed that the added features in our model are useful in reliable statistical determination of the crush spread. Although the interest here is on the soybean crush spread, our approach is applicable also to other tradable spreads such as oil and energy‐based crack and spark spreads."
39,Issue Information,no author,https://onlinelibrary.wiley.com/doi/10.1111/rssc.12358,no abstract
40,Analysis of tornado reports through replicated spatiotemporal point patterns,"Jonatan A. González, Ute Hahn, Jorge Mateu",https://onlinelibrary.wiley.com/doi/10.1111/rssc.12375,"Understanding the spatiotemporal distribution of tornado events is increasingly imperative, not only because of the natural phenomenon itself and its tremendous complexity but also because we can potentially reduce the risks that they entail. In particular, the US regions are particularly susceptible to tornadoes and they are the focus and motivation of our statistical analysis. Tornado reports can be treated as spatiotemporal point patterns, and we develop some methods for the analysis of replicated spatiotemporal patterns to identify significant structural differences between cold and warm seasons along the years. We extend some existing spatial techniques to the spatiotemporal context to test the null hypothesis that two (or more) observed spatiotemporal point patterns with replications are realizations of point processes that have the same second‐order descriptors. In particular, we develop a non‐parametric test to approximate the null distribution of the test statistics. We present intensive simulation studies that demonstrate the validity and power of our test and apply our methods to the motivating problem of tornadoes."
41,Longitudinal dynamic functional regression,"Ana‐Maria Staicu, Md Nazmul Islam, Raluca Dumitru, Eric van Heugten",https://onlinelibrary.wiley.com/doi/10.1111/rssc.12376,"The paper develops a parsimonious modelling framework to study the time‐varying association between scalar outcomes and functional predictors observed at many instances, in longitudinal studies. The methods enable us to reconstruct the full trajectory of the response and are applicable to Gaussian and non‐Gaussian responses. The idea is to model the time‐varying functional predictors by using orthogonal basis functions and to expand the time‐varying regression coefficient by using the same basis. Numerical investigation through simulation studies and data analysis show excellent performance in terms of accurate prediction and efficient computations, when compared with existing alternatives. The methods are inspired and applied to an animal science application, where of interest is to study the association between the feed intake of lactating sows and the minute‐by‐minute temperature throughout the 21 days of their lactation period. R code and an R illustration are provided."
42,Zoom‐in–out joint graphical lasso for different coarseness scales,"Eugen Pircalabelu, Gerda Claeskens, Lourens J. Waldorp",https://onlinelibrary.wiley.com/doi/10.1111/rssc.12378,A new method is proposed to estimate graphical models simultaneously from data obtained at different coarseness scales. Starting from a predefined scale the method offers the possibility to zoom in or out over scales on particular edges. The estimated graphs over the different scales have similar structures although their level of sparsity depends on the scale at which estimation takes place. The method makes it possible to evaluate the evolution of the graphs from the coarsest to the finest scale or vice versa. We select an optimal coarseness scale to be used for further analysis. Simulation studies and an application on functional magnetic resonance brain imaging data show the method's performance in practice.
43,Selecting biomarkers for building optimal treatment selection rules by using kernel machines,"Sayan Dasgupta, Ying Huang",https://onlinelibrary.wiley.com/doi/10.1111/rssc.12379,"Optimal biomarker combinations for treatment selection can be derived by minimizing the total burden to the population caused by the targeted disease and its treatment. However, when multiple biomarkers are present, including all in the model can be expensive and can hurt model performance. To remedy this, we consider feature selection in optimization by minimizing an extended total burden that additionally incorporates biomarker costs. Formulating it as a 0‐norm penalized weighted classification, we develop various procedures for estimating linear and non‐linear combinations. Through simulations and a real data example, we demonstrate the importance of incorporating feature selection and marker cost when deriving treatment selection rules."
44,Estimating the probability of default for no‐default and low‐default portfolios,Oliver Blümke,https://onlinelibrary.wiley.com/doi/10.1111/rssc.12381,The paper proposes a sequential Bayesian updating approach to estimate default probabilities on rating grade level for no‐ and low‐default portfolios. Bayesian sequential updating enables default probabilities to be obtained also for those rating grades for which no defaults have been observed. The advantage of this approach is that it preserves the rank order of rating grades in the case of no defaults. Rank preservation is not ensured when using an identical prior distribution across all rating grades. We discuss Bayesian sequential updating for the beta–binomial model and a model incorporating the asymptotic single‐risk factor model of the Basel Accord. Practical aspects such as incorporating information from external sources and the margin of conservatism are addressed.
45,Bayesian modelling of marked point processes with incomplete records: volcanic eruptions,"Ting Wang, Matthew Schofield, Mark Bebbington, Koji Kiyosugi",https://onlinelibrary.wiley.com/doi/10.1111/rssc.12380,"Modelling point processes with incomplete records is a challenging problem, especially when the degree of record completeness varies over time. For volcanic eruption records, we expect the degree of missingness to depend on both the time and the size of an eruption. We propose a time‐varying intensity function for a marked point process to model the non‐stationary variation of the observed point process caused by missing data. We apply this model to global and regional volcanic eruption records and use Bayesian inference to obtain hazard estimates and their uncertainties based on the observed incomplete records, to carry out residual analysis and to provide forecasts."
46,A full Bayesian implementation of a generalized partial credit model with an application to an international disability survey,"Sujit K. Sahu, Mark R. Bass, Carla Sabariego, Alarcos Cieza, Carolina S. Fellinghauer, Somnath Chatterji",https://onlinelibrary.wiley.com/doi/10.1111/rssc.12385,"Generalized partial credit models (GPCMs) are ubiquitous in many applications in the health and medical sciences that use item response theory. Such polytomous item response models have a great many uses ranging from assessing and predicting an individual's latent trait to ordering the items to test the effectiveness of the test instrumentation. By implementing these models in a full Bayesian framework, computed through the use of Markov chain Monte Carlo methods implemented in the efficient STAN software package, the paper exploits the full inferential capability of GPCMs. The GPCMs include explanatory covariate effects which allow simultaneous estimation of regression and item parameters. The Bayesian methods for ranking the items by using the Fisher information criterion are implemented by using Markov chain Monte Carlo sampling. This allows us to propagate fully and to ascertain uncertainty in the inferences by calculating the posterior predictive distribution of the item‐specific Fisher information criterion in a novel manner that has not been exploited in the literature before. Lastly, we propose a new Monte Carlo method for predicting the latent trait score of a new individual by approximating the relevant Bayesian predictive distribution. Data from a model disability survey carried out in Sri Lanka by the World Health Organization and the World Bank are used to illustrate the methods. The approaches proposed are shown to provide simultaneous model‐based inference for all aspects of disability which can be explained by environmental and socio‐economic factors."
47,A time‐varying Bayesian joint hierarchical copula model for analysing recurrent events and a terminal event: an application to the Cardiovascular Health Study,"Zheng Li, Vernon M. Chinchilli, Ming Wang",https://onlinelibrary.wiley.com/doi/10.1111/rssc.12382,"Recurrent events could be stopped by a terminal event, which commonly occurs in biomedical and clinical studies. Taking the Cardiovascular Health Study as a motivating example, patients can experience recurrent events of myocardial infarction (MI) or stroke during follow‐up, which, however, can be truncated by death. Since death could be a devastating complication of MI or stroke recurrences, ignoring dependent censoring when analysing recurrent events may lead to invalid inference. The joint shared frailty model is widely used but with several limitations: two event processes are conditionally independent given the subject level frailty, which could be violated because the dependence may rely on unknown covariates varying across recurrences; the correlation between recurrent events and death is constant over time because of the same frailty within subject, but MI or stroke recurrences could have a time‐varying influence on death due to higher risk of another event of MI or stroke after the first. We propose a time‐varying joint hierarchical copula model under the Bayesian framework to accommodate correlation between recurrent events and dependence between two event processes which may change over time. The performance of our method is extensively evaluated by simulation studies, and lastly by the Cardiovascular Health Study for illustration."
48,Inference for biomedical data by using diffusion models with covariates and mixed effects,"Mareile Große Ruse, Adeline Samson, Susanne Ditlevsen",https://onlinelibrary.wiley.com/doi/10.1111/rssc.12386,"Neurobiological data such as electroencephalography measurements pose a statistical challenge due to low spatial resolution and poor signal‐to‐noise ratio, as well as large variability from subject to subject. We propose a new modelling framework for this type of data based on stochastic processes. Stochastic differential equations with mixed effects are a popular framework for modelling biomedical data, e.g. in pharmacological studies. Whereas the inherent stochasticity of diffusion models accounts for prevalent model uncertainty or misspecification, random‐effects model intersubject variability. The two‐layer stochasticity, however, renders parameter inference challenging. Estimates are based on the discretized continuous time likelihood and we investigate finite sample and discretization bias. In applications, the comparison of, for example, treatment effects is often of interest. We discuss hypothesis testing and evaluate by simulations. Finally, we apply the framework to a statistical investigation of electroencephalography recordings from epileptic patients. We close the paper by examining asymptotics (the number of subjects going to ∞) of maximum likelihood estimators in multi‐dimensional, non‐linear and non‐homogeneous stochastic differential equations with random effects and included covariates."
49,Modelling and prediction of financial trading networks: an application to the New York Mercantile Exchange natural gas futures market,"Brenda Betancourt, Abel Rodríguez, Naomi Boyd",https://onlinelibrary.wiley.com/doi/10.1111/rssc.12387,"Over recent years there has been a growing interest in using financial trading networks to understand the microstructure of financial markets. Most of the methodologies that have been developed so far for this have been based on the study of descriptive summaries of the networks such as the average node degree and the clustering coefficient. In contrast, this paper develops novel statistical methods for modelling sequences of financial trading networks. Our approach uses a stochastic block model to describe the structure of the network during each period, and then links multiple time periods by using a hidden Markov model. This structure enables us to identify events that affect the structure of the market and make accurate short‐term prediction of future transactions. The methodology is illustrated by using data from the New York Mercantile Exchange natural gas futures market from January 2005 to December 2008."
