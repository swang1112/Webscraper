,title,author,link,abstract
0,Estimation of graphical models using the  norm,"Khai Xiang Chiong, Hyungsik Roger Moon",https://onlinelibrary.wiley.com/doi/10.1111/ectj.12104,"Gaussian graphical models are recently used in economics to obtain networks of dependence among agents. A widely used estimator is the graphical least absolute shrinkage and selection operator (GLASSO), which amounts to a maximum likelihood estimation regularized using the  matrix norm on the precision matrix Ω. The  norm is a LASSO penalty that controls for sparsity, or the number of zeros in Ω. We propose a new estimator called structured GLASSO (SGLASSO) that uses the  mixed norm. The use of the  penalty controls for the structure of the sparsity in Ω. We show that when the network size is fixed, SGLASSO is asymptotically equivalent to an infeasible GLASSO problem which prioritizes the sparsity‐recovery of high‐degree nodes. Monte Carlo simulation shows that SGLASSO outperforms GLASSO in terms of estimating the overall precision matrix and in terms of estimating the structure of the graphical model. In an empirical illustration using a classic firms' investment data set, we obtain a network of firms' dependence that exhibits the core–periphery structure, with General Motors, General Electric and US Steel forming the core group of firms."
1,CCE in panels with general unknown factors,Joakim Westerlund,https://onlinelibrary.wiley.com/doi/10.1111/ectj.12110,"A popular approach to factor‐augmented panel regressions is the common correlated effects (CCE) estimator of Pesaran (2006). In fact, the approach is so popular that it has given rise to a separate CCE literature. A common assumption in this literature is that the common factors are stationary, which would seem to rule out many empirically relevant cases. Moreover, deterministic factors are typically treated as known, which raises the issue of model misspecification. In the current paper, we show how the conditions placed on the factors in CCE can be made much more general than was previously thought possible. In fact, save for some mild regulatory moment conditions, the factors are essentially unrestricted. One implication of this result is that there is no need to discriminate between deterministic and stochastic factors, but that one can instead treat them all as unknown. This is very convenient for practitioners, because it means that under certain conditions they are spared the problem of having to decide which deterministic terms to include in the model."
2,Robust tests for deterministic seasonality and seasonal mean shifts,"S. Astill, A. M. R. Taylor",https://onlinelibrary.wiley.com/doi/10.1111/ectj.12111,"We develop tests for the presence of deterministic seasonal behaviour and seasonal mean shifts in a seasonally observed univariate time series. These tests are designed to be asymptotically robust to the order of integration of the series at both the zero and seasonal frequencies. Motivated by the approach of Hylleberg, Engle, Granger and Yoo, we base our approach on linear filters of the data that remove any potential unit roots at the frequencies not associated with the deterministic component(s) under test. Test statistics are constructed using the filtered data such that they have well defined limiting null distributions regardless of whether the data are either integrated or stationary at the frequency associated with the deterministic component(s) under test. In the same manner as Vogelsang, Bunzel and Vogelsang and Sayginsoy and Vogelsang, we scale these statistics by a function of an auxiliary seasonal unit root statistic. This allows us to construct tests that are asymptotically robust to the order of integration of the data at both the zero and seasonal frequencies. Monte Carlo evidence suggests that our proposed tests have good finite sample size and power properties. An empirical application to UK gross domestic product indicates the presence of seasonal mean shifts in the data."
3,Non‐parametric Bayesian inference of strategies in repeated games,"Max Kleiman‐Weiner, Joshua B. Tenenbaum, Penghui Zhou",https://onlinelibrary.wiley.com/doi/10.1111/ectj.12112,"Inferring underlying cooperative and competitive strategies from human behaviour in repeated games is important for accurately characterizing human behaviour and understanding how people reason strategically. Finite automata, a bounded model of computation, have been extensively used to compactly represent strategies for these games and are a standard tool in game theoretic analyses. However, inference over these strategies in repeated games is challenging since the number of possible strategies grows exponentially with the number of repetitions yet behavioural data are often sparse and noisy. As a result, previous approaches start by specifying a finite hypothesis space of automata that does not allow for flexibility. This limitation hinders the discovery of novel strategies that may be used by humans but are not anticipated a priori by current theory. Here we present a new probabilistic model for strategy inference in repeated games by exploiting non‐parametric Bayesian modelling. With simulated data, we show that the model is effective at inferring the true strategy rapidly and from limited data, which leads to accurate predictions of future behaviour. When applied to experimental data of human behaviour in a repeated prisoner's dilemma, we uncover strategies of varying complexity and diversity."
4,Beyond plausibly exogenous,"Hans van Kippersluis, Cornelius A. Rietveld",https://onlinelibrary.wiley.com/doi/10.1111/ectj.12113,"We synthesize two recent advances in the literature on instrumental variable (IV) estimation that test and relax the exclusion restriction. Our approach first estimates the direct effect of the IV on the outcome in a subsample for which the IV does not affect the treatment variable. Subsequently, this estimate for the direct effect is used as input for the plausibly exogenous method developed by Conley, Hansen and Rossi. This two‐step procedure provides a novel and informed sensitivity analysis for IV estimation. We illustrate the practical use by estimating the causal effect of (a) attending Catholic high school on schooling outcomes and (b) the number of children on female labour supply."
5,Identification of treatment effects with selective participation in a randomized trial,"Brendan Kline, Elie Tamer",https://onlinelibrary.wiley.com/doi/10.1111/ectj.12114,"Randomized trials (RTs) are used to learn about treatment effects. This paper studies identification of average treatment response (ATR) and average treatment effect (ATE) from RT data under various assumptions. The focus is the problem of external validity of the RT. RT data need not point identify the ATR or ATE because of selective participation in the RT. The paper reports partial‐identification and point‐identification results for the ATR and ATE based on RT data under a variety of assumptions. The results include assumptions sufficient to point identify the ATR or ATE from RT data. Under weaker assumptions, the ATR or ATE is partially identified. Further, attention is given to identification of the sign of the ATE and identification of whether participation in the RT is selective. Finally, identification from RT data is compared to identification from observational data."
6,Index to The Econometrics Journal Volume 21,no author,https://onlinelibrary.wiley.com/doi/10.1111/ectj.12119,no abstract
7,Adaptive wild bootstrap tests for a unit root with non‐stationary volatility,"H. Peter Boswijk, Yang Zu",https://onlinelibrary.wiley.com/doi/10.1111/ectj.12100,"Recent research has emphasized that permanent changes in the innovation variance (caused by structural shifts or an integrated volatility process) lead to size distortions in conventional unit root tests. It has been shown how these size distortions can be resolved using the wild bootstrap. In this paper, we first derive the asymptotic power envelope for the unit root testing problem when the non‐stationary volatility process is known. Next, we show that under suitable conditions, adaptation with respect to the volatility process is possible, in the sense that non‐parametric estimation of the volatility process leads to the same asymptotic power envelope. Implementation of the resulting test involves cross‐validation and the wild bootstrap. A Monte Carlo experiment shows that the asymptotic results are reflected in finite sample properties, and an empirical analysis of real exchange rates illustrates the applicability of the proposed procedures."
8,The wild bootstrap for few (treated) clusters,"James G. MacKinnon, Matthew D. Webb",https://onlinelibrary.wiley.com/doi/10.1111/ectj.12107,"Inference based on cluster‐robust standard errors in linear regression models, using either the Student's t‐distribution or the wild cluster bootstrap, is known to fail when the number of treated clusters is very small. We propose a family of new procedures called the subcluster wild bootstrap, which includes the ordinary wild bootstrap as a limiting case. In the case of pure treatment models, where all observations within clusters are either treated or not, the latter procedure can work remarkably well. The key requirement is that all cluster sizes, regardless of treatment, should be similar. Unfortunately, the analogue of this requirement is not likely to hold for difference‐in‐differences regressions. Our theoretical results are supported by extensive simulations and an empirical example."
9,"Non‐parametric inference on (conditional) quantile differences and interquantile ranges, using L‐statistics","Matt Goldman, David M. Kaplan",https://onlinelibrary.wiley.com/doi/10.1111/ectj.12095,"We provide novel, high‐order accurate methods for non‐parametric inference on quantile differences between two populations in both unconditional and conditional settings. These quantile differences correspond to (conditional) quantile treatment effects under (conditional) independence of a binary treatment and potential outcomes. Our methods use the probability integral transform and a Dirichlet (rather than Gaussian) reference distribution to pick appropriate L‐statistics as confidence interval endpoints, achieving high‐order accuracy. Using a similar approach, we also propose confidence intervals/sets for vectors of quantiles, interquantile ranges and differences of linear combinations of quantiles. In the conditional setting, when smoothing over continuous covariates, optimal bandwidth and coverage probability rates are derived for all methods. Simulations show that the new confidence intervals have a favourable combination of robust accuracy and short length compared with existing approaches. Detailed steps for confidence interval construction are provided in online Appendix E as supporting information, and code for all methods, simulations and empirical examples is provided."
10,"Central limit theorems for conditional efficiency measures and tests of the ‘separability’ condition in non‐parametric, two‐stage models of production","Cinzia Daraio, Léopold Simar, Paul W. Wilson",https://onlinelibrary.wiley.com/doi/10.1111/ectj.12103,"In this paper, we demonstrate that standard central limit theorem (CLT) results do not hold for means of non‐parametric, conditional efficiency estimators, and we provide new CLTs that permit applied researchers to make valid inference about mean conditional efficiency or to compare mean efficiency across groups of producers. The new CLTs are used to develop a test of the restrictive ‘separability’ condition that is necessary for second‐stage regressions of efficiency estimates on environmental variables. We show that if this condition is violated, not only are second‐stage regressions difficult to interpret and perhaps meaningless, but also first‐stage, unconditional efficiency estimates are misleading. As such, the test developed here is of fundamental importance to applied researchers using non‐parametric methods for efficiency estimation. The test is shown to be consistent and its local power is examined. Our simulation results indicate that our tests perform well both in terms of size and power. We provide a real‐world empirical example by re‐examining the paper by Aly et al. (1990, Review of Economics and Statistics 72, 211–18) and rejecting the separability assumption implicitly assumed by Aly et al., calling into question results that appear in hundreds of papers that have been published in recent years."
11,Testing for changing volatility,"Jilin Wu, Zhijie Xiao",https://onlinelibrary.wiley.com/doi/10.1111/ectj.12108,"In this paper, we propose a consistent U‐statistic test with good sampling properties to detect changes in volatility. We show that the test has a limiting standard normal distribution under the null hypothesis, and that it is powerful compared with various alternatives. A Monte Carlo experiment is conducted to highlight the merits of the proposed test relative to other popular tests for structural changes in volatility. An empirical example is examined to demonstrate the practical application of the proposed testing method."
12,Identification and estimation of heteroscedastic binary choice models with endogenous dummy regressors,"Beili Mu, Zhengyu Zhang",https://onlinelibrary.wiley.com/doi/10.1111/ectj.12109,"In this paper, we consider the semiparametric identification and estimation of a heteroscedastic binary choice model with endogenous dummy regressors and no parametric restriction on the distribution of the error term. Our approach addresses various drawbacks associated with previous estimators proposed for this model. It allows for: general multiplicative heteroscedasticity in both selection and outcome equations; a nonparametric selection mechanism; and multiple discrete endogenous regressors. The resulting three‐stage estimator is shown to be asymptotically normal, with a convergence rate that can be arbitrarily close to  if certain smoothness assumptions are satisfied. Simulation results show that our estimator performs reasonably well in finite samples. Our approach is then used to study the intergenerational transmission of smoking habits in British households."
13,Royal Economic Society Annual Conference 2016 Special Issue on Model Selection and Inference,Richard J. Smith,https://onlinelibrary.wiley.com/doi/10.1111/ectj.12098,no abstract
14,Double/debiased machine learning for treatment and structural parameters,"Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, James Robins",https://onlinelibrary.wiley.com/doi/10.1111/ectj.12097,"We revisit the classic semi‐parametric problem of inference on a low‐dimensional parameter θ0 in the presence of high‐dimensional nuisance parameters η0. We depart from the classical setting by allowing for η0 to be so high‐dimensional that the traditional assumptions (e.g. Donsker properties) that limit complexity of the parameter space for this object break down. To estimate η0, we consider the use of statistical or machine learning (ML) methods, which are particularly well suited to estimation in modern, very high‐dimensional cases. ML methods perform well by employing regularization to reduce variance and trading off regularization bias with overfitting in practice. However, both regularization bias and overfitting in estimating η0 cause a heavy bias in estimators of θ0 that are obtained by naively plugging ML estimators of η0 into estimating equations for θ0. This bias results in the naive estimator failing to be  consistent, where N is the sample size. We show that the impact of regularization bias and overfitting on estimation of the parameter of interest θ0 can be removed by using two simple, yet critical, ingredients: (1) using Neyman‐orthogonal moments/scores that have reduced sensitivity with respect to nuisance parameters to estimate θ0; (2) making use of cross‐fitting, which provides an efficient form of data‐splitting. We call the resulting set of methods double or debiased ML (DML). We verify that DML delivers point estimators that concentrate in an ‐neighbourhood of the true parameter values and are approximately unbiased and normally distributed, which allows construction of valid confidence statements. The generic statistical theory of DML is elementary and simultaneously relies on only weak theoretical requirements, which will admit the use of a broad array of modern ML methods for estimating the nuisance parameters, such as random forests, lasso, ridge, deep neural nets, boosted trees, and various hybrids and ensembles of these methods. We illustrate the general theory by applying it to provide theoretical properties of the following: DML applied to learn the main regression parameter in a partially linear regression model; DML applied to learn the coefficient on an endogenous variable in a partially linear instrumental variables model; DML applied to learn the average treatment effect and the average treatment effect on the treated under unconfoundedness; DML applied to learn the local average treatment effect in an instrumental variables setting. In addition to these theoretical applications, we also illustrate the use of DML in three empirical examples."
15,Simpler bootstrap estimation of the asymptotic variance of U‐statistic‐based estimators,"Bo E. Honoré, Luojia Hu",https://onlinelibrary.wiley.com/doi/10.1111/ectj.12099,"The bootstrap is a popular and useful tool for estimating the asymptotic variance of complicated estimators. Ironically, the fact that the estimators are complicated can make the standard bootstrap computationally burdensome because it requires repeated recalculation of the estimator. In this paper, we propose a method that is specific to extremum estimators based on U‐statistics. The contribution here is that rather than repeated recalculation of the U‐statistic‐based estimator, we can recalculate a related estimator based on single sums. A simulation study suggests that the approach leads to a good approximation to the standard bootstrap and that if this is the goal, then our approach is superior to numerical derivative methods."
16,Oracle and adaptive false discovery rate controlling methods for one‐sided testing: theory and application in treatment effect evaluation,"Jiaying Gu, Shu Shen",https://onlinelibrary.wiley.com/doi/10.1111/ectj.12092,"Economists are often interested in identifying effective policies or treatments together with subpopulations of individuals who respond positively (or with a sign that is expected) to these treatment interventions. In this paper, we propose an optimal false discovery rate controlling method that is especially useful for such one‐sided testing problems. The proposed procedure is optimal in the sense of minimizing the false non‐discovery rate while controlling the false discovery rate at a pre‐specified level; it uses a deconvolution method based on non‐parametric maximum likelihood estimation, which allows for a broader class of treatment effect distributions than existing methods do. The proposed test demonstrates good small‐sample performance in Monte Carlo simulations and it is applied to study the effect of attending a more selective high school in Romania. The application reveals strong evidence of treatment effect heterogeneity, in that students who marginally gain access to higher‐ranked schools are more likely to benefit if the higher‐ranked school has a relatively high admission score cut‐off – or, in other words, is more selective."
17,A simple and robust estimator for linear regression models with strictly exogenous instruments,Juan Carlos Escanciano,https://onlinelibrary.wiley.com/doi/10.1111/ectj.12087,"In this paper, I investigate the estimation of linear regression models with strictly exogenous instruments under minimal identifying assumptions. I introduce a uniformly (in the data‐generating process) consistent estimator under nearly minimal identifying assumptions. The proposed estimator, called the integrated instrumental variables (IIV) estimator, is a simple weighted least‐squares estimator. It does not require the choice of a bandwidth or tuning parameter, or the selection of a finite set of instruments. Thus, the estimator is extremely simple to implement. Monte Carlo evidence supports the theoretical claims and suggests that the IIV estimator is a robust complement to optimal instrumental variables in finite samples. In an application with quarterly UK data, the IIV estimator estimates a positive and significant elasticity of intertemporal substitution and an equally sensible estimate for its reciprocal, in sharp contrast to instrumental variables methods that fail to identify these parameters."
18,Identification and estimation of semi‐parametric censored dynamic panel data models of short time periods,"Yingyao Hu, Ji‐Liang Shiu",https://onlinelibrary.wiley.com/doi/10.1111/ectj.12086,"In this paper, we present a semi‐parametric identification and estimation method for censored dynamic panel data models of short time periods and their average partial effects with only two periods of data. The proposed method transforms the semi‐parametric specification of censored dynamic panel data models into a parametric family of distribution functions of observables without specifying the distribution of the initial condition. Then the censored dynamic panel data models are globally identified under a standard maximum likelihood estimation framework. The identifying assumptions are related to the completeness of the families of known parametric distribution functions corresponding to censored dynamic panel data models. Dynamic tobit models and two‐part dynamic regression models satisfy the key assumptions. We propose a sieve maximum likelihood estimator and we investigate the finite sample properties of these sieve‐based estimators using Monte Carlo analysis. Our empirical application using the Medical Expenditure Panel Survey shows that individuals consume more health care when their incomes increase, after controlling for past health expenditures."
